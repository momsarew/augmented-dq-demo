# üìö Documentation Technique - Framework Probabiliste DQ

> **Version** : 1.0
> **Date** : F√©vrier 2025
> **Auteur** : Framework DQ Team

---

## üìë Table des mati√®res

1. [Architecture globale](#1-architecture-globale)
2. [Module analyzer.py](#2-module-analyzerpy)
3. [Module beta_calculator.py](#3-module-beta_calculatorpy) ‚≠ê **IMPORTANT**
4. [Catalogue d'anomalies](#4-catalogue-danomalies) ‚≠ê **NOUVEAU**
5. [Module ahp_elicitor.py](#5-module-ahp_elicitorpy)
6. [Module risk_scorer.py](#6-module-risk_scorerpy)
7. [Module lineage_propagator.py](#7-module-lineage_propagatorpy)
8. [Module comparator.py](#8-module-comparatorpy)
9. [Application principale app.py](#9-application-principale-apppy)
10. [Formules math√©matiques](#10-formules-math√©matiques)
11. [Guide d'extension](#11-guide-dextension)

---

## 1. Architecture globale

### 1.1 Structure des fichiers

```
augmented-dq-demo/
‚îú‚îÄ‚îÄ app.py                          # Application Streamlit principale
‚îú‚îÄ‚îÄ streamlit_gray_css.py           # Styles CSS modernes
‚îú‚îÄ‚îÄ streamlit_anomaly_detection.py  # Module d√©tection anomalies (optionnel)
‚îú‚îÄ‚îÄ requirements.txt                # D√©pendances Python
‚îú‚îÄ‚îÄ GUIDE_UTILISATEUR.md           # Guide utilisateur
‚îú‚îÄ‚îÄ DOCUMENTATION_TECHNIQUE.md     # Ce fichier
‚îÇ
‚îî‚îÄ‚îÄ backend/
    ‚îî‚îÄ‚îÄ engine/
        ‚îú‚îÄ‚îÄ analyzer.py             # Analyse exploratoire des donn√©es
        ‚îú‚îÄ‚îÄ beta_calculator.py      # Calculs distributions Beta
        ‚îú‚îÄ‚îÄ ahp_elicitor.py         # √âlicitation pond√©rations AHP
        ‚îú‚îÄ‚îÄ risk_scorer.py          # Scoring de risque contextualis√©
        ‚îú‚îÄ‚îÄ lineage_propagator.py   # Propagation risque dans le lineage
        ‚îî‚îÄ‚îÄ comparator.py           # Comparaison DAMA vs Probabiliste
```

### 1.2 Flux de donn√©es

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Dataset   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Analyzer   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Beta     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Risk     ‚îÇ
‚îÇ   (CSV)     ‚îÇ     ‚îÇ  (stats)    ‚îÇ     ‚îÇ Calculator  ‚îÇ     ‚îÇ   Scorer    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                               ‚îÇ                    ‚îÇ
                                               ‚ñº                    ‚ñº
                                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                        ‚îÇ   Lineage   ‚îÇ     ‚îÇ  Comparator ‚îÇ
                                        ‚îÇ Propagator  ‚îÇ     ‚îÇ   (DAMA)    ‚îÇ
                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.3 D√©pendances

```python
# requirements.txt
streamlit>=1.29.0      # Interface utilisateur
pandas>=2.0.0          # Manipulation donn√©es
numpy>=1.24.0          # Calculs num√©riques
scipy>=1.11.0          # Distributions statistiques
plotly>=5.18.0         # Visualisations
openpyxl>=3.1.0        # Export Excel
anthropic>=0.18.0      # API Claude (optionnel)
```

---

## 2. Module analyzer.py

### 2.1 Description
Module d'analyse exploratoire des donn√©es. D√©tecte les probl√®mes de qualit√© dans chaque colonne.

### 2.2 Fonctions

#### `analyze_dataset(df, columns)`

```python
def analyze_dataset(df: pd.DataFrame, columns: List[str]) -> Dict[str, Any]
```

| Param√®tre | Type | Description |
|-----------|------|-------------|
| `df` | `pd.DataFrame` | Dataset √† analyser |
| `columns` | `List[str]` | Liste des colonnes √† analyser |

**Retourne** : `Dict[str, Any]`

```python
{
    "nom_colonne": {
        "dtype": "object",           # Type pandas
        "total_rows": 1000,          # Nombre de lignes
        "null_count": 50,            # Valeurs nulles
        "null_rate": 0.05,           # Taux de nullit√© (0-1)
        "unique_count": 150,         # Valeurs uniques
        "sample_values": [...],      # 5 premiers exemples
        "type_errors": {...},        # Erreurs de type d√©tect√©es
        "business_violations": {...} # Violations r√®gles m√©tier
    }
}
```

---

#### `detect_type_errors(series)`

```python
def detect_type_errors(series: pd.Series) -> Dict[str, Any]
```

| Param√®tre | Type | Description |
|-----------|------|-------------|
| `series` | `pd.Series` | Colonne √† analyser |

**Retourne** : `Dict[str, Any]`

```python
{
    "error_count": 25,              # Nombre d'erreurs
    "error_rate": 0.025,            # Taux d'erreur (0-1)
    "patterns": [                   # Patterns d√©tect√©s
        "virgule_decimale",         # Ex: "7,21" au lieu de 7.21
        "format_mixte"              # Ex: m√©lange DATE et STRING
    ],
    "examples": ["7,21", "N/A"]     # Exemples d'erreurs
}
```

**Algorithme** :
- Regex pour d√©tecter les virgules d√©cimales fran√ßaises
- D√©tection de formats de dates mixtes
- Identification des valeurs non-num√©riques dans colonnes num√©riques

---

#### `detect_business_violations(series, col_name)`

```python
def detect_business_violations(series: pd.Series, col_name: str) -> Dict[str, Any]
```

| Param√®tre | Type | Description |
|-----------|------|-------------|
| `series` | `pd.Series` | Colonne √† analyser |
| `col_name` | `str` | Nom de la colonne (pour r√®gles contextuelles) |

**Retourne** : `Dict[str, Any]`

```python
{
    "violation_count": 10,
    "violation_rate": 0.01,
    "rules_violated": [
        "date_future",              # Date > aujourd'hui
        "valeur_negative"           # Montant < 0
    ],
    "examples": ["2030-01-01", "-500"]
}
```

**R√®gles impl√©ment√©es** :
- `date_future` : Dates dans le futur (colonnes historiques)
- `valeur_negative` : Valeurs n√©gatives (montants, anciennet√©)
- `incoherence_calcul` : Incoh√©rences de calcul

---

## 3. Module beta_calculator.py

### 3.1 Description
Impl√©mente les calculs de distributions Beta pour la quantification de l'incertitude.

**‚ö†Ô∏è IMPORTANT : Ce module utilise le catalogue d'anomalies pour calculer les probabilit√©s P_DB, P_DP, P_BR, P_UP.**

---

### 3.2 LOGIQUE DE CALCUL DES PROBABILIT√âS (P_DB, P_DP, P_BR, P_UP)

#### 3.2.1 Vue d'ensemble

Le calcul des probabilit√©s d'erreur par dimension se fait en **scannant un r√©f√©rentiel d'anomalies** (et non par des formules simplistes).

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PROCESSUS DE CALCUL DES P_*                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                         ‚îÇ
‚îÇ  1. CHARGER le catalogue d'anomalies (60 anomalies)                    ‚îÇ
‚îÇ                         ‚Üì                                               ‚îÇ
‚îÇ  2. FILTRER selon le niveau de profiling                               ‚îÇ
‚îÇ     ‚Ä¢ Quick: Criticit√© non faible + Woodall SAST/SAMT                  ‚îÇ
‚îÇ     ‚Ä¢ Standard: Criticit√© non faible + SAST/SAMT/MAST                  ‚îÇ
‚îÇ     ‚Ä¢ Advanced: Toutes les anomalies                                    ‚îÇ
‚îÇ                         ‚Üì                                               ‚îÇ
‚îÇ  3. TRIER par score de priorit√© (apprentissage)                        ‚îÇ
‚îÇ     ‚Ä¢ Anomalies les plus fr√©quemment d√©tect√©es en premier              ‚îÇ
‚îÇ                         ‚Üì                                               ‚îÇ
‚îÇ  4. SCANNER les donn√©es pour chaque anomalie                           ‚îÇ
‚îÇ     ‚Ä¢ Ex√©cuter le d√©tecteur associ√©                                     ‚îÇ
‚îÇ     ‚Ä¢ Compter les lignes affect√©es                                      ‚îÇ
‚îÇ                         ‚Üì                                               ‚îÇ
‚îÇ  5. CALCULER P_dimension = lignes_affect√©es / total_lignes             ‚îÇ
‚îÇ                         ‚Üì                                               ‚îÇ
‚îÇ  6. METTRE √Ä JOUR l'apprentissage                                       ‚îÇ
‚îÇ     ‚Ä¢ Incr√©menter scan_count et detection_count                         ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 3.2.2 Le R√©f√©rentiel d'Anomalies

Chaque anomalie du catalogue est caract√©ris√©e par :

| Attribut | Description | Exemple |
|----------|-------------|---------|
| **ID** | Identifiant unique | `DB#1`, `DP#2`, `BR#3`, `UP#1` |
| **Dimension** | DB, DP, BR, ou UP | `DB` (Structure) |
| **Nom** | Description courte | "NULL dans colonnes obligatoires" |
| **Criticit√©** | CRITIQUE, √âLEV√â, MOYEN, FAIBLE | `CRITIQUE` (4 points) |
| **Woodall Level** | Fr√©quence de survenance | `SAST` (tr√®s fr√©quent) |
| **D√©tecteur** | Fonction Python de d√©tection | `detect_null_in_required()` |
| **detection_count** | Nombre de d√©tections (apprentissage) | `12` |
| **scan_count** | Nombre de scans (apprentissage) | `15` |
| **frequency** | Taux de d√©tection historique | `0.80` (80%) |

#### 3.2.3 Niveaux Woodall

| Niveau | Signification | Quand scanner ? |
|--------|---------------|-----------------|
| **SAST** | Anomalie **tr√®s fr√©quente** | Toujours (Quick, Standard, Advanced) |
| **SAMT** | Anomalie **fr√©quence moyenne** | Toujours (Quick, Standard, Advanced) |
| **MAST** | Anomalie **rare** | Standard et Advanced uniquement |

#### 3.2.4 Filtrage par Niveau de Profiling

```python
# Niveaux de profiling disponibles
class ProfilingLevel:
    QUICK = "quick"      # Scan rapide (~40% des anomalies)
    STANDARD = "standard"  # Scan standard (~60% des anomalies)
    ADVANCED = "advanced"  # Scan complet (100% des anomalies)
```

| Niveau | Criticit√© filtr√©e | Woodall filtr√© | Anomalies scann√©es |
|--------|-------------------|----------------|-------------------|
| **Quick** | ‚â† FAIBLE (Moyenne, √âlev√©e, Critique) | SAST + SAMT | ~24 sur 60 |
| **Standard** | ‚â† FAIBLE | SAST + SAMT + MAST | ~45 sur 60 |
| **Advanced** | Toutes (y compris FAIBLE) | Tous | 60 sur 60 |

#### 3.2.5 Effet d'Apprentissage

√Ä chaque scan, le syst√®me **apprend** quelles anomalies sont les plus fr√©quentes :

```python
# Apr√®s chaque scan d'une anomalie
anomaly.scan_count += 1
if detected:
    anomaly.detection_count += 1
anomaly.frequency = detection_count / scan_count

# Score de priorit√© adaptatif
def get_priority_score(self) -> float:
    impact = criticality.value * 25  # CRITIQUE=100, √âLEV√â=75, etc.
    freq_boost = frequency * 100 if scan_count >= 3 else impact
    return freq_boost * (impact / 100)
```

**R√©sultat** : Les anomalies les plus souvent d√©tect√©es sont scann√©es **en premier** lors des prochains scans.

#### 3.2.6 Fichier d'apprentissage

Les stats d'apprentissage sont persist√©es dans `extended_anomaly_stats.json` :

```json
{
  "DB#1": {"detection_count": 12, "scan_count": 12, "frequency": 1.0},
  "DB#2": {"detection_count": 9, "scan_count": 12, "frequency": 0.75},
  "DP#2": {"detection_count": 6, "scan_count": 6, "frequency": 1.0},
  "BR#2": {"detection_count": 10, "scan_count": 12, "frequency": 0.83}
}
```

---

### 3.3 Classe AnomalyBasedCalculator

```python
class AnomalyBasedCalculator:
    """Calculateur de probabilit√©s bas√© sur le catalogue d'anomalies"""

    def __init__(self, persistence_file: str = None):
        self.catalog_manager = ExtendedCatalogManager(persistence_file)
        self.beta_calculator = BetaCalculator()
```

#### `filter_anomalies_by_profiling_level(profiling_level, dimension)`

```python
def filter_anomalies_by_profiling_level(
    profiling_level: str,  # 'quick', 'standard', 'advanced'
    dimension: str = None  # 'DB', 'DP', 'BR', 'UP' (optionnel)
) -> List[CoreAnomaly]
```

**Retourne** : Liste d'anomalies filtr√©es et **tri√©es par score de priorit√©** (d√©croissant).

---

#### `scan_dimension(df, dimension, profiling_level, column_config)`

```python
def scan_dimension(
    df: pd.DataFrame,
    dimension: str,           # 'DB', 'DP', 'BR', 'UP'
    profiling_level: str,     # 'quick', 'standard', 'advanced'
    column_config: Dict = None
) -> Dict[str, Any]
```

**Retourne** :

```python
{
    'P_dimension': 0.15,           # Probabilit√© calcul√©e
    'dimension': 'DB',
    'profiling_level': 'standard',
    'anomalies_scanned': 12,       # Nombre d'anomalies scann√©es
    'anomalies_detected': [        # Anomalies trouv√©es
        {'id': 'DB#1', 'name': 'NULL obligatoire', 'affected_rows': 50},
        {'id': 'DB#2', 'name': 'Doublons PK', 'affected_rows': 25}
    ],
    'total_affected_rows': 75,
    'total_rows': 500,
    'scan_details': [...]          # D√©tails de chaque scan
}
```

---

#### `compute_4d_vector_from_catalog(df, profiling_level, column_config)`

```python
def compute_4d_vector_from_catalog(
    df: pd.DataFrame,
    profiling_level: str = 'standard',
    column_config: Dict = None
) -> Dict[str, Any]
```

**Processus complet** :
1. Pour chaque dimension (DB, DP, BR, UP) ‚Üí appeler `scan_dimension()`
2. Extraire P_DB, P_DP, P_BR, P_UP
3. Convertir en distributions Beta via `BetaCalculator`
4. Retourner le vecteur 4D avec m√©tadonn√©es

**Retourne** :

```python
{
    'profiling_level': 'standard',
    'total_rows': 687,
    'dimensions': {
        'DB': {'P_dimension': 0.15, 'anomalies_scanned': 12, ...},
        'DP': {'P_dimension': 0.08, 'anomalies_scanned': 10, ...},
        'BR': {'P_dimension': 0.12, 'anomalies_scanned': 8, ...},
        'UP': {'P_dimension': 0.05, 'anomalies_scanned': 6, ...}
    },
    'vector_4d': {
        'P_DB': 0.15, 'alpha_DB': 15.0, 'beta_DB': 85.0, ...
        'P_DP': 0.08, 'alpha_DP': 8.0, 'beta_DP': 92.0, ...
        ...
    },
    'summary': {
        'P_DB': 0.15, 'P_DP': 0.08, 'P_BR': 0.12, 'P_UP': 0.05,
        'anomalies_scanned_total': 36,
        'anomalies_detected_total': 8
    }
}
```

---

### 3.4 Classe BetaCalculator

```python
class BetaCalculator:
    CONFIDENCE_MAP = {
        'HIGH': 100,    # √âquivalent √† 100 observations
        'MEDIUM': 50,   # √âquivalent √† 50 observations
        'LOW': 20       # √âquivalent √† 20 observations
    }
```

#### `compute_beta_params(error_rate, confidence_level, n_obs_equivalent)`

```python
def compute_beta_params(
    error_rate: float,
    confidence_level: str = 'HIGH',
    n_obs_equivalent: int = None
) -> Dict[str, float]
```

| Param√®tre | Type | Description |
|-----------|------|-------------|
| `error_rate` | `float` | Taux d'erreur observ√© (0-1) |
| `confidence_level` | `str` | 'HIGH', 'MEDIUM', 'LOW' |
| `n_obs_equivalent` | `int` | Override du nombre d'observations |

**Retourne** : `Dict[str, float]`

```python
{
    "alpha": 2.0,           # Param√®tre Œ± de Beta
    "beta": 98.0,           # Param√®tre Œ≤ de Beta
    "E_P": 0.02,            # Esp√©rance E[P]
    "std": 0.014,           # √âcart-type
    "confidence": "HIGH",
    "n_obs_equiv": 100,
    "ci_lower": 0.003,      # Borne inf IC 95%
    "ci_upper": 0.052       # Borne sup IC 95%
}
```

**Formules** :
```
Œ± = error_rate √ó n
Œ≤ = (1 - error_rate) √ó n
E[P] = Œ± / (Œ± + Œ≤)
Var[P] = Œ±Œ≤ / ((Œ±+Œ≤)¬≤(Œ±+Œ≤+1))
IC_95% = [Beta.ppf(0.025), Beta.ppf(0.975)]
```

---

#### `compute_4d_vector(P_DB, P_DP, P_BR, P_UP, ...)`

```python
def compute_4d_vector(
    P_DB: float,                    # Taux erreur Structure (depuis scan DB)
    P_DP: float,                    # Taux erreur Traitements (depuis scan DP)
    P_BR: float,                    # Taux erreur R√®gles M√©tier (depuis scan BR)
    P_UP: float,                    # Taux erreur Utilisabilit√© (depuis scan UP)
    confidence_DB: str = 'HIGH',
    confidence_DP: str = 'MEDIUM',
    confidence_BR: str = 'MEDIUM',
    confidence_UP: str = 'LOW'
) -> Dict[str, Any]
```

**Retourne** : Vecteur 4D complet

```python
{
    # Dimension DB (Structure)
    "alpha_DB": 99.0, "beta_DB": 1.0,
    "P_DB": 0.99, "std_DB": 0.01,
    "ci_lower_DB": 0.95, "ci_upper_DB": 1.0,

    # Dimension DP (Traitements)
    "alpha_DP": 1.0, "beta_DP": 49.0,
    "P_DP": 0.02, ...

    # Dimension BR (R√®gles M√©tier)
    "alpha_BR": 10.0, "beta_BR": 40.0,
    "P_BR": 0.20, ...

    # Dimension UP (Utilisabilit√©)
    "alpha_UP": 2.0, "beta_UP": 18.0,
    "P_UP": 0.10, ...
}
```

---

### 3.5 Fonctions utilitaires

#### `compute_all_beta_vectors(df, columns, stats, profiling_level)`

```python
def compute_all_beta_vectors(
    df: pd.DataFrame,
    columns: List[str],
    stats: Dict[str, Any],
    profiling_level: str = 'standard'  # 'quick', 'standard', 'advanced'
) -> Dict[str, Dict]
```

**Algorithme de calcul des probabilit√©s (NOUVELLE LOGIQUE)** :

```
1. Charger le catalogue d'anomalies (60 anomalies)
2. Filtrer selon le niveau de profiling:
   ‚Ä¢ Quick: Criticit√© ‚â† FAIBLE + Woodall SAST/SAMT
   ‚Ä¢ Standard: Criticit√© ‚â† FAIBLE + Woodall SAST/SAMT/MAST
   ‚Ä¢ Advanced: Toutes les anomalies
3. Trier par score de priorit√© (apprentissage - fr√©quence)
4. Scanner chaque anomalie dans l'ordre de priorit√©
5. Calculer P_dimension = lignes_affect√©es / total_lignes
6. Mettre √† jour les stats d'apprentissage
```

---

#### `update_beta_with_new_evidence(current_alpha, current_beta, new_successes, new_failures)`

```python
def update_beta_with_new_evidence(
    current_alpha: float,
    current_beta: float,
    new_successes: int,
    new_failures: int
) -> Tuple[float, float]
```

**Mise √† jour Bay√©sienne** :
```
Beta(Œ±', Œ≤') = Beta(Œ± + failures, Œ≤ + successes)
```

**Exemple** :
```python
# Prior : Beta(2, 98) ‚Üí 2% d'erreurs
# Nouvelles observations : 5 erreurs sur 100
alpha_new, beta_new = update_beta_with_new_evidence(2, 98, 95, 5)
# Posterior : Beta(7, 193) ‚Üí ~3.5% d'erreurs
```

---

### 3.6 Diagramme de flux complet

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         CALCUL DES VECTEURS 4D                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. CHARGEMENT CATALOGUE                                                     ‚îÇ
‚îÇ     ExtendedCatalogManager() ‚Üí 60 anomalies                                  ‚îÇ
‚îÇ     + Chargement stats apprentissage (extended_anomaly_stats.json)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. FILTRAGE PAR NIVEAU DE PROFILING                                         ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ     ‚îÇ Quick    : Criticit√© ‚â† FAIBLE + Woodall ‚àà {SAST, SAMT}    ‚Üí ~24    ‚îÇ  ‚îÇ
‚îÇ     ‚îÇ Standard : Criticit√© ‚â† FAIBLE + Woodall ‚àà {SAST,SAMT,MAST}‚Üí ~45    ‚îÇ  ‚îÇ
‚îÇ     ‚îÇ Advanced : Toutes                                          ‚Üí 60     ‚îÇ  ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. TRI PAR SCORE DE PRIORIT√â (APPRENTISSAGE)                                ‚îÇ
‚îÇ     score = frequency √ó criticality_impact                                   ‚îÇ
‚îÇ     ‚Üí Anomalies les plus fr√©quemment d√©tect√©es en premier                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. SCAN PAR DIMENSION                                                       ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ     ‚îÇ    DB      ‚îÇ    DP      ‚îÇ    BR      ‚îÇ    UP      ‚îÇ                   ‚îÇ
‚îÇ     ‚îÇ scan_dim() ‚îÇ scan_dim() ‚îÇ scan_dim() ‚îÇ scan_dim() ‚îÇ                   ‚îÇ
‚îÇ     ‚îÇ    ‚Üì       ‚îÇ    ‚Üì       ‚îÇ    ‚Üì       ‚îÇ    ‚Üì       ‚îÇ                   ‚îÇ
‚îÇ     ‚îÇ  P_DB=0.15 ‚îÇ  P_DP=0.08 ‚îÇ  P_BR=0.12 ‚îÇ  P_UP=0.05 ‚îÇ                   ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  5. CONVERSION EN DISTRIBUTIONS BETA                                         ‚îÇ
‚îÇ     BetaCalculator.compute_4d_vector(P_DB, P_DP, P_BR, P_UP)                ‚îÇ
‚îÇ     ‚Üí alpha_DB, beta_DB, std_DB, ci_lower_DB, ci_upper_DB, ...              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  6. MISE √Ä JOUR APPRENTISSAGE                                                ‚îÇ
‚îÇ     Pour chaque anomalie scann√©e:                                            ‚îÇ
‚îÇ       anomaly.scan_count += 1                                                ‚îÇ
‚îÇ       if detected: anomaly.detection_count += 1                              ‚îÇ
‚îÇ       anomaly.frequency = detection_count / scan_count                       ‚îÇ
‚îÇ     ‚Üí Sauvegarde dans extended_anomaly_stats.json                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 4. Catalogue d'anomalies

### 4.1 Description

Le **catalogue d'anomalies** est le c≈ìur du syst√®me de calcul des probabilit√©s. Il contient 60 anomalies (15 par dimension) avec leurs d√©tecteurs et leurs statistiques d'apprentissage.

### 4.2 Fichiers du catalogue

| Fichier | Description |
|---------|-------------|
| `backend/core_anomaly_catalog.py` | Catalogue CORE (15 anomalies r√©elles) |
| `backend/extended_anomaly_catalog.py` | Catalogue √âTENDU (60 anomalies) |
| `extended_anomaly_stats.json` | Stats d'apprentissage (persistance) |

### 4.3 Structure d'une anomalie

```python
@dataclass
class CoreAnomaly:
    id: str                    # "DB#1", "DP#2", etc.
    dimension: Dimension       # DB, DP, BR, UP
    name: str                  # "NULL dans colonnes obligatoires"
    description: str           # Description d√©taill√©e
    criticality: Criticality   # CRITIQUE, √âLEV√â, MOYEN, FAIBLE
    woodall_level: str         # "SAST", "SAMT", "MAST"
    detector: Callable         # Fonction de d√©tection
    sql_template: str          # Template SQL √©quivalent
    example: str               # Exemple d'impact business

    # M√©tadonn√©es apprentissage
    detection_count: int = 0   # Nombre de fois d√©tect√©e
    scan_count: int = 0        # Nombre de fois scann√©e
    frequency: float = 0.0     # detection_count / scan_count
```

### 4.4 Liste des anomalies par dimension

#### Dimension DB (Structure) - 15 anomalies

| ID | Nom | Criticit√© | Woodall | D√©tecteur |
|----|-----|-----------|---------|-----------|
| DB#1 | NULL dans colonnes obligatoires | CRITIQUE | SAST | ‚úÖ R√©el |
| DB#2 | Doublons cl√© primaire | CRITIQUE | SAMT | ‚úÖ R√©el |
| DB#3 | Formats email invalides | MOYEN | SAST | ‚úÖ R√©el |
| DB#4 | Valeurs hors domaine | √âLEV√â | SAST | ‚úÖ R√©el |
| DB#5 | Valeurs n√©gatives interdites | MOYEN | SAST | ‚úÖ R√©el |
| DB#6-15 | Templates | MOYEN | SAMT | üìù Template |

#### Dimension DP (Traitements) - 15 anomalies

| ID | Nom | Criticit√© | Woodall | D√©tecteur |
|----|-----|-----------|---------|-----------|
| DP#1 | Calculs d√©riv√©s incorrects | √âLEV√â | MAST | ‚úÖ R√©el |
| DP#2 | Divisions par z√©ro | MOYEN | SAST | ‚úÖ R√©el |
| DP#3 | Type donn√©es incorrect | MOYEN | SAST | ‚úÖ R√©el |
| DP#4-15 | Templates | MOYEN | MAST | üìù Template |

#### Dimension BR (R√®gles M√©tier) - 15 anomalies

| ID | Nom | Criticit√© | Woodall | D√©tecteur |
|----|-----|-----------|---------|-----------|
| BR#1 | Incoh√©rences temporelles | √âLEV√â | MAST | ‚úÖ R√©el |
| BR#2 | Valeurs hors bornes m√©tier | CRITIQUE | MAST | ‚úÖ R√©el |
| BR#3 | Combinaisons interdites | √âLEV√â | MAST | ‚úÖ R√©el |
| BR#4 | Obligations m√©tier (SI A ALORS B) | √âLEV√â | MAST | ‚úÖ R√©el |
| BR#5-15 | Templates | MOYEN | MAST | üìù Template |

#### Dimension UP (Utilisabilit√©) - 15 anomalies

| ID | Nom | Criticit√© | Woodall | D√©tecteur |
|----|-----|-----------|---------|-----------|
| UP#1 | Donn√©es obsol√®tes | √âLEV√â | SAMT | ‚úÖ R√©el |
| UP#2 | Granularit√© excessive | FAIBLE | SAMT | ‚úÖ R√©el |
| UP#3 | Granularit√© insuffisante | MOYEN | SAMT | ‚úÖ R√©el |
| UP#4-15 | Templates | FAIBLE | SAMT | üìù Template |

### 4.5 Exemples de d√©tecteurs

#### DB#1 - NULL dans colonnes obligatoires

```python
def detect_null_in_required(df: pd.DataFrame, columns: List[str]) -> Dict:
    results = {}
    total_nulls = 0
    samples = []

    for col in columns:
        if col in df.columns:
            nulls = df[df[col].isnull()]
            if len(nulls) > 0:
                total_nulls += len(nulls)
                samples.extend(nulls.head(2).to_dict('records'))

    return {
        'detected': total_nulls > 0,
        'affected_rows': total_nulls,
        'columns_with_nulls': [...],
        'sample': samples[:5]
    }
```

#### BR#2 - Valeurs hors bornes m√©tier

```python
def detect_out_of_business_range(df: pd.DataFrame, column: str,
                                  min_val: float, max_val: float) -> Dict:
    values = pd.to_numeric(df[column], errors='coerce')
    out_of_range = df[(values < min_val) | (values > max_val)]

    return {
        'detected': len(out_of_range) > 0,
        'affected_rows': len(out_of_range),
        'business_range': [min_val, max_val],
        'actual_min': float(values.min()),
        'actual_max': float(values.max()),
        'sample': out_of_range.head(5).to_dict('records')
    }
```

### 4.6 Syst√®me d'apprentissage

#### Score de priorit√© adaptatif

```python
def get_priority_score(self) -> float:
    """
    Calcule le score de priorit√© pour le tri des anomalies

    - Les anomalies les plus critiques ont un score de base plus √©lev√©
    - Les anomalies fr√©quemment d√©tect√©es sont boost√©es
    """
    impact = self.criticality.value * 25  # CRITIQUE=100, √âLEV√â=75, MOYEN=50, FAIBLE=25

    # Boost bas√© sur la fr√©quence de d√©tection (apr√®s 3 scans minimum)
    freq_boost = self.frequency * 100 if self.scan_count >= 3 else impact

    return freq_boost * (impact / 100)
```

#### Exemple de calcul

| Anomalie | Criticit√© | Impact | D√©tections | Scans | Fr√©quence | Score |
|----------|-----------|--------|------------|-------|-----------|-------|
| DB#1 | CRITIQUE | 100 | 12 | 12 | 100% | **100.0** |
| BR#2 | CRITIQUE | 100 | 10 | 12 | 83% | **83.0** |
| DB#2 | CRITIQUE | 100 | 9 | 12 | 75% | **75.0** |
| DP#2 | MOYEN | 50 | 6 | 6 | 100% | **50.0** |
| DB#4 | √âLEV√â | 75 | 2 | 7 | 29% | **21.4** |

**R√©sultat** : Lors du prochain scan, DB#1 sera test√© en premier, puis BR#2, DB#2, etc.

### 4.7 Gestionnaire de catalogue

```python
class ExtendedCatalogManager:
    def __init__(self, persistence_file: str = "extended_anomaly_stats.json"):
        self.catalog = EXTENDED_CATALOG
        self._load_stats()  # Charge les stats d'apprentissage

    def get_by_dimension(self, dimension: str) -> List[CoreAnomaly]:
        """Filtre par dimension (DB, DP, BR, UP)"""

    def get_top_priority(self, n: int = 10) -> List[CoreAnomaly]:
        """Top N anomalies par score de priorit√©"""

    def update_stats(self, anomaly_id: str, detected: bool):
        """Met √† jour les stats apr√®s un scan"""
        anomaly.scan_count += 1
        if detected:
            anomaly.detection_count += 1
        anomaly.frequency = detection_count / scan_count
        self._save_stats()  # Persiste dans le fichier JSON

    def get_real_detectors(self) -> List[CoreAnomaly]:
        """Retourne uniquement les anomalies avec d√©tecteurs r√©els (non-templates)"""
```

---

## 5. Module ahp_elicitor.py

### 4.1 Description
Impl√©mente l'√©licitation des pond√©rations via la m√©thode AHP (Analytic Hierarchy Process).

### 4.2 Classe AHPElicitor

#### Presets de pond√©rations

```python
PRESET_WEIGHTS = {
    "paie_reglementaire": {
        "w_DB": 0.40,  # Structure critique (calculs l√©gaux)
        "w_DP": 0.30,  # Traitements importants
        "w_BR": 0.30,  # R√®gles m√©tier strictes
        "w_UP": 0.00   # Utilisabilit√© non prioritaire
    },
    "reporting_social": {
        "w_DB": 0.25,
        "w_DP": 0.20,
        "w_BR": 0.30,
        "w_UP": 0.25
    },
    "dashboard_operationnel": {
        "w_DB": 0.10,
        "w_DP": 0.10,
        "w_BR": 0.20,
        "w_UP": 0.60   # Utilisabilit√© prime
    },
    "audit_conformite": {
        "w_DB": 0.35,
        "w_DP": 0.35,
        "w_BR": 0.30,
        "w_UP": 0.00
    },
    "analytics_decisional": {
        "w_DB": 0.20,
        "w_DP": 0.25,
        "w_BR": 0.25,
        "w_UP": 0.30
    }
}
```

---

#### `get_weights_preset(usage_type)`

```python
def get_weights_preset(usage_type: str) -> Dict[str, float]
```

| Param√®tre | Type | Description |
|-----------|------|-------------|
| `usage_type` | `str` | Type d'usage (fuzzy matching) |

**Retourne** :

```python
{
    "w_DB": 0.40,
    "w_DP": 0.30,
    "w_BR": 0.30,
    "w_UP": 0.00,
    "rationale": "Paie r√©glementaire : structure et calculs critiques"
}
```

---

#### `compute_ahp_matrix(comparisons)`

```python
def compute_ahp_matrix(
    comparisons: List[Tuple[str, str, float]]
) -> Dict[str, float]
```

| Param√®tre | Type | Description |
|-----------|------|-------------|
| `comparisons` | `List[Tuple]` | Comparaisons par paires |

**Format des comparaisons** (√©chelle de Saaty) :

| Score | Signification |
|-------|---------------|
| 1 | √âgale importance |
| 3 | Importance mod√©r√©e |
| 5 | Importance forte |
| 7 | Importance tr√®s forte |
| 9 | Importance extr√™me |

**Exemple** :
```python
comparisons = [
    ("DB", "DP", 3),   # DB 3√ó plus important que DP
    ("DB", "BR", 5),   # DB 5√ó plus important que BR
    ("DP", "BR", 2),   # DP 2√ó plus important que BR
    ...
]
weights = ahp.compute_ahp_matrix(comparisons)
# {"w_DB": 0.52, "w_DP": 0.26, "w_BR": 0.15, "w_UP": 0.07}
```

**Algorithme** :
1. Construction matrice 4√ó4 r√©ciproque
2. Calcul vecteur propre principal (np.linalg.eig)
3. Normalisation pour Œ£w = 1.0

---

#### `normalize_weights(weights)`

```python
def normalize_weights(weights: Dict[str, float]) -> Dict[str, float]
```

**Normalise** les poids pour que leur somme = 1.0

---

## 6. Module risk_scorer.py

### 5.1 Description
Calcule les scores de risque contextualis√©s par la combinaison [Attribut √ó Usage].

### 5.2 Classe RiskScorer

#### Seuils de risque

```python
RISK_THRESHOLDS = {
    "CRITIQUE": 0.40,      # ‚â• 40%
    "√âLEV√â": 0.25,         # 25-40%
    "MOYEN": 0.15,         # 15-25%
    "ACCEPTABLE": 0.10,    # 10-15%
    "TR√àS_FAIBLE": 0.00    # < 10%
}
```

---

#### `compute_risk_score(vector_4d, weights)`

```python
def compute_risk_score(
    vector_4d: Dict[str, float],
    weights: Dict[str, float]
) -> float
```

| Param√®tre | Type | Description |
|-----------|------|-------------|
| `vector_4d` | `Dict` | `{"P_DB": 0.99, "P_DP": 0.02, "P_BR": 0.20, "P_UP": 0.10}` |
| `weights` | `Dict` | `{"w_DB": 0.40, "w_DP": 0.30, "w_BR": 0.30, "w_UP": 0.00}` |

**Formule** :
```
R(a, U) = w_DB √ó P_DB + w_DP √ó P_DP + w_BR √ó P_BR + w_UP √ó P_UP
```

**Exemple** :
```python
R = 0.40 √ó 0.99 + 0.30 √ó 0.02 + 0.30 √ó 0.20 + 0.00 √ó 0.10
R = 0.396 + 0.006 + 0.060 + 0.000
R = 0.462  # 46.2% ‚Üí CRITIQUE
```

---

#### `compute_all_scores(vecteurs_4d, weights_by_usage)`

```python
def compute_all_scores(
    vecteurs_4d: Dict[str, Dict],
    weights_by_usage: Dict[str, Dict]
) -> Dict[str, float]
```

**Retourne** : Matrice compl√®te [Attribut √ó Usage]

```python
{
    "Anciennete_Paie": 0.462,
    "Anciennete_Reporting": 0.337,
    "Anciennete_Dashboard": 0.201,
    "Dates_promos_Paie": 0.312,
    ...
}
```

---

#### `compute_impact_business(risk_score, attribut, usage, n_records)`

```python
def compute_impact_business(
    risk_score: float,
    attribut: str,
    usage: str,
    n_records: int = 687
) -> Dict[str, Any]
```

**Retourne** :

```python
{
    "records_affected": 317,           # risk_score √ó n_records
    "impact_financier_mensuel": 15850, # EUR (estimation)
    "severite": "CRITIQUE",
    "actions_recommandees": [
        "Audit imm√©diat de la source",
        "Correction du sch√©ma en base",
        "Mise en place monitoring"
    ]
}
```

---

### 5.3 Fonctions utilitaires

#### `get_top_priorities(scores, top_n)`

```python
def get_top_priorities(
    scores: Dict[str, float],
    top_n: int = 5
) -> List[Dict[str, Any]]
```

**Retourne** : Top N priorit√©s enrichies

```python
[
    {
        "attribut": "Anciennete",
        "usage": "Paie",
        "score": 0.462,
        "severite": "CRITIQUE",
        "color": "red",
        "records_affected": 317,
        "impact_mensuel": 15850,
        "actions": [...]
    },
    ...
]
```

---

## 7. Module lineage_propagator.py

### 6.1 Description
Simule la propagation du risque √† travers les transformations de donn√©es (ETL, enrichissements, etc.).

### 6.2 Classe LineagePropagator

#### `propagate_dimension(P_initial, transformations)`

```python
def propagate_dimension(
    P_initial: float,
    transformations: List[Dict[str, float]]
) -> List[float]
```

| Param√®tre | Type | Description |
|-----------|------|-------------|
| `P_initial` | `float` | Probabilit√© initiale (0-1) |
| `transformations` | `List[Dict]` | `[{"nom": "ETL", "P_add": 0.05}, ...]` |

**Formule (convolution bay√©sienne)** :
```
P_d(N) ‚âà 1 - ‚àè(1 - P_d(i))

# Ou de mani√®re r√©cursive :
P_new = 1 - (1 - P_current) √ó (1 - P_add)
```

**Exemple** :
```python
# P_DP initial = 2%
# Apr√®s ETL (+5%) : P_DP = 1 - (1-0.02)(1-0.05) = 6.9%
# Apr√®s Enrichissement (+8%) : P_DP = 1 - (1-0.069)(1-0.08) = 14.3%
```

---

#### `simulate_pipeline_propagation(vector_4d_source, pipeline)`

```python
def simulate_pipeline_propagation(
    vector_4d_source: Dict[str, Any],
    pipeline: List[Dict]
) -> Dict[str, Any]
```

**Format pipeline** :

```python
pipeline = [
    {
        "nom": "ETL Extraction",
        "P_DB_add": 0.00,
        "P_DP_add": 0.05,
        "P_BR_add": 0.00,
        "P_UP_add": 0.02
    },
    {
        "nom": "Enrichissement M√©tier",
        "P_DB_add": 0.00,
        "P_DP_add": 0.02,
        "P_BR_add": 0.03,
        "P_UP_add": 0.01
    },
    ...
]
```

**Retourne** :

```python
{
    "vector_final": {
        "P_DB": 0.99,   # Inchang√© (pas de P_DB_add)
        "P_DP": 0.285,  # D√©grad√© : 2% ‚Üí 28.5%
        "P_BR": 0.23,   # D√©grad√© : 20% ‚Üí 23%
        "P_UP": 0.15    # D√©grad√© : 10% ‚Üí 15%
    },
    "degradation": {
        "delta_DB": 0.00,
        "delta_DP": +0.265,  # +26.5 points
        "delta_BR": +0.03,
        "delta_UP": +0.05
    },
    "history": [
        {"stage": "Source", "P_DB": 0.99, "P_DP": 0.02, ...},
        {"stage": "ETL", "P_DB": 0.99, "P_DP": 0.069, ...},
        ...
    ]
}
```

---

### 6.3 Fonction utilitaire

#### `simulate_lineage(vector_4d_source, weights_usage, pipeline_config)`

```python
def simulate_lineage(
    vector_4d_source: Dict,
    weights_usage: Dict,
    pipeline_config: List = None  # D√©faut : pipeline paie 4 √©tapes
) -> Dict[str, Any]
```

**Pipeline par d√©faut** :

| √âtape | P_DP_add | P_BR_add | Description |
|-------|----------|----------|-------------|
| ETL Extraction | +5% | +0% | Extraction source |
| Enrichissement | +2% | +2% | Jointures, calculs |
| Agr√©gation Paie | +8% | +0% | Consolidation |
| Calcul Final | +1% | +0% | G√©n√©ration bulletins |

---

## 8. Module comparator.py

### 7.1 Description
Compare l'approche DAMA classique avec l'approche probabiliste contextualis√©e.

### 7.2 Classe DAMACalculator

#### `compute_dama_score(df, column)`

```python
def compute_dama_score(
    df: pd.DataFrame,
    column: str
) -> Dict[str, float]
```

**Dimensions ISO 8000 calcul√©es** :

| Dimension | Formule | Calculable ? |
|-----------|---------|--------------|
| Completeness | `1 - (null_count / total)` | ‚úÖ Oui |
| Consistency | N√©cessite r√®gles de coh√©rence | ‚ùå Non |
| Accuracy | N√©cessite donn√©es de r√©f√©rence | ‚ùå Non |
| Timeliness | N√©cessite r√®gle de fra√Æcheur | ‚ùå Non |
| Validity | N√©cessite domaine de valeurs | ‚ùå Non |
| Uniqueness | `1 - (duplicates / total)` | ‚úÖ Oui |

**Retourne** :

```python
{
    "completeness": 0.68,
    "consistency": None,      # Non calculable
    "accuracy": None,
    "timeliness": None,
    "validity": None,
    "uniqueness": 0.997,
    "score_global": 0.8385,   # Moyenne des calculables
    "dimensions_calculables": 2,
    "dimensions_total": 6,
    "note": "Seulement Completeness et Uniqueness calculables"
}
```

---

### 7.3 Classe Comparator

#### `compare_approaches(df, columns, scores_probabilistes, vecteurs_4d)`

```python
def compare_approaches(
    df: pd.DataFrame,
    columns: List[str],
    scores_probabilistes: Dict[str, float],
    vecteurs_4d: Dict[str, Dict] = None
) -> Dict[str, Any]
```

**Retourne** :

```python
{
    "dama_scores": {...},
    "probabiliste_scores": {...},
    "problemes_masques": [
        {
            "attribut": "Anciennete",
            "type": "DB_masqu√©",
            "P_DB": 0.99,
            "score_DAMA": 0.818,
            "explication": "100% violation DB dilu√© dans score 81.8%"
        }
    ],
    "gains": [
        {
            "categorie": "Quantification incertitude",
            "methode_dama": "Point estimate unique",
            "methode_probabiliste": "Distribution Beta(Œ±,Œ≤) avec IC 95%",
            "gain": "Distingue 10% haute certitude vs 10% haute incertitude"
        },
        ...
    ]
}
```

---

### 7.4 Gains m√©thodologiques quantifi√©s

| Cat√©gorie | DAMA | Probabiliste | Gain |
|-----------|------|--------------|------|
| **Incertitude** | Point estimate | Beta(Œ±,Œ≤) + IC 95% | D√©cisions risque-inform√©es |
| **Contextualisation** | Score unique | Scores par usage | Priorisation ROI |
| **Propagation** | Aucune | Convolution bay√©sienne | D√©tection impact ETL |
| **Dimensions** | 6 ISO agr√©g√©es | 4 causales | Diagnostic cause racine |
| **Apprentissage** | Recalcul complet | Mise √† jour bay√©sienne | Convergence progressive |

**Gains op√©rationnels** :
- -70% faux positifs (50% ‚Üí 15%)
- -60% temps assessment (240h ‚Üí 96h pour 500 attributs)
- ROI corrections : 8-18√ó (vs non calculable DAMA)
- Scalabilit√© : 50k attributs (vs 5k max DAMA)

---

## 9. Application principale app.py

### 8.1 Fonctions utilitaires

#### `get_risk_color(s)`

```python
def get_risk_color(s: float) -> str
```

| Score | Couleur |
|-------|---------|
| ‚â• 0.40 | `#eb3349` (rouge) |
| 0.25-0.40 | `#F2994A` (orange) |
| 0.15-0.25 | `#F2C94C` (jaune) |
| < 0.15 | `#38ef7d` (vert) |

---

#### `explain_with_ai(scope, data, cache_key, max_tokens)`

```python
def explain_with_ai(
    scope: str,           # "vector", "priority", "lineage", "dama", "global"
    data: Dict,           # Donn√©es √† expliquer
    cache_key: str,       # Cl√© de cache
    max_tokens: int = 400
) -> str
```

**Utilise** : API Anthropic (Claude Sonnet 4)

---

#### `create_vector_chart(v)`

```python
def create_vector_chart(v: Dict) -> go.Figure
```

Cr√©e un graphique en barres Plotly pour le vecteur 4D.

---

#### `create_heatmap(scores)`

```python
def create_heatmap(scores: Dict[str, float]) -> go.Figure
```

Cr√©e une heatmap [Attribut √ó Usage] avec palette personnalis√©e.

---

#### `export_excel(results)`

```python
def export_excel(results: Dict) -> str
```

Exporte en Excel avec 3 feuilles :
- **Vecteurs** : Vecteurs 4D par attribut
- **Scores** : Scores de risque
- **Priorites** : Top priorit√©s

---

### 8.2 Variables de session Streamlit

```python
session_state = {
    "df": None,                    # DataFrame charg√©
    "results": None,               # R√©sultats d'analyse
    "analysis_done": False,        # Flag analyse termin√©e
    "anthropic_api_key": "",       # Cl√© API Claude
    "ai_explanations": {},         # Cache explications IA
    "ai_tokens_used": 0,           # Compteur tokens
    "custom_weights": {},          # Pond√©rations personnalis√©es
    "selected_profile": "gouvernance"  # Profil reporting
}
```

---

### 8.3 Onglets de l'application

| Onglet | Ic√¥ne | Description | Utilisation |
|--------|-------|-------------|-------------|
| **Scan** | üîç | D√©tection automatique des anomalies | Premier diagnostic |
| **Dashboard** | üìä | Vue globale, heatmap des risques | Pr√©sentation COMEX |
| **Vecteurs** | üéØ | D√©tail des 4 dimensions par attribut | Diagnostic technique |
| **Priorit√©s** | ‚ö†Ô∏è | Top 5 des urgences √† traiter | Plan d'action |
| **√âlicitation** | üéöÔ∏è | Ajuster les pond√©rations par usage | Personnalisation m√©tier |
| **Lineage** | üîÑ | Impact des transformations ETL | Debug pipeline |
| **DAMA** | üìà | Comparaison avec approche classique | Justification m√©thode |
| **Reporting** | üìã | Rapport personnalis√© par profil | Communication |
| **Aide** | ‚ùì | Guide utilisateur int√©gr√© | Formation |

---

### 8.4 Onglet Reporting - S√©lection multiple d'attributs

L'onglet Reporting permet de g√©n√©rer des rapports personnalis√©s pour **plusieurs attributs** simultan√©ment.

#### Fonctionnalit√©s

```python
# S√©lection multiple d'attributs
attributs_focus = st.multiselect(
    "üìå Attribut(s) √† analyser",
    options=attributs,
    default=[attributs[0]],
    help="S√©lectionne un ou plusieurs attributs pour le rapport"
)
```

#### Structure des donn√©es g√©n√©r√©es

```python
rapport_data = {
    "profil": "üí∞ CFO",
    "usage": "paie_reglementaire",
    "nombre_attributs": 3,
    "attributs_analyses": ["Anciennete", "Salaire", "Grade"],
    "resume_global": {
        "score_moyen": 0.35,
        "score_max": 0.46,
        "score_min": 0.18,
        "attribut_plus_critique": "Anciennete",
        "nb_alertes_critiques": 1
    },
    "ponderations_usage": {
        "w_DB": 0.40, "w_DP": 0.30, "w_BR": 0.20, "w_UP": 0.10
    },
    "detail_par_attribut": [
        {
            "attribut": "Anciennete",
            "score_risque": 0.46,
            "vecteur_4d": {"P_DB": 0.99, "P_DP": 0.02, ...},
            "dimension_critique": {"nom": "DB", "valeur": 0.99},
            "scores_dama": {"completude": 1.0, "unicite": 0.85},
            "priorites": [...]
        },
        ...
    ]
}
```

#### Profils disponibles

| Profil | Description | Focus |
|--------|-------------|-------|
| üí∞ CFO | Chief Financial Officer | Impact financier, ROI |
| üîß Data Engineer | D√©veloppeur / Ing√©nieur | D√©tails techniques, ETL |
| üë• DRH | Directeur RH | Conformit√© sociale |
| üîç Auditeur | Compliance Officer | R√®gles m√©tier, tra√ßabilit√© |
| üìä Gouvernance | Responsable DQ | Vue globale, KPIs |
| ‚ö° Manager Ops | Op√©rationnel | Actions imm√©diates |
| ‚úèÔ∏è Custom | Personnalis√© | Configurable |

---

### 8.5 Guide utilisateur int√©gr√©

Le guide utilisateur est affich√© **d√®s la page d'entr√©e** (avant l'analyse) ET dans l'onglet "Aide" apr√®s analyse.

#### Contenu du guide

1. **En 30 secondes** : Explication de l'outil
2. **DAMA vs Probabiliste** : Comparaison des approches
3. **4 dimensions** : DB, DP, BR, UP expliqu√©es
4. **Code couleur** : Seuils de risque
5. **Onglets** : Description de chaque fonctionnalit√©
6. **3 insights cl√©s** : Points essentiels √† retenir

---

### 8.6 Calcul de l'Unicit√© DAMA

**‚ö†Ô∏è IMPORTANT** : La formule d'unicit√© suit le standard DAMA.

#### Formule

```python
# Unicit√© DAMA = 1 - (nb_lignes_dupliqu√©es / total)
if total > 0:
    duplicated_count = series.duplicated(keep='first').sum()
    uniqueness = 1.0 - (duplicated_count / total)
else:
    uniqueness = 0.0
```

#### Interpr√©tation

| Situation | Exemple | Unicit√© |
|-----------|---------|---------|
| Toutes valeurs uniques | [A, B, C, D, E] | **100%** |
| Quelques doublons | [A, B, A, C, C, C] ‚Üí 3 doublons sur 6 | **50%** |
| Toutes valeurs identiques | [X, X, X, X, X] ‚Üí 4 doublons sur 5 | **20%** |

#### Affichage

```python
# Affichage avec 1 d√©cimale si valeur < 5% pour √©viter "0%"
if dim_value < 0.05 and dim_value > 0:
    display_value = f"{dim_value:.1%}"  # Ex: "0.4%"
else:
    display_value = f"{dim_value:.0%}"  # Ex: "85%"
```

---

## 10. Formules math√©matiques

### 9.1 Distribution Beta

```
Param√®tres :
    Œ± = p √ó n        (succ√®s)
    Œ≤ = (1-p) √ó n    (√©checs)

Esp√©rance :
    E[P] = Œ± / (Œ± + Œ≤)

Variance :
    Var[P] = Œ±Œ≤ / ((Œ±+Œ≤)¬≤(Œ±+Œ≤+1))

Intervalle de confiance 95% :
    IC = [Beta.ppf(0.025, Œ±, Œ≤), Beta.ppf(0.975, Œ±, Œ≤)]
```

### 9.2 Score de risque

```
R(a, U) = Œ£(w_d √ó P_d)

R(a, U) = w_DB √ó P_DB + w_DP √ó P_DP + w_BR √ó P_BR + w_UP √ó P_UP

Contrainte : Œ£w_d = 1.0
```

### 9.3 Propagation Lineage

```
Convolution bay√©sienne :
    P_new = 1 - (1 - P_current) √ó (1 - P_add)

√âquivalent √† :
    P_d(N) ‚âà 1 - ‚àè(1 - P_d(i))
```

### 9.4 Mise √† jour Bay√©sienne

```
Prior :     Beta(Œ±, Œ≤)
Likelihood: Binomiale(k succ√®s, n-k √©checs)
Posterior : Beta(Œ± + (n-k), Œ≤ + k)

O√π k = nombre de nouvelles erreurs observ√©es
```

---

## 11. Guide d'extension

### 10.1 Ajouter une nouvelle dimension

1. **beta_calculator.py** : Ajouter `P_XX` dans `compute_4d_vector()`
2. **ahp_elicitor.py** : Ajouter `w_XX` dans les presets
3. **risk_scorer.py** : Inclure dans la formule de scoring
4. **app.py** : Mettre √† jour les visualisations

### 10.2 Ajouter un nouveau type d'usage

```python
# ahp_elicitor.py
PRESET_WEIGHTS["nouveau_usage"] = {
    "w_DB": 0.25,
    "w_DP": 0.25,
    "w_BR": 0.25,
    "w_UP": 0.25,
    "rationale": "Description du cas d'usage"
}
```

### 10.3 Ajouter une r√®gle m√©tier

```python
# analyzer.py
def detect_business_violations(series, col_name):
    violations = []

    # Nouvelle r√®gle
    if "salaire" in col_name.lower():
        invalid = series[series < 0]
        if len(invalid) > 0:
            violations.append({
                "rule": "salaire_negatif",
                "count": len(invalid),
                "examples": invalid.head(3).tolist()
            })

    return violations
```

### 10.4 Personnaliser le pipeline de lineage

```python
# Cr√©er un pipeline personnalis√©
custom_pipeline = [
    {"nom": "Source DB", "P_DB_add": 0.00, "P_DP_add": 0.01, "P_BR_add": 0.00, "P_UP_add": 0.00},
    {"nom": "API Gateway", "P_DB_add": 0.00, "P_DP_add": 0.03, "P_BR_add": 0.00, "P_UP_add": 0.02},
    {"nom": "Data Lake", "P_DB_add": 0.00, "P_DP_add": 0.02, "P_BR_add": 0.01, "P_UP_add": 0.01},
    {"nom": "ML Pipeline", "P_DB_add": 0.00, "P_DP_add": 0.05, "P_BR_add": 0.02, "P_UP_add": 0.03},
]

result = simulate_lineage(vector_4d, weights, pipeline_config=custom_pipeline)
```

---

### 10.5 Ajouter une nouvelle anomalie au catalogue

```python
# extended_anomaly_catalog.py
CoreAnomaly(
    id="DB#16",
    dimension=Dimension.DB,
    name="Ma nouvelle anomalie",
    description="Description d√©taill√©e",
    criticality=Criticality.√âLEV√â,
    woodall_level="SAMT",
    detector=ma_fonction_detection,
    sql_template="SELECT * FROM {table} WHERE condition",
    example="Exemple d'impact business"
)
```

---

## üìù Changelog

| Version | Date | Modifications |
|---------|------|---------------|
| 1.0 | F√©v 2025 | Version initiale |
| 1.1 | F√©v 2025 | Ajout catalogue anomalies (60 anomalies), syst√®me apprentissage |
| 1.2 | F√©v 2025 | **Reporting multi-attributs** : s√©lection de plusieurs attributs pour g√©n√©ration de rapports |
| 1.2 | F√©v 2025 | **Guide utilisateur** visible d√®s la page d'entr√©e |
| 1.2 | F√©v 2025 | **Correction Unicit√© DAMA** : formule `1 - (doublons/total)` corrig√©e dans tous les modules |
| 1.2 | F√©v 2025 | **CSS contraste** : am√©lioration lisibilit√© dropdowns et menus |

---

*Documentation g√©n√©r√©e pour le Framework Probabiliste DQ*
