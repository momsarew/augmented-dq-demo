"""
Framework Probabiliste DQ - Application Streamlit
Interface web compl√®te pour analyse qualit√© donn√©es

Lancement : streamlit run app.py
"""

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from typing import Dict, List
import sys
import os
from datetime import datetime, timedelta
import time
import json

# Import Anthropic pour LLM
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
import anthropic
import json

# Ajouter path backend
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

# Import moteur
from backend.engine import (
    analyze_dataset,
    compute_all_beta_vectors,
    elicit_weights_auto,
    compute_risk_scores,
    get_top_priorities,
    simulate_lineage,
    compare_dama_vs_probabiliste
)


# ============================================================================
# CONFIGURATION PAGE
# ============================================================================

st.set_page_config(
    page_title="Framework Probabiliste DQ",
    page_icon="üéØ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS Custom
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        background: linear-gradient(90deg, #00d4ff, #0099ff);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: #1e1e1e;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 4px solid #00d4ff;
        margin: 1rem 0;
    }
    .success-box {
        background: #0f3d0f;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #00ff00;
    }
    .warning-box {
        background: #3d2a0f;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #ff9900;
    }
    .danger-box {
        background: #3d0f0f;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #ff0000;
    }
</style>
""", unsafe_allow_html=True)


# ============================================================================
# SESSION STATE (pour conserver donn√©es entre interactions)
# ============================================================================

if 'df' not in st.session_state:
    st.session_state.df = None
if 'results' not in st.session_state:
    st.session_state.results = None
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False


# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================

def get_risk_color(score: float) -> str:
    """Retourne couleur selon score risque"""
    if score >= 0.40:
        return "#ff0000"  # Rouge
    elif score >= 0.25:
        return "#ff9900"  # Orange
    elif score >= 0.15:
        return "#ffcc00"  # Jaune
    elif score >= 0.10:
        return "#90ee90"  # Vert clair
    else:
        return "#00ff00"  # Vert


def call_claude_api(messages: List[Dict], system_prompt: str = None) -> str:
    """
    Appelle Claude API pour dialogue √©licitation
    
    Args:
        messages: Historique conversation [{"role": "user/assistant", "content": "..."}]
        system_prompt: Prompt syst√®me optionnel
    
    Returns:
        R√©ponse de Claude
    """
    try:
        # V√©rifier si cl√© API configur√©e
        api_key = st.session_state.get('claude_api_key', None)
        
        if not api_key:
            return """‚ö†Ô∏è **Cl√© API Claude non configur√©e**
            
Pour activer le dialogue IA r√©el, configure ta cl√© API dans la sidebar :
1. Va sur https://console.anthropic.com/
2. Cr√©e une cl√© API
3. Colle-la dans le champ "Cl√© API Claude" (sidebar)

**En attendant, voici une r√©ponse simul√©e** :
‚úÖ Not√© ! J'augmente **w_DB √† 40%** pour la Paie.

Les donn√©es structurelles (VARCHAR au lieu DECIMAL) sont effectivement critiques pour la conformit√© r√©glementaire de la paie.

Quelle est la **deuxi√®me dimension la plus importante** pour ce cas d'usage ?"""
        
        # Import dynamique (seulement si cl√© pr√©sente)
        try:
            import anthropic
        except ImportError:
            return """‚ö†Ô∏è **Module Anthropic non install√©**
            
Pour installer : `pip install anthropic`

R√©ponse simul√©e en attendant..."""
        
        # Appeler API Claude
        client = anthropic.Anthropic(api_key=api_key)
        
        # Prompt syst√®me par d√©faut
        if not system_prompt:
            system_prompt = """Tu es un expert en Data Quality et en sciences de la d√©cision. 
Tu aides √† √©liciter les pond√©rations AHP pour un framework probabiliste de qualit√© des donn√©es.

Tu poses des questions progressives pour comprendre l'importance relative des 4 dimensions :
- [DB] Database : Contraintes structurelles (types, nullable, cl√©s)
- [DP] Data Processing : Erreurs transformations ETL
- [BR] Business Rules : Violations r√®gles m√©tier
- [UP] Usage-fit : Inad√©quation contextuelle

Ton style :
- Questions courtes et cibl√©es
- Exemples concrets
- Validation des choix utilisateur
- Propositions de valeurs Beta si pertinent

Contexte actuel : √âlicitation pour usage "Paie r√©glementaire" (conformit√© l√©gale stricte)."""
        
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1000,
            system=system_prompt,
            messages=messages
        )
        
        # Extraire texte r√©ponse
        return response.content[0].text
        
    except Exception as e:
        return f"""‚ö†Ô∏è **Erreur API Claude** : {str(e)}

V√©rifie que :
- Ta cl√© API est valide
- Tu as des cr√©dits disponibles
- Ta connexion internet fonctionne

R√©ponse simul√©e en attendant..."""


def get_risk_color(score: float) -> str:
    """Retourne couleur selon score risque"""
    if score >= 0.40:
        return "#ff0000"  # Rouge
    elif score >= 0.25:
        return "#ff9900"  # Orange
    elif score >= 0.15:
        return "#ffcc00"  # Jaune
    elif score >= 0.10:
        return "#90ee90"  # Vert clair
    else:
        return "#00ff00"  # Vert


def create_vector_chart(vector_4d: Dict) -> go.Figure:
    """Cr√©e graphique barres vecteur 4D"""
    dimensions = ['DB', 'DP', 'BR', 'UP']
    values = [
        vector_4d.get('P_DB', 0) * 100,
        vector_4d.get('P_DP', 0) * 100,
        vector_4d.get('P_BR', 0) * 100,
        vector_4d.get('P_UP', 0) * 100
    ]
    
    colors = [get_risk_color(v/100) for v in values]
    
    fig = go.Figure(data=[
        go.Bar(
            x=dimensions,
            y=values,
            marker=dict(color=colors),
            text=[f"{v:.1f}%" for v in values],
            textposition='outside'
        )
    ])
    
    fig.update_layout(
        title="Vecteur Qualit√© 4D",
        xaxis_title="Dimensions",
        yaxis_title="Taux erreur (%)",
        height=400,
        template="plotly_dark"
    )
    
    return fig


def export_to_excel(results: Dict, output_path: str = "resultats_framework_dq.xlsx"):
    """
    Exporte r√©sultats complets vers Excel multi-onglets
    """
    from datetime import datetime
    import openpyxl
    from openpyxl.styles import Font, PatternFill, Alignment
    
    # Ajouter timestamp au nom fichier
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f"resultats_framework_dq_{timestamp}.xlsx"
    
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        
        # ====================================================================
        # ONGLET 1 : VECTEURS 4D
        # ====================================================================
        vecteurs_data = []
        for attr, vector in results['vecteurs_4d'].items():
            vecteurs_data.append({
                'Attribut': attr,
                'P_DB': vector.get('P_DB', 0),
                'alpha_DB': vector.get('alpha_DB', 0),
                'beta_DB': vector.get('beta_DB', 0),
                'std_DB': vector.get('std_DB', 0),
                'ci_lower_DB': vector.get('ci_lower_DB', 0),
                'ci_upper_DB': vector.get('ci_upper_DB', 0),
                'P_DP': vector.get('P_DP', 0),
                'alpha_DP': vector.get('alpha_DP', 0),
                'beta_DP': vector.get('beta_DP', 0),
                'std_DP': vector.get('std_DP', 0),
                'ci_lower_DP': vector.get('ci_lower_DP', 0),
                'ci_upper_DP': vector.get('ci_upper_DP', 0),
                'P_BR': vector.get('P_BR', 0),
                'alpha_BR': vector.get('alpha_BR', 0),
                'beta_BR': vector.get('beta_BR', 0),
                'std_BR': vector.get('std_BR', 0),
                'ci_lower_BR': vector.get('ci_lower_BR', 0),
                'ci_upper_BR': vector.get('ci_upper_BR', 0),
                'P_UP': vector.get('P_UP', 0),
                'alpha_UP': vector.get('alpha_UP', 0),
                'beta_UP': vector.get('beta_UP', 0),
                'std_UP': vector.get('std_UP', 0),
                'ci_lower_UP': vector.get('ci_lower_UP', 0),
                'ci_upper_UP': vector.get('ci_upper_UP', 0)
            })
        
        df_vecteurs = pd.DataFrame(vecteurs_data)
        df_vecteurs.to_excel(writer, sheet_name='Vecteurs_4D', index=False)
        
        # ====================================================================
        # ONGLET 2 : SCORES RISQUE
        # ====================================================================
        scores_data = []
        for key, score in results['scores'].items():
            parts = key.rsplit('_', 1)
            attribut = parts[0] if len(parts) > 1 else key
            usage = parts[1] if len(parts) > 1 else "Unknown"
            
            scores_data.append({
                'Attribut': attribut,
                'Usage': usage,
                'Score_Risque': score,
                'Score_Pourcent': f"{score:.1%}",
                'Criticit√©': 'CRITIQUE' if score > 0.4 else '√âLEV√â' if score > 0.25 else 'MOYEN' if score > 0.15 else 'ACCEPTABLE'
            })
        
        df_scores = pd.DataFrame(scores_data)
        df_scores_pivot = df_scores.pivot(index='Attribut', columns='Usage', values='Score_Risque')
        df_scores_pivot.to_excel(writer, sheet_name='Scores_Risque')
        
        # ====================================================================
        # ONGLET 3 : POND√âRATIONS USAGES
        # ====================================================================
        weights_data = []
        for usage_name, weights in results['weights'].items():
            weights_data.append({
                'Usage': usage_name,
                'w_DB': weights.get('w_DB', 0),
                'w_DP': weights.get('w_DP', 0),
                'w_BR': weights.get('w_BR', 0),
                'w_UP': weights.get('w_UP', 0),
                'Rationale': weights.get('rationale', '')
            })
        
        df_weights = pd.DataFrame(weights_data)
        df_weights.to_excel(writer, sheet_name='Pond√©rations_AHP', index=False)
        
        # ====================================================================
        # ONGLET 4 : PRIORIT√âS ACTIONS
        # ====================================================================
        priorities_data = []
        for i, priority in enumerate(results['top_priorities'], 1):
            priorities_data.append({
                'Rang': i,
                'Attribut': priority['attribut'],
                'Usage': priority['usage'],
                'Score_Risque': priority['score'],
                'Score_Pourcent': f"{priority['score']:.1%}",
                'S√©v√©rit√©': priority['severite'],
                'Enregistrements_Affect√©s': priority['records_affected'],
                'Impact_Mensuel_‚Ç¨': priority['impact_mensuel'],
                'Action_Principale': priority['actions'][0] if priority.get('actions') else ''
            })
        
        df_priorities = pd.DataFrame(priorities_data)
        df_priorities.to_excel(writer, sheet_name='Priorit√©s_Actions', index=False)
        
        # ====================================================================
        # ONGLET 5 : LINEAGE (si disponible)
        # ====================================================================
        if results.get('lineage'):
            lineage = results['lineage']
            lineage_data = []
            
            for step in lineage['history']:
                lineage_data.append({
                    '√âtape': step['etape'],
                    'P_DB': step['P_DB'],
                    'P_DP': step['P_DP'],
                    'P_BR': step['P_BR'],
                    'P_UP': step['P_UP']
                })
            
            df_lineage = pd.DataFrame(lineage_data)
            df_lineage.to_excel(writer, sheet_name='Lineage_Propagation', index=False)
            
            # M√©triques lineage
            lineage_summary = pd.DataFrame([{
                'Risque_Source': lineage['risk_source'],
                'Risque_Final': lineage['risk_final'],
                'Delta_Absolu': lineage['delta']['delta_absolute'],
                'Delta_Relatif': lineage['delta']['delta_relative'],
                'Interpr√©tation': lineage['delta']['interpretation']
            }])
            lineage_summary.to_excel(writer, sheet_name='Lineage_Summary', index=False)
        
        # ====================================================================
        # ONGLET 6 : COMPARAISON DAMA
        # ====================================================================
        dama_data = []
        for attr, dama_score in results['comparaison']['dama_scores'].items():
            dama_data.append({
                'Attribut': attr,
                'Score_DAMA_Global': dama_score['score_global'],
                'DAMA_Completeness': dama_score.get('completeness', 0),
                'DAMA_Consistency': dama_score.get('consistency', 0),
                'DAMA_Accuracy': dama_score.get('accuracy', 0),
                'DAMA_Validity': dama_score.get('validity', 0)
            })
        
        df_dama = pd.DataFrame(dama_data)
        df_dama.to_excel(writer, sheet_name='Comparaison_DAMA', index=False)
        
        # Probl√®mes masqu√©s
        if results['comparaison'].get('problemes_masques'):
            problemes_data = []
            for pb in results['comparaison']['problemes_masques']:
                problemes_data.append({
                    'Attribut': pb['attribut'],
                    'Type_Probl√®me': pb['type'],
                    'Explication': pb['explication']
                })
            
            df_problemes = pd.DataFrame(problemes_data)
            df_problemes.to_excel(writer, sheet_name='Probl√®mes_Masqu√©s_DAMA', index=False)
    
    return output_path


def export_to_csv(results: Dict, output_dir: str = "."):
    """
    Exporte r√©sultats en plusieurs fichiers CSV
    """
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    files_created = []
    
    # Vecteurs 4D
    vecteurs_data = []
    for attr, vector in results['vecteurs_4d'].items():
        vecteurs_data.append({
            'Attribut': attr,
            **{k: v for k, v in vector.items()}
        })
    
    df_vecteurs = pd.DataFrame(vecteurs_data)
    file_vecteurs = f"{output_dir}/vecteurs_4d_{timestamp}.csv"
    df_vecteurs.to_csv(file_vecteurs, index=False)
    files_created.append(file_vecteurs)
    
    # Scores
    scores_data = []
    for key, score in results['scores'].items():
        parts = key.rsplit('_', 1)
        scores_data.append({
            'Attribut': parts[0] if len(parts) > 1 else key,
            'Usage': parts[1] if len(parts) > 1 else "Unknown",
            'Score_Risque': score
        })
    
    df_scores = pd.DataFrame(scores_data)
    file_scores = f"{output_dir}/scores_risque_{timestamp}.csv"
    df_scores.to_csv(file_scores, index=False)
    files_created.append(file_scores)
    
    # Priorit√©s
    df_priorities = pd.DataFrame(results['top_priorities'])
    file_priorities = f"{output_dir}/priorites_{timestamp}.csv"
    df_priorities.to_csv(file_priorities, index=False)
    files_created.append(file_priorities)
    
    return files_created


def generate_ai_comment(context: str, data: Dict, api_key: str) -> str:
    """
    G√©n√®re un commentaire IA contextualis√© avec Claude
    
    Args:
        context: Type de section (heatmap, vector, priority, lineage, dama)
        data: Donn√©es √† commenter
        api_key: Cl√© API Anthropic
    
    Returns:
        Commentaire IA en texte
    """
    try:
        client = anthropic.Anthropic(api_key=api_key)
        
        # Construire prompt selon contexte
        prompts = {
            "heatmap": f"""Tu es un expert Data Quality. Analyse cette matrice de scores risque et fournis un commentaire de 3-4 phrases maximum.

Donn√©es: {json.dumps(data, indent=2)}

Concentre-toi sur:
1. Les patterns principaux (attributs/usages les plus risqu√©s)
2. Les diff√©rences de scores selon contexte (m√™me attribut, usages diff√©rents)
3. Une recommandation prioritaire

Ton style: Direct, p√©dagogique, actionnable. Utilise "Je remarque que...", "Cela signifie...", "Je recommande...".""",

            "vector": f"""Tu es un expert Data Quality. Commente ce vecteur 4D Beta et explique sa signification en 3-4 phrases maximum.

Attribut: {data.get('attribut', 'Unknown')}
Vecteurs:
- P_DB = {data.get('P_DB', 0):.1%} (Database)
- P_DP = {data.get('P_DP', 0):.1%} (Data Processing)  
- P_BR = {data.get('P_BR', 0):.1%} (Business Rules)
- P_UP = {data.get('P_UP', 0):.1%} (Usage-fit)

Explique:
1. Quelle dimension domine et pourquoi
2. Ce que r√©v√®le la distribution Beta sous-jacente (confiance)
3. Implication op√©rationnelle concr√®te

Style: Technique mais accessible, p√©dagogique.""",

            "priority": f"""Tu es un expert Data Quality. Commente cette priorit√© d'action en 3-4 phrases maximum.

Priorit√©: {data.get('attribut', 'Unknown')} √ó {data.get('usage', 'Unknown')}
Score risque: {data.get('score', 0):.1%}
S√©v√©rit√©: {data.get('severite', 'Unknown')}
Impact: {data.get('records_affected', 0)} enregistrements, {data.get('impact_mensuel', 0):,}‚Ç¨/mois

Fournis:
1. Pourquoi ce combo attribut√óusage est critique
2. Recommandation d'action imm√©diate concr√®te
3. Estimation ROI si correction

Style: Consultant senior, actionnable, chiffr√©.""",

            "lineage": f"""Tu es un expert Data Quality. Commente cette propagation de risque en 3-4 phrases maximum.

Propagation:
- Risque initial: {data.get('risk_source', 0):.1%}
- Risque final: {data.get('risk_final', 0):.1%}
- Delta: {data.get('delta_absolute', 0):.1%} points
- √âtapes pipeline: {len(data.get('history', []))}

Explique:
1. Comment le risque s'est aggrav√© dans le pipeline
2. Quelle √©tape a le plus d√©grad√© (si visible)
3. Gain d√©tection proactive (9h vs 3 semaines)

Style: Analyse forensique, factuelle, impactante.""",

            "dama": f"""Tu es un expert Data Quality. Compare les approches DAMA vs Probabiliste en 3-4 phrases maximum.

Comparaison:
- Score DAMA moyen: {data.get('dama_avg', 0):.1%}
- Probl√®mes masqu√©s: {data.get('masked_count', 0)}
- Gains m√©thodologiques: {len(data.get('gains', []))}

Synth√©tise:
1. Pourquoi DAMA masque certains probl√®mes critiques
2. L'avantage principal du framework probabiliste
3. Impact business concret (-70% faux positifs, ROI 8-18√ó)

Style: Comparatif mais objectif, factuel, business-oriented."""
        }
        
        prompt = prompts.get(context, prompts["heatmap"])
        
        # Appel API
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=500,
            system="Tu es un expert Data Quality qui commente des analyses avec un style direct, p√©dagogique et actionnable. Toujours 3-4 phrases maximum, sans jargon inutile.",
            messages=[{"role": "user", "content": prompt}]
        )
        
        return response.content[0].text
        
    except Exception as e:
        return f"‚ö†Ô∏è Erreur g√©n√©ration commentaire : {str(e)}"


def create_heatmap(scores: Dict) -> go.Figure:
    """Cr√©e heatmap scores [Attribut √ó Usage]"""
    # Parser scores en matrice
    attributs = set()
    usages = set()
    
    for key in scores.keys():
        parts = key.rsplit('_', 1)
        if len(parts) == 2:
            attributs.add(parts[0])
            usages.add(parts[1])
    
    attributs = sorted(list(attributs))
    usages = sorted(list(usages))
    
    # Construire matrice
    matrix = []
    for attr in attributs:
        row = []
        for usage in usages:
            key = f"{attr}_{usage}"
            row.append(scores.get(key, 0) * 100)
        matrix.append(row)
    
    fig = go.Figure(data=go.Heatmap(
        z=matrix,
        x=usages,
        y=attributs,
        colorscale='RdYlGn_r',
        text=[[f"{v:.1f}%" for v in row] for row in matrix],
        texttemplate='%{text}',
        textfont={"size": 12},
        colorbar=dict(title="Risque (%)")
    ))
    
    fig.update_layout(
        title="Matrice Scores Risque [Attribut √ó Usage]",
        height=400,
        template="plotly_dark"
    )
    
    return fig


# ============================================================================
# HEADER
# ============================================================================

st.markdown('<div class="main-header">üéØ Framework Probabiliste DQ</div>', unsafe_allow_html=True)

st.markdown("""
<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 2rem; border-radius: 15px; margin-bottom: 2rem;">
    <h2 style="color: white; margin: 0 0 1rem 0;">üìä D√©mo Interactive - Proof of Concept</h2>
    <p style="color: white; font-size: 1.1rem; margin: 0;">
        <strong>De 240h d'Assessment manuel √† 30 min de Dialogue IA ‚Äî Gain 480√ó</strong>
    </p>
</div>
""", unsafe_allow_html=True)

# Descriptif projet
with st.expander("üìã √Ä propos de cette d√©mo", expanded=False):
    st.markdown("""
    ### üéØ Contexte du Projet
    
    Cette d√©mo pr√©sente un **framework r√©volutionnaire de Data Quality** d√©velopp√© dans le cadre d'un projet de recherche appliqu√©e Big 4. 
    L'objectif : **transformer radicalement l'approche traditionnelle d'assessment qualit√© donn√©es**.
    
    ### üî¨ Probl√©matique adress√©e
    
    **Approches traditionnelles (DAMA-DMBOK, ISO 8000)** :
    - ‚è±Ô∏è **240 heures** d'√©licitation manuelle des r√®gles m√©tier par usage
    - üéØ **R√®gles binaires** (Pass/Fail) inadapt√©es aux donn√©es imparfaites
    - üìä **Scores moyenn√©s** qui masquent les probl√®mes critiques
    - üîç **D√©tection r√©active** : 3 semaines pour identifier un incident
    - üí∞ **ROI faible** : ratio co√ªt/b√©n√©fice d√©favorable
    
    ### üí° Notre Solution : Framework Probabiliste
    
    **Paradigme Bay√©sien + Sciences de la D√©cision** :
    - üé≤ **Distributions Beta** au lieu de scores binaires ‚Üí Mod√©lisation incertitude
    - üìê **4 dimensions causales** (DB-DP-BR-UP) au lieu de 6 g√©n√©riques
    - üéØ **Contextualisation usage** : scores diff√©renci√©s par usage m√©tier
    - ü§ñ **IA conversationnelle** : √©licitation assist√©e en 12 minutes
    - üîî **Surveillance proactive** : d√©tection incidents en 9 heures
    
    ### üë• Acteurs Engag√©s
    
    **√âquipe Projet** :
    - **Porteur de projet** : Thierno Diaw, Senior Manager Data Governance (Big 4)
    - **Sponsor acad√©mique** : Professeur en Sciences de la D√©cision (validation m√©thodologique)
    - **√âquipe technique** : 2 d√©veloppeurs Python/React (impl√©mentation POC)
    - **Partenaires m√©tier** : DRH + DSI d'un Groupe CAC40 (cas d'usage RH)
    
    **Comit√© de pilotage** :
    - Direction Data & Analytics (Big 4)
    - Direction Innovation & R&D
    - Responsable Offre Data Quality
    
    ### üìä Cas d'Usage D√©mo
    
    **Dataset : Donn√©es RH** (687 enregistrements)
    - Anciennet√© (erreur structurelle VARCHAR au lieu DECIMAL)
    - Dates promotions (252 valeurs manquantes)
    - Niveaux hi√©rarchiques (nomenclature incoh√©rente)
    - Dates contrats (violations r√®gles m√©tier)
    
    **Usages m√©tier test√©s** :
    - Paie r√©glementaire (conformit√© l√©gale stricte)
    - Reporting Social CSE (coh√©rence m√©tier prime)
    - Dashboard op√©rationnel (flexibilit√© contextuelle)
    
    ### üéØ Objectifs de la D√©mo
    
    **Prouver 3 hypoth√®ses** :
    1. ‚è±Ô∏è **Gain temps** : √âlicitation 12 min vs 240h (ratio 480√ó)
    2. üí∞ **Gain ROI** : 8-18√ó vs approches traditionnelles
    3. üéØ **Gain pr√©cision** : -70% faux positifs, d√©tection 9h vs 3 semaines
    
    ### üöÄ Navigation D√©mo
    
    **7 onglets interactifs** :
    1. **üìä Dashboard** : Analyse compl√®te avec export Excel/CSV
    2. **üéØ Vecteurs 4D** : Distributions Beta par dimension
    3. **‚ö†Ô∏è Priorit√©s** : Top 5 actions class√©es par ROI
    4. **üîÑ Lineage** : Propagation risque le long des pipelines
    5. **üìà Comparaison DAMA** : Benchmark vs m√©thodes traditionnelles
    6. **üí¨ √âlicitation IA** : Dialogue interactif 12 minutes
    7. **üîî Surveillance** : D√©tection proactive incidents 9h
    
    ### üìà R√©sultats Attendus
    
    **Gains op√©rationnels** :
    - üéØ **-70% faux positifs** (pr√©cision contextualis√©e)
    - ‚è±Ô∏è **-60% temps assessment** (automatisation + IA)
    - üí∞ **8-18√ó ROI** vs m√©thodes actuelles
    - üîî **-95% temps d√©tection** incidents (9h vs 3 semaines)
    
    **Livrables d√©mo** :
    - ‚úÖ Application Streamlit fonctionnelle (cette d√©mo)
    - ‚úÖ Dataset RH r√©el anonymis√© (687 lignes)
    - ‚úÖ 3 sc√©narios interactifs (Dashboard, √âlicitation, Surveillance)
    - ‚úÖ Export Excel avec 6 onglets de calculs d√©taill√©s
    - ‚úÖ Documentation technique compl√®te
    
    ### üéì Publications Acad√©miques
    
    **Article soumis** :
    - Titre : *"Bayesian Framework for Context-Aware Data Quality Assessment"*
    - Conf√©rence : IEEE International Conference on Big Data Quality
    - Statut : Under review (Q1 2025)
    
    ### üìû Contact
    
    Pour plus d'informations sur le framework ou d√©ploiement en production :
    - **Email** : thierno.diaw@big4consulting.com
    - **LinkedIn** : Thierno Diaw - Senior Manager Data Governance
    """)

st.markdown("---")


# ============================================================================
# SIDEBAR - CONFIGURATION
# ============================================================================

with st.sidebar:
    st.header("‚öôÔ∏è Configuration")
    
    # Configuration API Claude
    st.subheader("üîë API Claude (Anthropic)")
    
    # Tenter de charger cl√© API depuis Streamlit secrets (Cloud) ou variable env
    api_key_default = ""
    try:
        # Streamlit Cloud secrets
        api_key_default = st.secrets.get("ANTHROPIC_API_KEY", "")
    except:
        # Variable environnement locale
        api_key_default = os.getenv("ANTHROPIC_API_KEY", "")
    
    api_key_input = st.text_input(
        "Cl√© API Claude",
        type="password",
        value=api_key_default,
        help="N√©cessaire pour le dialogue IA dans l'onglet √âlicitation. Obtenir une cl√© : https://console.anthropic.com/",
        placeholder="sk-ant-api03-..."
    )
    
    if api_key_input:
        st.session_state.anthropic_api_key = api_key_input
        st.success("‚úÖ Cl√© API configur√©e")
    else:
        if 'anthropic_api_key' not in st.session_state:
            st.warning("‚ö†Ô∏è Onglet √âlicitation IA n√©cessite une cl√© API")
            with st.expander("üí° Comment obtenir une cl√© ?"):
                st.markdown("""
                1. Va sur https://console.anthropic.com/
                2. Cr√©er un compte (gratuit)
                3. Dans "API Keys", clique "Create Key"
                4. Copie la cl√© et colle-la ci-dessus
                
                **Cr√©dit gratuit** : 5$ offerts pour tester !
                """)
    
    st.markdown("---")
    
    # Upload fichier
    st.subheader("1Ô∏è‚É£ Upload Dataset")
    uploaded_file = st.file_uploader(
        "Choisir CSV ou Excel",
        type=['csv', 'xlsx', 'xls'],
        help="Fichier avec donn√©es RH (Anciennet√©, Dates, Niveaux, etc.)"
    )
    
    # Ou charger dataset d√©mo
    use_demo = st.checkbox("üìä Utiliser dataset d√©mo (687 lignes RH)")
    
    # Si fichier upload√© ou d√©mo
    if uploaded_file or use_demo:
        # Charger donn√©es
        if use_demo:
            # Dataset d√©mo
            st.session_state.df = pd.DataFrame({
                'Anciennete': ['7,21', '3,45', '12,08', '5,67', '9,12'] * 137 + ['7,21'] * 2,
                'Dates_promos': [None] * 252 + ['01/01/2020', '15/06/2021'] * 217 + ['01/01/2020'] * 3,
                'Date_debut_contrat': ['2015-03-01'] * 687,
                'LEVEL_ACN': ['MGR7', 'CON9', 'AN10', 'SMR6'] * 171 + ['MGR7'] * 3
            })
            st.success("‚úÖ Dataset d√©mo charg√© (687 lignes)")
        else:
            # Fichier upload√©
            if uploaded_file.name.endswith('.csv'):
                st.session_state.df = pd.read_csv(uploaded_file)
            else:
                st.session_state.df = pd.read_excel(uploaded_file)
            st.success(f"‚úÖ {uploaded_file.name} charg√© ({len(st.session_state.df)} lignes)")
        
        # Afficher preview
        with st.expander("üëÅÔ∏è Preview donn√©es"):
            st.dataframe(st.session_state.df.head(5))
        
        # S√©lection colonnes
        st.subheader("2Ô∏è‚É£ Colonnes √† analyser")
        all_columns = st.session_state.df.columns.tolist()
        
        # Suggestions automatiques
        suggested = [col for col in all_columns if any(
            kw in col.lower() for kw in ['anciennete', 'date', 'level', 'salaire', 'prime', 'matricule']
        )]
        
        selected_columns = st.multiselect(
            "S√©lectionner colonnes",
            options=all_columns,
            default=suggested if suggested else all_columns[:3],
            help="Colonnes critiques pour analyse qualit√©"
        )
        
        # Configuration usages
        st.subheader("3Ô∏è‚É£ Usages m√©tier")
        
        # Usages presets
        preset_usage_types = {
            "Paie r√©glementaire": "paie_reglementaire",
            "Reporting Social (CSE)": "reporting_social",
            "Dashboard Op√©rationnel": "dashboard_operationnel",
            "Audit Conformit√©": "audit_conformite",
            "Analytics D√©cisionnel": "analytics_decisional"
        }
        
        # Stocker usages personnalis√©s dans session state
        if 'custom_usages' not in st.session_state:
            st.session_state.custom_usages = {}
        
        # Combiner presets + custom
        all_usage_types = {**preset_usage_types, **st.session_state.custom_usages}
        
        # Bouton ajouter usage personnalis√©
        col_add1, col_add2 = st.columns([3, 1])
        with col_add1:
            st.markdown("**Usages disponibles**")
        with col_add2:
            add_usage = st.button("‚ûï Ajouter", use_container_width=True, help="Cr√©er un usage m√©tier personnalis√©")
        
        # Formulaire ajout usage
        if add_usage or 'show_add_form' in st.session_state and st.session_state.show_add_form:
            st.session_state.show_add_form = True
            
            with st.expander("‚ûï Nouvel usage personnalis√©", expanded=True):
                with st.form("add_custom_usage_form"):
                    st.markdown("### Cr√©er un usage m√©tier")
                    
                    usage_name = st.text_input(
                        "Nom de l'usage",
                        placeholder="Ex: Reporting URSSAF, Pilotage RH, Audit CNIL...",
                        help="Nom descriptif de votre usage m√©tier"
                    )
                    
                    st.markdown("#### Pond√©rations AHP")
                    st.caption("D√©finir l'importance de chaque dimension (total = 100%)")
                    
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        w_db_new = st.slider("DB", 0, 100, 25, 5, key="new_db", help="Database: Contraintes structurelles")
                    with col2:
                        w_dp_new = st.slider("DP", 0, 100, 25, 5, key="new_dp", help="Data Processing: Transformations ETL")
                    with col3:
                        w_br_new = st.slider("BR", 0, 100, 25, 5, key="new_br", help="Business Rules: R√®gles m√©tier")
                    with col4:
                        w_up_new = st.slider("UP", 0, 100, 25, 5, key="new_up", help="Usage-fit: Ad√©quation contextuelle")
                    
                    total_new = w_db_new + w_dp_new + w_br_new + w_up_new
                    
                    if total_new == 100:
                        st.success(f"‚úÖ Somme = {total_new}%")
                    else:
                        st.error(f"‚ö†Ô∏è Somme = {total_new}% (doit √™tre 100%)")
                    
                    rationale = st.text_area(
                        "Justification (optionnel)",
                        placeholder="Ex: Priorit√© conformit√© l√©gale, peu de flexibilit√© contextuelle...",
                        help="Expliquer pourquoi ces pond√©rations"
                    )
                    
                    col_btn1, col_btn2 = st.columns(2)
                    
                    with col_btn1:
                        submitted = st.form_submit_button("‚úÖ Cr√©er usage", type="primary", use_container_width=True)
                    
                    with col_btn2:
                        cancelled = st.form_submit_button("‚ùå Annuler", use_container_width=True)
                    
                    if submitted:
                        if not usage_name:
                            st.error("‚ö†Ô∏è Le nom de l'usage est obligatoire")
                        elif usage_name in all_usage_types:
                            st.error(f"‚ö†Ô∏è Un usage '{usage_name}' existe d√©j√†")
                        elif total_new != 100:
                            st.error("‚ö†Ô∏è La somme des pond√©rations doit √™tre 100%")
                        else:
                            # Ajouter usage personnalis√©
                            st.session_state.custom_usages[usage_name] = {
                                'type': 'custom',
                                'w_DB': w_db_new / 100,
                                'w_DP': w_dp_new / 100,
                                'w_BR': w_br_new / 100,
                                'w_UP': w_up_new / 100,
                                'rationale': rationale if rationale else f"Usage personnalis√© : {usage_name}"
                            }
                            st.session_state.show_add_form = False
                            st.success(f"‚úÖ Usage '{usage_name}' cr√©√© avec succ√®s !")
                            st.rerun()
                    
                    if cancelled:
                        st.session_state.show_add_form = False
                        st.rerun()
        
        # S√©lection usages (presets + custom)
        selected_usages = st.multiselect(
            "S√©lectionner usages",
            options=list(all_usage_types.keys()),
            default=[k for k in list(all_usage_types.keys())[:2] if k in all_usage_types],
            help="Contextes m√©tier pour scoring risque"
        )
        
        # Afficher usages personnalis√©s avec option suppression
        if st.session_state.custom_usages:
            with st.expander("üóëÔ∏è G√©rer usages personnalis√©s", expanded=False):
                for custom_name, custom_data in list(st.session_state.custom_usages.items()):
                    col1, col2 = st.columns([4, 1])
                    with col1:
                        st.write(f"**{custom_name}**")
                        st.caption(f"DB={custom_data['w_DB']:.0%}, DP={custom_data['w_DP']:.0%}, BR={custom_data['w_BR']:.0%}, UP={custom_data['w_UP']:.0%}")
                    with col2:
                        if st.button("üóëÔ∏è", key=f"delete_{custom_name}", help=f"Supprimer {custom_name}"):
                            del st.session_state.custom_usages[custom_name]
                            # Retirer de s√©lection si pr√©sent
                            if custom_name in selected_usages:
                                selected_usages.remove(custom_name)
                            st.rerun()
                    st.markdown("---")
        
        # Mode √©dition pond√©rations
        if selected_usages:
            from backend.engine.ahp_elicitor import AHPElicitor
            elicitor = AHPElicitor()
            
            # Stocker pond√©rations personnalis√©es dans session state
            if 'custom_weights' not in st.session_state:
                st.session_state.custom_weights = {}
            
            st.markdown("---")
            st.markdown("### ‚öôÔ∏è Configuration pond√©rations")
            
            mode = st.radio(
                "Mode configuration",
                ["üìä Utiliser valeurs par d√©faut", "‚úèÔ∏è Ajuster finement les pond√©rations"],
                help="Par d√©faut = valeurs expertes, Ajuster = modifier manuellement"
            )
            
            if mode == "‚úèÔ∏è Ajuster finement les pond√©rations":
                st.info("üí° Ajuste les pond√©rations avec les sliders. La somme doit faire 100%.")
                
                for usage_name in selected_usages:
                    # R√©cup√©rer valeurs par d√©faut
                    if usage_name not in st.session_state.custom_weights:
                        # Si preset, charger preset
                        if usage_name in preset_usage_types:
                            usage_type = preset_usage_types[usage_name]
                            preset = elicitor.get_weights_preset(usage_type)
                            st.session_state.custom_weights[usage_name] = {
                                'w_DB': preset['w_DB'],
                                'w_DP': preset['w_DP'],
                                'w_BR': preset['w_BR'],
                                'w_UP': preset['w_UP']
                            }
                        # Si custom, utiliser valeurs custom
                        elif usage_name in st.session_state.custom_usages:
                            custom_data = st.session_state.custom_usages[usage_name]
                            st.session_state.custom_weights[usage_name] = {
                                'w_DB': custom_data['w_DB'],
                                'w_DP': custom_data['w_DP'],
                                'w_BR': custom_data['w_BR'],
                                'w_UP': custom_data['w_UP']
                            }
                    
                    with st.expander(f"‚öôÔ∏è {usage_name}", expanded=True):
                        # Indicateur si custom
                        if usage_name in st.session_state.custom_usages:
                            st.badge("Custom", icon="‚ú®")
                        
                        st.markdown(f"**{usage_name}**")
                        
                        col1, col2, col3, col4 = st.columns(4)
                        
                        with col1:
                            w_db = st.slider(
                                "DB",
                                min_value=0,
                                max_value=100,
                                value=int(st.session_state.custom_weights[usage_name]['w_DB'] * 100),
                                step=5,
                                key=f"slider_db_{usage_name}",
                                help="Database: Contraintes structurelles"
                            )
                        
                        with col2:
                            w_dp = st.slider(
                                "DP",
                                min_value=0,
                                max_value=100,
                                value=int(st.session_state.custom_weights[usage_name]['w_DP'] * 100),
                                step=5,
                                key=f"slider_dp_{usage_name}",
                                help="Data Processing: Transformations ETL"
                            )
                        
                        with col3:
                            w_br = st.slider(
                                "BR",
                                min_value=0,
                                max_value=100,
                                value=int(st.session_state.custom_weights[usage_name]['w_BR'] * 100),
                                step=5,
                                key=f"slider_br_{usage_name}",
                                help="Business Rules: R√®gles m√©tier"
                            )
                        
                        with col4:
                            w_up = st.slider(
                                "UP",
                                min_value=0,
                                max_value=100,
                                value=int(st.session_state.custom_weights[usage_name]['w_UP'] * 100),
                                step=5,
                                key=f"slider_up_{usage_name}",
                                help="Usage-fit: Ad√©quation contextuelle"
                            )
                        
                        # Calculer somme
                        total = w_db + w_dp + w_br + w_up
                        
                        # Afficher somme avec couleur
                        if total == 100:
                            st.success(f"‚úÖ Somme = {total}% (OK)")
                        else:
                            st.error(f"‚ö†Ô∏è Somme = {total}% (doit √™tre 100%)")
                        
                        # Normaliser automatiquement si demand√©
                        if total != 100:
                            if st.button(f"üîÑ Normaliser automatiquement", key=f"normalize_{usage_name}"):
                                # Normaliser √† 100%
                                if total > 0:
                                    st.session_state.custom_weights[usage_name] = {
                                        'w_DB': round(w_db / total, 2),
                                        'w_DP': round(w_dp / total, 2),
                                        'w_BR': round(w_br / total, 2),
                                        'w_UP': round(w_up / total, 2)
                                    }
                                    st.rerun()
                        else:
                            # Sauvegarder valeurs
                            st.session_state.custom_weights[usage_name] = {
                                'w_DB': w_db / 100,
                                'w_DP': w_dp / 100,
                                'w_BR': w_br / 100,
                                'w_UP': w_up / 100
                            }
                        
                        # Bouton reset
                        if st.button(f"üîÑ R√©initialiser", key=f"reset_{usage_name}"):
                            if usage_name in preset_usage_types:
                                usage_type = preset_usage_types[usage_name]
                                preset = elicitor.get_weights_preset(usage_type)
                                st.session_state.custom_weights[usage_name] = {
                                    'w_DB': preset['w_DB'],
                                    'w_DP': preset['w_DP'],
                                    'w_BR': preset['w_BR'],
                                    'w_UP': preset['w_UP']
                                }
                            elif usage_name in st.session_state.custom_usages:
                                custom_data = st.session_state.custom_usages[usage_name]
                                st.session_state.custom_weights[usage_name] = {
                                    'w_DB': custom_data['w_DB'],
                                    'w_DP': custom_data['w_DP'],
                                    'w_BR': custom_data['w_BR'],
                                    'w_UP': custom_data['w_UP']
                                }
                            st.rerun()
            
            else:
                # Mode par d√©faut : afficher juste les valeurs
                with st.expander("üìä Voir pond√©rations (valeurs par d√©faut)", expanded=False):
                    for usage_name in selected_usages:
                        # Charger valeurs par d√©faut
                        if usage_name in preset_usage_types:
                            usage_type = preset_usage_types[usage_name]
                            weights = elicitor.get_weights_preset(usage_type)
                        elif usage_name in st.session_state.custom_usages:
                            weights = st.session_state.custom_usages[usage_name]
                        else:
                            continue
                        
                        st.markdown(f"**{usage_name}**")
                        if usage_name in st.session_state.custom_usages:
                            st.caption("‚ú® Usage personnalis√©")
                        
                        col1, col2, col3, col4 = st.columns(4)
                        col1.metric("DB", f"{weights['w_DB']:.0%}")
                        col2.metric("DP", f"{weights['w_DP']:.0%}")
                        col3.metric("BR", f"{weights['w_BR']:.0%}")
                        col4.metric("UP", f"{weights['w_UP']:.0%}")
                        st.caption(weights.get('rationale', ''))
                        st.markdown("---")
        
        # Bouton ANALYSER
        st.markdown("---")
        if st.button("üöÄ LANCER ANALYSE", type="primary", use_container_width=True):
            if not selected_columns:
                st.error("‚ö†Ô∏è S√©lectionne au moins une colonne !")
            elif not selected_usages:
                st.error("‚ö†Ô∏è S√©lectionne au moins un usage !")
            else:
                # LANCER ANALYSE
                with st.spinner("‚è≥ Analyse en cours..."):
                    try:
                        # Pr√©parer config usages
                        usages = []
                        for name in selected_usages:
                            # Utiliser valeurs custom si c'est un usage custom
                            if name in st.session_state.custom_usages:
                                usages.append({
                                    "nom": name,
                                    "type": "custom",
                                    "criticite": "MEDIUM"
                                })
                            else:
                                usages.append({
                                    "nom": name.split('(')[0].strip(),
                                    "type": all_usage_types[name],
                                    "criticite": "HIGH" if "Paie" in name else "MEDIUM"
                                })
                        
                        # 1. Stats exploratoires
                        stats = analyze_dataset(st.session_state.df, selected_columns)
                        
                        # 2. Vecteurs 4D
                        vecteurs_4d = compute_all_beta_vectors(st.session_state.df, selected_columns, stats)
                        
                        # 3. Pond√©rations
                        # Si mode personnalis√© ET poids valides, utiliser custom, sinon valeurs par d√©faut
                        if mode == "‚úèÔ∏è Ajuster finement les pond√©rations" and st.session_state.custom_weights:
                            # V√©rifier que tous les usages ont des poids valides
                            weights = {}
                            all_valid = True
                            
                            for usage_name in selected_usages:
                                if usage_name in st.session_state.custom_weights:
                                    custom = st.session_state.custom_weights[usage_name]
                                    total = sum(custom.values())
                                    
                                    if abs(total - 1.0) < 0.01:  # Tol√©rance 1%
                                        # Utiliser nom nettoy√©
                                        clean_name = usage_name.split('(')[0].strip() if '(' in usage_name else usage_name
                                        weights[clean_name] = {
                                            'w_DB': custom['w_DB'],
                                            'w_DP': custom['w_DP'],
                                            'w_BR': custom['w_BR'],
                                            'w_UP': custom['w_UP'],
                                            'rationale': 'Pond√©rations ajust√©es manuellement'
                                        }
                                    else:
                                        all_valid = False
                                        st.error(f"‚ö†Ô∏è {usage_name}: somme = {total:.0%} (doit √™tre 100%)")
                            
                            if not all_valid:
                                st.stop()
                        else:
                            # Utiliser valeurs par d√©faut
                            weights = {}
                            for usage in usages:
                                usage_name = usage['nom']
                                
                                # Si usage custom, utiliser ses valeurs
                                if usage_name in st.session_state.custom_usages:
                                    custom_data = st.session_state.custom_usages[usage_name]
                                    weights[usage_name] = {
                                        'w_DB': custom_data['w_DB'],
                                        'w_DP': custom_data['w_DP'],
                                        'w_BR': custom_data['w_BR'],
                                        'w_UP': custom_data['w_UP'],
                                        'rationale': custom_data.get('rationale', 'Usage personnalis√©')
                                    }
                                # Sinon utiliser preset
                                else:
                                    preset_weights = elicit_weights_auto([usage], vecteurs_4d)
                                    weights.update(preset_weights)
                        
                        # 4. Scores risque
                        scores = compute_risk_scores(vecteurs_4d, weights, usages)
                        
                        # 5. Top priorit√©s
                        top_priorities = get_top_priorities(scores, top_n=5)
                        
                        # 6. Lineage (premier attribut/usage)
                        lineage = None
                        if len(selected_columns) > 0 and len(usages) > 0:
                            first_attr = selected_columns[0]
                            first_usage_name = usages[0]['nom']
                            if first_attr in vecteurs_4d and first_usage_name in weights:
                                lineage = simulate_lineage(vecteurs_4d[first_attr], weights[first_usage_name])
                        
                        # 7. Comparaison DAMA
                        comparaison = compare_dama_vs_probabiliste(
                            st.session_state.df,
                            selected_columns,
                            scores,
                            vecteurs_4d
                        )
                        
                        # Stocker r√©sultats
                        st.session_state.results = {
                            'stats': stats,
                            'vecteurs_4d': vecteurs_4d,
                            'weights': weights,
                            'scores': scores,
                            'top_priorities': top_priorities,
                            'lineage': lineage,
                            'comparaison': comparaison
                        }
                        
                        st.session_state.analysis_done = True
                        st.success("‚úÖ Analyse termin√©e !")
                        st.balloons()
                        
                    except Exception as e:
                        st.error(f"‚ùå Erreur analyse : {str(e)}")
                        st.exception(e)


# ============================================================================
# MAIN - AFFICHAGE R√âSULTATS
# ============================================================================

if st.session_state.analysis_done and st.session_state.results:
    results = st.session_state.results
    
    # ONGLETS
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
        "üìä Dashboard",
        "üéØ Vecteurs 4D",
        "‚ö†Ô∏è Priorit√©s",
        "üîÑ Lineage",
        "üìà Comparaison DAMA",
        "üí¨ √âlicitation IA",
        "üîî Surveillance"
    ])
    
    # ========================================================================
    # TAB 1 : DASHBOARD
    # ========================================================================
    with tab1:
        st.header("üìä Dashboard Qualit√©")
        
        # Boutons Export en haut
        col_export1, col_export2, col_export3 = st.columns([2, 2, 4])
        
        with col_export1:
            if st.button("üì• Export Excel", type="primary", use_container_width=True):
                try:
                    output_file = export_to_excel(results)
                    
                    # Lire fichier pour download
                    with open(output_file, 'rb') as f:
                        st.download_button(
                            label="üíæ T√©l√©charger Excel",
                            data=f,
                            file_name=output_file,
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True
                        )
                    st.success(f"‚úÖ Excel cr√©√© : {output_file}")
                except Exception as e:
                    st.error(f"‚ùå Erreur export Excel : {str(e)}")
        
        with col_export2:
            if st.button("üì• Export CSV", use_container_width=True):
                try:
                    files = export_to_csv(results)
                    st.success(f"‚úÖ {len(files)} fichiers CSV cr√©√©s")
                    for file in files:
                        st.text(f"‚Ä¢ {file}")
                except Exception as e:
                    st.error(f"‚ùå Erreur export CSV : {str(e)}")
        
        st.markdown("---")
        
        # M√©triques cl√©s
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "Attributs analys√©s",
                len(results['vecteurs_4d']),
                help="Nombre colonnes analys√©es"
            )
        
        with col2:
            st.metric(
                "Usages m√©tier",
                len(results['weights']),
                help="Contextes m√©tier configur√©s"
            )
        
        with col3:
            top_score = max(results['scores'].values()) if results['scores'] else 0
            st.metric(
                "Risque max",
                f"{top_score:.1%}",
                delta=f"{'CRITIQUE' if top_score > 0.4 else 'OK'}",
                delta_color="inverse"
            )
        
        with col4:
            critical_count = len([s for s in results['scores'].values() if s > 0.4])
            st.metric(
                "Alertes critiques",
                critical_count,
                help="Scores > 40%"
            )
        
        st.markdown("---")
        
        # Heatmap scores
        st.subheader("üî• Matrice Scores Risque")
        fig_heatmap = create_heatmap(results['scores'])
        st.plotly_chart(fig_heatmap, use_container_width=True, key="heatmap_dashboard")
        
        # Bouton commentaire IA
        if st.button("üí¨ Expliquer avec Claude", key="explain_heatmap", help="G√©n√©rer un commentaire IA sur cette matrice"):
            if 'anthropic_api_key' in st.session_state and st.session_state.anthropic_api_key:
                with st.spinner("Claude analyse la matrice..."):
                    # Pr√©parer donn√©es pour IA
                    top_scores = sorted(results['scores'].items(), key=lambda x: x[1], reverse=True)[:5]
                    data_for_ai = {
                        "total_combinations": len(results['scores']),
                        "top_5_risks": [{"combo": k, "score": f"{v:.1%}"} for k, v in top_scores],
                        "critical_count": len([s for s in results['scores'].values() if s > 0.4]),
                        "attributs_analyzed": len(results['vecteurs_4d']),
                        "usages_configured": len(results['weights'])
                    }
                    
                    comment = generate_ai_comment("heatmap", data_for_ai, st.session_state.anthropic_api_key)
                    
                    st.info(f"üí¨ **Commentaire Claude** :\n\n{comment}")
            else:
                st.warning("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar pour utiliser les commentaires IA")
    
    # ========================================================================
    # TAB 2 : VECTEURS 4D
    # ========================================================================
    with tab2:
        st.header("üéØ Vecteurs Qualit√© 4D")
        
        for attr, vector in results['vecteurs_4d'].items():
            st.subheader(f"üìå {attr}")
            
            # Graphique
            fig = create_vector_chart(vector)
            st.plotly_chart(fig, use_container_width=True, key=f"vector_chart_{attr}")
            
            # Bouton commentaire IA
            if st.button("üí¨ Expliquer avec Claude", key=f"explain_vector_{attr}", help="Analyser ce vecteur 4D"):
                if 'anthropic_api_key' in st.session_state and st.session_state.anthropic_api_key:
                    with st.spinner("Claude analyse le vecteur..."):
                        data_for_ai = {
                            "attribut": attr,
                            "P_DB": vector.get('P_DB', 0),
                            "P_DP": vector.get('P_DP', 0),
                            "P_BR": vector.get('P_BR', 0),
                            "P_UP": vector.get('P_UP', 0),
                            "alpha_DB": vector.get('alpha_DB', 0),
                            "beta_DB": vector.get('beta_DB', 0),
                            "confidence": "HIGH" if vector.get('beta_DB', 0) > 50 else "MEDIUM"
                        }
                        
                        comment = generate_ai_comment("vector", data_for_ai, st.session_state.anthropic_api_key)
                        
                        st.info(f"üí¨ **Commentaire Claude** :\n\n{comment}")
                else:
                    st.warning("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar")
            
            # D√©tails Beta
            with st.expander("üî¨ Param√®tres Beta d√©taill√©s"):
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.markdown("**[DB] Database**")
                    st.write(f"Beta({vector['alpha_DB']:.1f}, {vector['beta_DB']:.1f})")
                    st.write(f"P = {vector['P_DB']:.3f}")
                    st.write(f"œÉ = {vector.get('std_DB', 0):.3f}")
                
                with col2:
                    st.markdown("**[DP] Data Processing**")
                    st.write(f"Beta({vector['alpha_DP']:.1f}, {vector['beta_DP']:.1f})")
                    st.write(f"P = {vector['P_DP']:.3f}")
                    st.write(f"œÉ = {vector.get('std_DP', 0):.3f}")
                
                with col3:
                    st.markdown("**[BR] Business Rules**")
                    st.write(f"Beta({vector['alpha_BR']:.1f}, {vector['beta_BR']:.1f})")
                    st.write(f"P = {vector['P_BR']:.3f}")
                    st.write(f"œÉ = {vector.get('std_BR', 0):.3f}")
                
                with col4:
                    st.markdown("**[UP] Usage-fit**")
                    st.write(f"Beta({vector['alpha_UP']:.1f}, {vector['beta_UP']:.1f})")
                    st.write(f"P = {vector['P_UP']:.3f}")
                    st.write(f"œÉ = {vector.get('std_UP', 0):.3f}")
            
            st.markdown("---")
    
    # ========================================================================
    # TAB 3 : PRIORIT√âS
    # ========================================================================
    with tab3:
        st.header("‚ö†Ô∏è Top Priorit√©s Actions")
        
        for i, priority in enumerate(results['top_priorities'][:3], 1):  # Top 3 seulement avec bouton IA
            # Couleur box selon s√©v√©rit√©
            if priority['severite'] == 'CRITIQUE':
                box_class = "danger-box"
                emoji = "üö®"
            elif priority['severite'] == '√âLEV√â':
                box_class = "warning-box"
                emoji = "‚ö†Ô∏è"
            else:
                box_class = "success-box"
                emoji = "‚úÖ"
            
            st.markdown(f"""
            <div class="{box_class}">
                <h3>{emoji} #{i} - {priority['attribut']} √ó {priority['usage']}</h3>
                <p><strong>Score risque :</strong> {priority['score']:.1%} ({priority['severite']})</p>
                <p><strong>Impact :</strong> {priority['records_affected']} enregistrements, {priority['impact_mensuel']:,}‚Ç¨/mois</p>
            </div>
            """, unsafe_allow_html=True)
            
            # Actions recommand√©es
            with st.expander("üìã Actions recommand√©es"):
                for action in priority.get('actions', []):
                    st.write(f"‚Ä¢ {action}")
            
            # Bouton commentaire IA
            if st.button("üí¨ Analyser avec Claude", key=f"explain_priority_{i}", help="Recommandation IA pour cette priorit√©"):
                if 'anthropic_api_key' in st.session_state and st.session_state.anthropic_api_key:
                    with st.spinner("Claude analyse la priorit√©..."):
                        comment = generate_ai_comment("priority", priority, st.session_state.anthropic_api_key)
                        st.info(f"üí¨ **Recommandation Claude** :\n\n{comment}")
                else:
                    st.warning("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar")
            
            st.markdown("<br>", unsafe_allow_html=True)
        
        # Afficher les priorit√©s restantes sans bouton IA
        if len(results['top_priorities']) > 3:
            st.markdown("---")
            st.subheader("Autres priorit√©s")
            
            for i, priority in enumerate(results['top_priorities'][3:], 4):
                # Couleur box selon s√©v√©rit√© (version simplifi√©e sans IA)
                if priority['severite'] == 'CRITIQUE':
                    box_class = "danger-box"
                    emoji = "üö®"
                elif priority['severite'] == '√âLEV√â':
                    box_class = "warning-box"
                    emoji = "‚ö†Ô∏è"
                else:
                    box_class = "success-box"
                    emoji = "‚úÖ"
                
                st.markdown(f"""
                <div class="{box_class}">
                    <h4>{emoji} #{i} - {priority['attribut']} √ó {priority['usage']}</h4>
                    <p><strong>Score :</strong> {priority['score']:.1%} ({priority['severite']})</p>
                </div>
                """, unsafe_allow_html=True)
                st.markdown("<br>", unsafe_allow_html=True)
    
    # ========================================================================
    # TAB 4 : LINEAGE AVEC SC√âNARIOS R√âALISTES
    # ========================================================================
    with tab4:
        st.header("üîÑ Propagation Risque Lineage")
        
        st.markdown("""
        ### üéØ Objectif : Simuler la d√©gradation qualit√© le long des pipelines
        
        S√©lectionne un **sc√©nario r√©aliste** pour voir comment le risque se propage et s'aggrave √† travers les transformations.
        """)
        
        # S√©lection sc√©nario
        st.markdown("### üìã Choisis un sc√©nario")
        
        scenario = st.radio(
            "Incidents bas√©s sur cas r√©els fr√©quents en entreprise",
            [
                "‚úÖ Baseline (pas d'incident - qualit√© stable)",
                "üîß Incident ETL Parser (bug d√©ploiement - tr√®s fr√©quent)",
                "üìä Enrichissement M√©tier D√©faillant (r√©f√©rentiel obsol√®te)",
                "üìà Reporting D√©faillant (donn√©es incompl√®tes - deadline approche)",
                "‚ö†Ô∏è D√©gradation Cumulative (micro-erreurs qui s'accumulent)",
                "‚úèÔ∏è Personnalis√© (cr√©er ton propre sc√©nario avec Claude)"
            ],
            help="Sc√©narios benchmark√©s sur incidents r√©els industriels + option personnalis√©e"
        )
        
        # D√©finir donn√©es selon sc√©nario
        if "Baseline" in scenario:
            # Sc√©nario stable (actuel)
            scenario_data = {
                "nom": "Baseline - Qualit√© Stable",
                "emoji": "‚úÖ",
                "contexte": """**Situation normale** : Aucun incident d√©tect√©.
                
Les pipelines ETL fonctionnent correctement, les transformations pr√©servent la qualit√©.
C'est la situation de r√©f√©rence (baseline) pour comparer avec les incidents.""",
                "risk_source": 0.332,
                "risk_final": 0.337,
                "delta": 0.005,
                "timeline": [
                    {"etape": "Source SIRH", "P_DB": 0.367, "P_DP": 1.000, "P_BR": 0.050, "P_UP": 0.100},
                    {"etape": "ETL Extraction", "P_DB": 0.367, "P_DP": 1.000, "P_BR": 0.050, "P_UP": 0.100},
                    {"etape": "Enrichissement", "P_DB": 0.367, "P_DP": 1.000, "P_BR": 0.069, "P_UP": 0.100},
                    {"etape": "Agr√©gation Paie", "P_DB": 0.367, "P_DP": 1.000, "P_BR": 0.069, "P_UP": 0.100},
                    {"etape": "Calcul Final", "P_DB": 0.367, "P_DP": 1.000, "P_BR": 0.069, "P_UP": 0.100}
                ],
                "interpretation": "‚û°Ô∏è STABLE - Qualit√© pr√©serv√©e tout au long du pipeline",
                "cause_racine": "Aucun incident",
                "impact_records": 0,
                "impact_financier": 0,
                "temps_detection": "N/A",
                "actions": []
            }
        
        elif "Incident ETL" in scenario:
            # Sc√©nario ETL cass√© (li√© √† tab Surveillance)
            scenario_data = {
                "nom": "Incident ETL Parser",
                "emoji": "üîß",
                "contexte": """**D√©ploiement ETL v2.8.2** le 12/01/2025 √† 23:15 (commit abc123def).

**Bug introduit** : Parser Anciennet√© modifi√©, virgules d√©cimales non g√©r√©es.
```python
# AVANT (v2.8.1) ‚úÖ
anciennete = float(value.replace(',', '.'))

# APR√àS (v2.8.2) ‚ùå  
anciennete = float(value)  # Crash sur "7,21"
```

**R√©sultat** : 141/687 enregistrements cass√©s lors de l'extraction ETL.""",
                "risk_source": 0.463,
                "risk_final": 0.623,
                "delta": 0.160,
                "timeline": [
                    {"etape": "Source SIRH", "P_DB": 0.990, "P_DP": 0.020, "P_BR": 0.200, "P_UP": 0.100},
                    {"etape": "ETL Extraction", "P_DB": 0.990, "P_DP": 0.285, "P_BR": 0.200, "P_UP": 0.100},
                    {"etape": "Enrichissement", "P_DB": 0.990, "P_DP": 0.285, "P_BR": 0.220, "P_UP": 0.100},
                    {"etape": "Agr√©gation Paie", "P_DB": 0.990, "P_DP": 0.310, "P_BR": 0.220, "P_UP": 0.100},
                    {"etape": "Calcul Final", "P_DB": 0.990, "P_DP": 0.320, "P_BR": 0.220, "P_UP": 0.100}
                ],
                "interpretation": "üö® CRITIQUE - D√©gradation DP massive (+26.5 points) d√®s extraction ETL",
                "cause_racine": "Commit abc123def - Parser Anciennet√© cass√© (virgule non g√©r√©e)",
                "impact_records": 141,
                "impact_financier": 45080,
                "temps_detection": "9 heures (vs 3 semaines avec DAMA)",
                "actions": [
                    "üîÑ Rollback imm√©diat vers ETL v2.8.1",
                    "üîß Fix parser : value.replace(',', '.') avant float()",
                    "üìä Batch correction des 141 enregistrements",
                    "üîî Activer monitoring continu DP sur Anciennet√©",
                    "üß™ Ajouter test unitaire parser avec virgules"
                ]
            }
        
        elif "Enrichissement" in scenario:
            # Sc√©nario r√©f√©rentiel obsol√®te
            scenario_data = {
                "nom": "Enrichissement M√©tier D√©faillant",
                "emoji": "üìä",
                "contexte": """**Fusion d√©partements RH** : Nord + Sud ‚Üí Grand Nord (01/12/2024).

**Probl√®me** : Table r√©f√©rentiel `ref_departements` non mise √† jour.

**Impact progressif** :
- D√©cembre : 15 employ√©s mal rattach√©s (2%)
- Janvier : 45 employ√©s mal rattach√©s (7%)
- F√©vrier : 89 employ√©s mal rattach√©s (13%)

**D√©couverte** : Audit interne CSE d√©tecte incoh√©rences reporting social.""",
                "risk_source": 0.332,
                "risk_final": 0.485,
                "delta": 0.153,
                "timeline": [
                    {"etape": "Source SIRH", "P_DB": 0.367, "P_DP": 0.020, "P_BR": 0.050, "P_UP": 0.100},
                    {"etape": "ETL Extraction", "P_DB": 0.367, "P_DP": 0.020, "P_BR": 0.050, "P_UP": 0.100},
                    {"etape": "Enrichissement", "P_DB": 0.367, "P_DP": 0.020, "P_BR": 0.280, "P_UP": 0.100},
                    {"etape": "Agr√©gation Paie", "P_DB": 0.367, "P_DP": 0.025, "P_BR": 0.280, "P_UP": 0.100},
                    {"etape": "Calcul Final", "P_DB": 0.367, "P_DP": 0.030, "P_BR": 0.280, "P_UP": 0.100}
                ],
                "interpretation": "‚ö†Ô∏è √âLEV√â - D√©gradation BR progressive (+23 points) lors enrichissement m√©tier",
                "cause_racine": "Table ref_departements obsol√®te (fusion Nord+Sud non int√©gr√©e)",
                "impact_records": 89,
                "impact_financier": 12400,
                "temps_detection": "2 jours avec monitoring BR (vs 3 semaines manuellement)",
                "actions": [
                    "üìã Mise √† jour table ref_departements (fusion Nord+Sud)",
                    "üîÑ Recalcul batch des rattachements depuis 01/12",
                    "üìä Correction des 89 employ√©s mal rattach√©s",
                    "üîî Alerte automatique sur changements r√©f√©rentiels",
                    "üìÖ Workflow validation changements orga (DRH‚ÜíIT)"
                ]
            }
        
        elif "Reporting" in scenario:
            # NOUVEAU : Sc√©nario reporting d√©faillant
            scenario_data = {
                "nom": "Reporting CSE D√©faillant",
                "emoji": "üìà",
                "contexte": """**Rapport mensuel CSE** (Comit√© Social √âconomique) - Deadline : 05/02/2025.

**D√©couverte J-2** (03/02) lors revue DRH : 252/687 dates promotions manquantes (37%).

**Cause racine** : Workflow validation promotions non respect√©.
- Managers ne saisissent pas les promotions dans SIRH
- Processus manuel, pas d'alerte automatique
- D√©gradation progressive sur 6 mois (jamais d√©tect√©e)

**Impact** : Reporting incomplet, cr√©dibilit√© DRH aupr√®s CSE compromise.""",
                "risk_source": 0.364,
                "risk_final": 0.520,
                "delta": 0.156,
                "timeline": [
                    {"etape": "Source SIRH", "P_DB": 0.364, "P_DP": 0.143, "P_BR": 0.252, "P_UP": 0.250},
                    {"etape": "ETL Extraction", "P_DB": 0.364, "P_DP": 0.150, "P_BR": 0.252, "P_UP": 0.250},
                    {"etape": "Enrichissement", "P_DB": 0.364, "P_DP": 0.160, "P_BR": 0.280, "P_UP": 0.250},
                    {"etape": "Agr√©gation CSE", "P_DB": 0.364, "P_DP": 0.170, "P_BR": 0.300, "P_UP": 0.380},
                    {"etape": "Export Reporting", "P_DB": 0.364, "P_DP": 0.180, "P_BR": 0.310, "P_UP": 0.420}
                ],
                "interpretation": "‚ö†Ô∏è √âLEV√â - D√©gradation multi-dimensions (UP +17 pts, BR +6 pts) pour reporting CSE",
                "cause_racine": "Workflow promotions non respect√© - 252 dates manquantes (37%)",
                "impact_records": 252,
                "impact_financier": 7500,
                "temps_detection": "J-2 avant deadline (d√©tection manuelle tardive)",
                "actions": [
                    "üö® URGENT : Relance managers pour saisie promotions (252 cas)",
                    "üìß Email automatique rappel mensuel + escalade N+1",
                    "üîî Dashboard temps r√©el compl√©tude donn√©es promotions",
                    "üìä Alerte seuil critique (<90%) J-15 avant deadline",
                    "üîÑ Processus backup : extraction fichiers Direction (plan B)",
                    "üí∞ Co√ªt opportunit√© : cr√©dibilit√© DRH + risque contentieux CSE"
                ]
            }
        
        elif "Personnalis√©" in scenario:
            # SC√âNARIO PERSONNALISABLE
            st.markdown("### ‚úèÔ∏è Cr√©er ton Sc√©nario Personnalis√©")
            
            # Choix mode
            mode_custom = st.radio(
                "Comment veux-tu cr√©er ton sc√©nario ?",
                [
                    "ü§ñ Mode IA : D√©cris l'incident, Claude g√©n√®re les chiffres",
                    "üìä Mode Manuel : Entre les chiffres toi-m√™me"
                ],
                help="Mode IA = plus rapide, Mode Manuel = contr√¥le total"
            )
            
            if "Mode IA" in mode_custom:
                # MODE IA : Description textuelle ‚Üí Claude g√©n√®re les chiffres
                st.markdown("#### ü§ñ D√©cris ton incident √† Claude")
                
                with st.form("custom_scenario_ia_form"):
                    nom_incident = st.text_input(
                        "Nom de l'incident",
                        placeholder="Ex: Migration base de donn√©es rat√©e",
                        help="Titre court pour identifier l'incident"
                    )
                    
                    description_incident = st.text_area(
                        "D√©cris l'incident en d√©tail",
                        placeholder="""Ex: Notre migration de SQL Server vers PostgreSQL a caus√© des pertes de donn√©es.
Les colonnes avec NULL ont √©t√© mal converties, cr√©ant des erreurs dans les calculs.
Les transformations ETL ont √©chou√© sur 30% des records.
Le reporting mensuel √©tait incomplet pendant 2 semaines.""",
                        height=150,
                        help="Plus tu donnes de d√©tails, mieux Claude pourra g√©n√©rer les chiffres r√©alistes"
                    )
                    
                    pipeline_etapes = st.text_input(
                        "√âtapes du pipeline (s√©par√©es par virgule)",
                        value="Source DB, Migration, ETL, Transformation, Reporting",
                        help="Ex: Source, Extraction, Enrichissement, Agr√©gation, Export"
                    )
                    
                    impact_estime = st.selectbox(
                        "Impact estim√©",
                        ["Faible (5-15%)", "Moyen (15-30%)", "√âlev√© (30-50%)", "Critique (>50%)"],
                        help="Gravit√© globale de l'incident"
                    )
                    
                    submitted_ia = st.form_submit_button("üöÄ G√©n√©rer avec Claude", type="primary")
                
                if submitted_ia:
                    if not nom_incident or not description_incident:
                        st.error("‚ö†Ô∏è Le nom et la description sont obligatoires")
                    elif 'anthropic_api_key' not in st.session_state or not st.session_state.anthropic_api_key:
                        st.error("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar")
                    else:
                        with st.spinner("ü§ñ Claude g√©n√®re ton sc√©nario personnalis√©..."):
                            try:
                                client = anthropic.Anthropic(api_key=st.session_state.anthropic_api_key)
                                
                                # Prompt pour g√©n√©ration sc√©nario
                                prompt_generation = f"""Tu es un expert Data Quality. G√©n√®re un sc√©nario de lineage r√©aliste bas√© sur cette description d'incident.

**Incident** : {nom_incident}

**Description** : {description_incident}

**√âtapes pipeline** : {pipeline_etapes}

**Impact estim√©** : {impact_estime}

**G√©n√®re un JSON avec cette structure exacte** :

```json
{{
  "nom": "Nom incident",
  "contexte": "Description d√©taill√©e 3-4 phrases",
  "risk_source": 0.XX (probabilit√© 0-1),
  "risk_final": 0.XX (probabilit√© 0-1),
  "delta": 0.XX (diff√©rence),
  "timeline": [
    {{"etape": "Nom √©tape 1", "P_DB": 0.XX, "P_DP": 0.XX, "P_BR": 0.XX, "P_UP": 0.XX}},
    {{"etape": "Nom √©tape 2", "P_DB": 0.XX, "P_DP": 0.XX, "P_BR": 0.XX, "P_UP": 0.XX}},
    ...
  ],
  "interpretation": "Texte explication",
  "cause_racine": "Cause technique pr√©cise",
  "impact_records": nombre (entier),
  "impact_financier": montant euros/mois (entier),
  "temps_detection": "Dur√©e",
  "actions": ["Action 1", "Action 2", ...]
}}
```

**R√àGLES CRITIQUES** :
1. Les probabilit√©s P_DB, P_DP, P_BR, P_UP sont entre 0.0 et 1.0
2. Chaque √©tape doit montrer une √©volution r√©aliste
3. Les dimensions affect√©es d√©pendent du type d'incident :
   - Probl√®me structurel ‚Üí P_DB √©lev√©
   - Erreur transformation ‚Üí P_DP √©lev√©
   - R√®gle m√©tier viol√©e ‚Üí P_BR √©lev√©
   - Inad√©quation usage ‚Üí P_UP √©lev√©
4. Le risque doit progresser ou rester stable (jamais diminuer sans correction)
5. Impact financier r√©aliste : 5k‚Ç¨-150k‚Ç¨/mois selon gravit√©
6. Temps d√©tection : comparer avec approche DAMA (heures vs semaines)

**R√©ponds UNIQUEMENT avec le JSON, rien d'autre.**"""
                                
                                response = client.messages.create(
                                    model="claude-sonnet-4-20250514",
                                    max_tokens=2000,
                                    system="Tu es un expert Data Quality qui g√©n√®re des sc√©narios de lineage r√©alistes au format JSON. Tu r√©ponds UNIQUEMENT avec le JSON valide, sans markdown ni texte additionnel.",
                                    messages=[{"role": "user", "content": prompt_generation}]
                                )
                                
                                # Extraire JSON
                                response_text = response.content[0].text
                                
                                # Nettoyer markdown si pr√©sent
                                if "```json" in response_text:
                                    json_start = response_text.find("```json") + 7
                                    json_end = response_text.find("```", json_start)
                                    response_text = response_text[json_start:json_end].strip()
                                elif "```" in response_text:
                                    json_start = response_text.find("```") + 3
                                    json_end = response_text.find("```", json_start)
                                    response_text = response_text[json_start:json_end].strip()
                                
                                # Parser JSON
                                scenario_data = json.loads(response_text)
                                scenario_data['emoji'] = "‚úèÔ∏è"
                                
                                # Stocker dans session state
                                st.session_state.custom_scenario_data = scenario_data
                                
                                st.success("‚úÖ Sc√©nario g√©n√©r√© avec succ√®s par Claude !")
                                st.balloons()
                                
                            except json.JSONDecodeError as e:
                                st.error(f"‚ùå Erreur parsing JSON : {str(e)}")
                                st.code(response_text)
                            except Exception as e:
                                st.error(f"‚ùå Erreur g√©n√©ration : {str(e)}")
                
                # Afficher sc√©nario g√©n√©r√© si disponible
                if 'custom_scenario_data' in st.session_state:
                    scenario_data = st.session_state.custom_scenario_data
                    st.success("‚úÖ Utilisation du sc√©nario personnalis√© g√©n√©r√© par Claude")
                else:
                    st.info("üí° Remplis le formulaire ci-dessus et clique 'G√©n√©rer' pour cr√©er ton sc√©nario")
                    st.stop()
            
            else:
                # MODE MANUEL : Saisie des chiffres
                st.markdown("#### üìä Entre les chiffres manuellement")
                
                with st.form("custom_scenario_manual_form"):
                    nom_incident_manual = st.text_input(
                        "Nom de l'incident",
                        value="Mon Incident Personnalis√©"
                    )
                    
                    contexte_manual = st.text_area(
                        "Contexte / Description",
                        value="Description de l'incident...",
                        height=100
                    )
                    
                    st.markdown("**Nombre d'√©tapes du pipeline**")
                    nb_etapes = st.slider("Nombre d'√©tapes", 3, 8, 5)
                    
                    timeline_manual = []
                    
                    for i in range(nb_etapes):
                        st.markdown(f"##### üîπ √âtape {i+1}")
                        
                        col_nom, col_db, col_dp, col_br, col_up = st.columns([2,1,1,1,1])
                        
                        with col_nom:
                            nom_etape = st.text_input(
                                f"Nom √©tape {i+1}",
                                value=f"√âtape {i+1}",
                                key=f"nom_etape_{i}"
                            )
                        
                        with col_db:
                            p_db = st.number_input(
                                "DB (%)",
                                0, 100,
                                value=min(10 + i*5, 90),
                                key=f"p_db_{i}"
                            )
                        
                        with col_dp:
                            p_dp = st.number_input(
                                "DP (%)",
                                0, 100,
                                value=min(5 + i*8, 85),
                                key=f"p_dp_{i}"
                            )
                        
                        with col_br:
                            p_br = st.number_input(
                                "BR (%)",
                                0, 100,
                                value=min(8 + i*3, 75),
                                key=f"p_br_{i}"
                            )
                        
                        with col_up:
                            p_up = st.number_input(
                                "UP (%)",
                                0, 100,
                                value=min(12 + i*2, 70),
                                key=f"p_up_{i}"
                            )
                        
                        timeline_manual.append({
                            "etape": nom_etape,
                            "P_DB": p_db / 100,
                            "P_DP": p_dp / 100,
                            "P_BR": p_br / 100,
                            "P_UP": p_up / 100
                        })
                    
                    st.markdown("**Impact business**")
                    col_rec, col_fin = st.columns(2)
                    
                    with col_rec:
                        impact_records_manual = st.number_input(
                            "Records affect√©s",
                            0, 10000,
                            value=100
                        )
                    
                    with col_fin:
                        impact_financier_manual = st.number_input(
                            "Impact ‚Ç¨/mois",
                            0, 500000,
                            value=15000,
                            step=1000
                        )
                    
                    submitted_manual = st.form_submit_button("‚úÖ Cr√©er Sc√©nario", type="primary")
                
                if submitted_manual:
                    # Calculer risques
                    risk_source = (timeline_manual[0]['P_DB'] + timeline_manual[0]['P_DP'] + 
                                 timeline_manual[0]['P_BR'] + timeline_manual[0]['P_UP']) / 4
                    risk_final = (timeline_manual[-1]['P_DB'] + timeline_manual[-1]['P_DP'] + 
                                timeline_manual[-1]['P_BR'] + timeline_manual[-1]['P_UP']) / 4
                    
                    scenario_data = {
                        "nom": nom_incident_manual,
                        "emoji": "‚úèÔ∏è",
                        "contexte": contexte_manual,
                        "risk_source": risk_source,
                        "risk_final": risk_final,
                        "delta": risk_final - risk_source,
                        "timeline": timeline_manual,
                        "interpretation": f"{'üö® CRITIQUE' if risk_final > 0.5 else '‚ö†Ô∏è √âLEV√â' if risk_final > 0.3 else '‚û°Ô∏è MOYEN'} - Sc√©nario personnalis√©",
                        "cause_racine": "Incident personnalis√© (d√©tails dans contexte)",
                        "impact_records": impact_records_manual,
                        "impact_financier": impact_financier_manual,
                        "temps_detection": "√Ä d√©finir selon monitoring",
                        "actions": [
                            "Action 1 : √Ä d√©finir selon contexte",
                            "Action 2 : Impl√©menter monitoring sp√©cifique",
                            "Action 3 : Corriger cause racine identifi√©e"
                        ]
                    }
                    
                    st.session_state.custom_scenario_data = scenario_data
                    st.success("‚úÖ Sc√©nario manuel cr√©√© !")
                    st.balloons()
                
                # Afficher sc√©nario manuel si disponible
                if 'custom_scenario_data' in st.session_state:
                    scenario_data = st.session_state.custom_scenario_data
                    st.success("‚úÖ Utilisation du sc√©nario manuel")
                else:
                    st.info("üí° Remplis le formulaire ci-dessus et clique 'Cr√©er' pour d√©finir ton sc√©nario")
                    st.stop()
        
        else:  # D√©gradation cumulative
            scenario_data = {
                "nom": "D√©gradation Cumulative Multi-√âtapes",
                "emoji": "‚ö†Ô∏è",
                "contexte": """**Absence monitoring interm√©diaire** : Chaque √©tape d√©grade l√©g√®rement la qualit√©.

**Probl√®me** : Aucune micro-d√©gradation n'est critique individuellement, mais l'accumulation devient critique.

**√âtapes** :
1. Extraction : +3% erreurs parsing (tol√©r√©e)
2. Enrichissement : +5% incoh√©rences r√©f√©rentiels (tol√©r√©e)
3. Transformation : +4% erreurs calculs (tol√©r√©e)
4. Agr√©gation : +6% doublons (tol√©r√©e)
5. **Cumul : +18% = CRITIQUE** ‚ùå

**D√©couverte** : R√©clamations utilisateurs apr√®s 1 semaine production.""",
                "risk_source": 0.332,
                "risk_final": 0.518,
                "delta": 0.186,
                "timeline": [
                    {"etape": "Source SIRH", "P_DB": 0.367, "P_DP": 0.020, "P_BR": 0.050, "P_UP": 0.100},
                    {"etape": "ETL Extraction", "P_DB": 0.367, "P_DP": 0.050, "P_BR": 0.050, "P_UP": 0.100},
                    {"etape": "Enrichissement", "P_DB": 0.367, "P_DP": 0.085, "P_BR": 0.100, "P_UP": 0.100},
                    {"etape": "Transformation", "P_DB": 0.367, "P_DP": 0.125, "P_BR": 0.140, "P_UP": 0.120},
                    {"etape": "Agr√©gation", "P_DB": 0.367, "P_DP": 0.180, "P_BR": 0.180, "P_UP": 0.150},
                    {"etape": "Calcul Final", "P_DB": 0.367, "P_DP": 0.245, "P_BR": 0.230, "P_UP": 0.180}
                ],
                "interpretation": "‚ö†Ô∏è √âLEV√â - Accumulation progressive (+18.6 pts) pass√©e inaper√ßue sans monitoring",
                "cause_racine": "Absence monitoring interm√©diaire granulaire (only input/output)",
                "impact_records": 127,
                "impact_financier": 18200,
                "temps_detection": "1 semaine (r√©clamations users vs d√©tection proactive impossible)",
                "actions": [
                    "üîî Impl√©menter monitoring INTER-√âTAPES (pas juste I/O)",
                    "üìä Seuils tol√©rance par √©tape (ex: +2% max)",
                    "üö® Alerte cumulative si somme d√©gradations >10%",
                    "üìà Dashboard lineage temps r√©el (visualiser accumulation)",
                    "üß™ Tests automatis√©s apr√®s chaque transformation"
                ]
            }
        
        # Afficher contexte sc√©nario
        st.markdown("---")
        st.markdown(f"### {scenario_data['emoji']} {scenario_data['nom']}")
        
        with st.expander("üìã Contexte d√©taill√©", expanded=True):
            st.markdown(scenario_data['contexte'])
        
        # M√©triques impact
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "Risque source",
                f"{scenario_data['risk_source']:.1%}"
            )
        
        with col2:
            delta_val = scenario_data['delta']
            st.metric(
                "Risque final",
                f"{scenario_data['risk_final']:.1%}",
                delta=f"+{delta_val:.1%}",
                delta_color="inverse"
            )
        
        with col3:
            st.metric(
                "Enregistrements affect√©s",
                f"{scenario_data['impact_records']}"
            )
        
        with col4:
            if scenario_data['impact_financier'] > 0:
                st.metric(
                    "Impact financier/mois",
                    f"{scenario_data['impact_financier']:,}‚Ç¨"
                )
            else:
                st.metric("Impact financier", "0‚Ç¨")
        
        st.markdown("---")
        
        # Timeline propagation
        st.subheader("üìÖ Timeline Propagation D√©taill√©e")
        
        for idx, step in enumerate(scenario_data['timeline']):
            etape = step['etape']
            
            # Afficher nom √©tape
            st.markdown(f"### üîπ √âtape {idx+1} : {etape}")
            
            # M√©triques 4D
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("DB", f"{step['P_DB']:.1%}", 
                       delta=f"{(step['P_DB'] - scenario_data['timeline'][max(0,idx-1)]['P_DB']):.1%}" if idx > 0 else None,
                       delta_color="inverse")
            col2.metric("DP", f"{step['P_DP']:.1%}",
                       delta=f"{(step['P_DP'] - scenario_data['timeline'][max(0,idx-1)]['P_DP']):.1%}" if idx > 0 else None,
                       delta_color="inverse")
            col3.metric("BR", f"{step['P_BR']:.1%}",
                       delta=f"{(step['P_BR'] - scenario_data['timeline'][max(0,idx-1)]['P_BR']):.1%}" if idx > 0 else None,
                       delta_color="inverse")
            col4.metric("UP", f"{step['P_UP']:.1%}",
                       delta=f"{(step['P_UP'] - scenario_data['timeline'][max(0,idx-1)]['P_UP']):.1%}" if idx > 0 else None,
                       delta_color="inverse")
            
            # Bouton explication par √©tape
            if st.button(f"üí¨ Expliquer cette √©tape", key=f"explain_lineage_step_{idx}", help=f"Que se passe-t-il dans '{etape}' ?"):
                if 'anthropic_api_key' in st.session_state and st.session_state.anthropic_api_key:
                    with st.spinner(f"Claude analyse l'√©tape '{etape}'..."):
                        # Pr√©parer contexte pour IA
                        step_context = f"""Tu es un expert Data Quality. Explique ce qui se passe dans cette √©tape de lineage en 3-4 phrases maximum.

**Sc√©nario global** : {scenario_data['nom']}
**√âtape actuelle** : {etape} (√©tape {idx+1}/{len(scenario_data['timeline'])})

**Probabilit√©s actuelles** :
- P_DB = {step['P_DB']:.1%} (Database)
- P_DP = {step['P_DP']:.1%} (Data Processing)
- P_BR = {step['P_BR']:.1%} (Business Rules)
- P_UP = {step['P_UP']:.1%} (Usage-fit)

**√âvolution depuis √©tape pr√©c√©dente** :
{f"- Delta DB : {(step['P_DB'] - scenario_data['timeline'][idx-1]['P_DB'])*100:+.1f} points" if idx > 0 else "- √âtape source (baseline)"}
{f"- Delta DP : {(step['P_DP'] - scenario_data['timeline'][idx-1]['P_DP'])*100:+.1f} points" if idx > 0 else ""}
{f"- Delta BR : {(step['P_BR'] - scenario_data['timeline'][idx-1]['P_BR'])*100:+.1f} points" if idx > 0 else ""}
{f"- Delta UP : {(step['P_UP'] - scenario_data['timeline'][idx-1]['P_UP'])*100:+.1f} points" if idx > 0 else ""}

**Contexte incident** : {scenario_data['contexte'][:200]}...

Explique :
1. Ce que fait cette √©tape techniquement (extraction, transformation, agr√©gation, etc.)
2. Pourquoi telle(s) dimension(s) sont affect√©es √† cette √©tape
3. L'impact op√©rationnel concret de cette d√©gradation

Style : Technique mais accessible, p√©dagogique, avec exemples concrets."""
                        
                        try:
                            client = anthropic.Anthropic(api_key=st.session_state.anthropic_api_key)
                            
                            response = client.messages.create(
                                model="claude-sonnet-4-20250514",
                                max_tokens=500,
                                system="Tu es un expert Data Quality qui explique les transformations de donn√©es avec un style direct, p√©dagogique et technique. Toujours 3-4 phrases maximum.",
                                messages=[{"role": "user", "content": step_context}]
                            )
                            
                            comment = response.content[0].text
                            st.info(f"üí¨ **Explication Claude - {etape}** :\n\n{comment}")
                            
                        except Exception as e:
                            st.error(f"‚ö†Ô∏è Erreur g√©n√©ration commentaire : {str(e)}")
                else:
                    st.warning("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar")
            
            st.markdown("---")
        
        # Interpr√©tation
        st.info(f"**Interpr√©tation :** {scenario_data['interpretation']}")
        
        # Cause racine + Impact
        st.markdown("### üîç Analyse Incident")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**üéØ Cause Racine**")
            st.write(scenario_data['cause_racine'])
            
            if scenario_data['temps_detection'] != "N/A":
                st.success(f"‚è±Ô∏è **Temps d√©tection** : {scenario_data['temps_detection']}")
        
        with col2:
            st.markdown("**üìä Impact Mesur√©**")
            st.write(f"‚Ä¢ Records affect√©s : **{scenario_data['impact_records']}** / 687")
            if scenario_data['impact_financier'] > 0:
                st.write(f"‚Ä¢ Impact financier : **{scenario_data['impact_financier']:,}‚Ç¨/mois**")
            st.write(f"‚Ä¢ Delta risque : **+{scenario_data['delta']:.1%}** points")
        
        # Actions recommand√©es
        if scenario_data['actions']:
            st.markdown("### üõ†Ô∏è Actions Recommand√©es")
            
            for action in scenario_data['actions']:
                st.write(f"‚Ä¢ {action}")
        
        # Bouton commentaire IA
        st.markdown("---")
        if st.button("üí¨ Analyser avec Claude", key="explain_lineage", help="Analyse d√©taill√©e de la propagation"):
            if 'anthropic_api_key' in st.session_state and st.session_state.anthropic_api_key:
                with st.spinner("Claude analyse la propagation..."):
                    data_for_ai = {
                        "scenario": scenario_data['nom'],
                        "risk_source": scenario_data['risk_source'],
                        "risk_final": scenario_data['risk_final'],
                        "delta_absolute": scenario_data['delta'],
                        "cause_racine": scenario_data['cause_racine'],
                        "impact_records": scenario_data['impact_records'],
                        "impact_financier": scenario_data['impact_financier'],
                        "temps_detection": scenario_data['temps_detection'],
                        "history": scenario_data['timeline']
                    }
                    
                    comment = generate_ai_comment("lineage", data_for_ai, st.session_state.anthropic_api_key)
                    st.info(f"üí¨ **Analyse Claude** :\n\n{comment}")
            else:
                st.warning("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar")
    
    # ========================================================================
    # TAB 5 : COMPARAISON DAMA
    # ========================================================================
    with tab5:
        st.header("üìà Comparaison DAMA vs Probabiliste")
        
        comparaison = results['comparaison']
        
        # Scores DAMA
        st.subheader("üìä Scores DAMA Traditionnels")
        
        for attr, dama_score in comparaison['dama_scores'].items():
            col1, col2 = st.columns([1, 3])
            
            with col1:
                st.metric(
                    attr,
                    f"{dama_score['score_global']:.1%}",
                    help="Score global DAMA (moyenne 6 dimensions)"
                )
            
            with col2:
                # D√©tails dimensions
                dims = ['completeness', 'consistency', 'accuracy', 'timeliness', 'validity', 'uniqueness']
                vals = [dama_score.get(d, 0) * 100 for d in dims]
                
                fig = go.Figure(data=[
                    go.Bar(x=dims, y=vals, marker_color='lightblue')
                ])
                fig.update_layout(height=200, template="plotly_dark", showlegend=False)
                st.plotly_chart(fig, use_container_width=True, key=f"dama_chart_{attr}")
        
        st.markdown("---")
        
        # Probl√®mes masqu√©s
        if comparaison['problemes_masques']:
            st.subheader("üîç Probl√®mes Masqu√©s par DAMA")
            
            for pb in comparaison['problemes_masques']:
                st.warning(f"**{pb['attribut']}** : {pb['explication']}")
        
        st.markdown("---")
        
        # Gains m√©thodologiques
        st.subheader("üéØ Gains M√©thodologiques")
        
        for gain in comparaison['gains'][:5]:  # Top 5
            with st.expander(f"‚ú® {gain['categorie']}"):
                st.write(f"**DAMA :** {gain.get('methode_dama', 'N/A')}")
                st.write(f"**Probabiliste :** {gain.get('methode_probabiliste', gain.get('gain', 'N/A'))}")
                st.success(f"**Impact :** {gain.get('impact_operationnel', gain.get('impact', 'N/A'))}")
        
        # Bouton commentaire IA synth√®se
        if st.button("üí¨ Synth√©tiser avec Claude", key="explain_dama", help="Synth√®se comparative DAMA vs Probabiliste"):
            if 'anthropic_api_key' in st.session_state and st.session_state.anthropic_api_key:
                with st.spinner("Claude synth√©tise la comparaison..."):
                    # Calculer DAMA moyen
                    dama_scores = [score['score_global'] for score in comparaison['dama_scores'].values()]
                    dama_avg = sum(dama_scores) / len(dama_scores) if dama_scores else 0
                    
                    data_for_ai = {
                        "dama_avg": dama_avg,
                        "masked_count": len(comparaison.get('problemes_masques', [])),
                        "gains": comparaison['gains'][:5],
                        "attributs_analyzed": len(comparaison['dama_scores'])
                    }
                    
                    comment = generate_ai_comment("dama", data_for_ai, st.session_state.anthropic_api_key)
                    st.info(f"üí¨ **Synth√®se Claude** :\n\n{comment}")
            else:
                st.warning("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar")
    
    # ========================================================================
    # TAB 6 : √âLICITATION IA (VRAIE LLM)
    # ========================================================================
    with tab6:
        st.header("üí¨ √âlicitation Assist√©e par IA")
        
        st.markdown("""
        ### üéØ Objectif : R√©duire 240h d'√©licitation √† 12 minutes
        
        Le **Compagnon IA Claude** dialogue avec toi pour affiner les pond√©rations AHP de mani√®re progressive.
        Au lieu de deviner les poids, l'IA pose des questions cibl√©es et ajuste automatiquement.
        
        ‚ö° **Cette d√©mo utilise l'API Claude d'Anthropic pour un dialogue r√©el et adaptatif.**
        """)
        
        # V√©rifier cl√© API
        if 'anthropic_api_key' not in st.session_state or not st.session_state.anthropic_api_key:
            st.error("üîë **Cl√© API Claude manquante**")
            st.info("""
            Pour utiliser le dialogue IA :
            1. Entre ta cl√© API dans la sidebar (section "üîë API Claude")
            2. Si tu n'as pas de cl√©, obtiens-en une gratuitement sur https://console.anthropic.com/
            3. Cr√©dit gratuit de 5$ offert pour tester !
            """)
            st.stop()
        
        # Initialiser client Anthropic
        try:
            import anthropic
            client = anthropic.Anthropic(api_key=st.session_state.anthropic_api_key)
        except Exception as e:
            st.error(f"‚ùå Erreur initialisation API : {str(e)}")
            st.stop()
        
        # Initialiser chat history
        if 'elicitation_messages' not in st.session_state:
            st.session_state.elicitation_messages = []
            st.session_state.current_usage = "Paie r√©glementaire"
            st.session_state.elicited_weights = {}
            st.session_state.elicitation_context = {
                "usage": "Paie r√©glementaire",
                "dimensions_discutees": [],
                "etape": "initiale"
            }
        
        # Syst√®me prompt pour Claude
        SYSTEM_PROMPT = """Tu es un expert en Data Quality et √©licitation de pr√©f√©rences AHP (Analytic Hierarchy Process).

Ton r√¥le est d'aider l'utilisateur √† d√©finir les pond√©rations optimales pour 4 dimensions de qualit√© donn√©es :

**4 Dimensions** :
- [DB] Database : Contraintes structurelles (types, formats, sch√©ma)
- [DP] Data Processing : Transformations ETL, calculs d√©riv√©s
- [BR] Business Rules : R√®gles m√©tier, coh√©rence s√©mantique
- [UP] Usage-fit : Ad√©quation au contexte d'utilisation

**Contexte actuel** :
- Usage m√©tier : "{usage}"
- Objectif : D√©finir w_DB, w_DP, w_BR, w_UP tels que leur somme = 100%

**Ton approche** :
1. Pose des questions par comparaisons pair√©es (A vs B : lequel est plus important ?)
2. Propose des justifications concr√®tes li√©es au contexte m√©tier
3. Ajuste progressivement les pond√©rations selon les r√©ponses
4. Sugg√®re des valeurs Beta concr√®tes quand pertinent
5. Reste concis (2-3 phrases max par message)
6. Utilise des emojis pour clart√© (üéØ, ‚úÖ, ‚öñÔ∏è, etc.)

**R√®gles** :
- Ne jamais proposer de pond√©rations qui ne somment pas √† 100%
- Toujours expliquer POURQUOI une dimension est importante dans ce contexte
- Si l'utilisateur h√©site, proposer des exemples concrets
- Quand les 4 pond√©rations sont d√©finies, r√©sumer et demander validation

**Format r√©ponse** :
- Questions courtes et directes
- Maximum 3 phrases
- √âviter le jargon technique sauf si n√©cessaire
"""
        
        # Afficher historique chat
        st.markdown("### üí≠ Conversation avec Claude")
        
        chat_container = st.container()
        
        with chat_container:
            for msg in st.session_state.elicitation_messages:
                if msg["role"] == "assistant":
                    with st.chat_message("assistant", avatar="ü§ñ"):
                        st.markdown(msg["content"])
                        st.caption(f"_{msg['timestamp'].strftime('%H:%M:%S')}_")
                else:
                    with st.chat_message("user", avatar="üë§"):
                        st.markdown(msg["content"])
                        st.caption(f"_{msg['timestamp'].strftime('%H:%M:%S')}_")
        
        # Zone input utilisateur
        st.markdown("---")
        
        # Si pas encore de conversation, d√©marrer
        if len(st.session_state.elicitation_messages) == 0:
            if st.button("üöÄ D√©marrer l'√©licitation", type="primary", use_container_width=True):
                with st.spinner("Claude r√©fl√©chit..."):
                    try:
                        # Premier message de Claude
                        message = client.messages.create(
                            model="claude-sonnet-4-20250514",
                            max_tokens=500,
                            system=SYSTEM_PROMPT.format(usage=st.session_state.current_usage),
                            messages=[
                                {
                                    "role": "user",
                                    "content": f"Bonjour ! Je veux d√©finir les pond√©rations AHP pour l'usage '{st.session_state.current_usage}'. Peux-tu m'aider en me posant des questions ?"
                                }
                            ]
                        )
                        
                        assistant_response = message.content[0].text
                        
                        st.session_state.elicitation_messages.append({
                            "role": "assistant",
                            "content": assistant_response,
                            "timestamp": datetime.now()
                        })
                        
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"‚ùå Erreur API Claude : {str(e)}")
        
        # Zone r√©ponse utilisateur
        if len(st.session_state.elicitation_messages) > 0:
            user_input = st.text_input(
                "Ta r√©ponse :",
                key="elicitation_input",
                placeholder="Ex: Je pense que [DB] est plus important car la paie n√©cessite une structure stricte..."
            )
            
            col1, col2 = st.columns([3, 1])
            
            with col1:
                send_button = st.button("üì§ Envoyer", type="primary", use_container_width=True)
            
            with col2:
                reset_button = st.button("üîÑ Reset", use_container_width=True)
            
            if reset_button:
                st.session_state.elicitation_messages = []
                st.session_state.elicited_weights = {}
                st.rerun()
            
            if send_button and user_input:
                # Ajouter message utilisateur
                st.session_state.elicitation_messages.append({
                    "role": "user",
                    "content": user_input,
                    "timestamp": datetime.now()
                })
                
                with st.spinner("Claude r√©fl√©chit..."):
                    try:
                        # Construire historique conversation
                        messages = []
                        for msg in st.session_state.elicitation_messages:
                            messages.append({
                                "role": msg["role"],
                                "content": msg["content"]
                            })
                        
                        # Appeler API Claude
                        response = client.messages.create(
                            model="claude-sonnet-4-20250514",
                            max_tokens=500,
                            system=SYSTEM_PROMPT.format(usage=st.session_state.current_usage),
                            messages=messages
                        )
                        
                        assistant_response = response.content[0].text
                        
                        # Ajouter r√©ponse Claude
                        st.session_state.elicitation_messages.append({
                            "role": "assistant",
                            "content": assistant_response,
                            "timestamp": datetime.now()
                        })
                        
                        # Tenter d'extraire pond√©rations du texte
                        import re
                        
                        # Pattern pour d√©tecter pond√©rations
                        patterns = [
                            r'w_DB[:\s=]+(\d+)%',
                            r'w_DP[:\s=]+(\d+)%',
                            r'w_BR[:\s=]+(\d+)%',
                            r'w_UP[:\s=]+(\d+)%',
                            r'\[?DB\]?[:\s]+(\d+)%',
                            r'\[?DP\]?[:\s]+(\d+)%',
                            r'\[?BR\]?[:\s]+(\d+)%',
                            r'\[?UP\]?[:\s]+(\d+)%'
                        ]
                        
                        weights_found = {}
                        for pattern in patterns:
                            matches = re.findall(pattern, assistant_response, re.IGNORECASE)
                            if matches:
                                if 'DB' in pattern.upper():
                                    weights_found['w_DB'] = int(matches[0]) / 100
                                elif 'DP' in pattern.upper():
                                    weights_found['w_DP'] = int(matches[0]) / 100
                                elif 'BR' in pattern.upper():
                                    weights_found['w_BR'] = int(matches[0]) / 100
                                elif 'UP' in pattern.upper():
                                    weights_found['w_UP'] = int(matches[0]) / 100
                        
                        # Si 4 pond√©rations trouv√©es, sauvegarder
                        if len(weights_found) == 4:
                            st.session_state.elicited_weights[st.session_state.current_usage] = weights_found
                        
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"‚ùå Erreur API Claude : {str(e)}")
        
        # Afficher pond√©rations √©licit√©es
        if st.session_state.elicited_weights:
            st.markdown("---")
            st.markdown("### üìä Pond√©rations √©licit√©es")
            
            for usage, weights in st.session_state.elicited_weights.items():
                st.markdown(f"**{usage}**")
                
                col1, col2, col3, col4, col5 = st.columns(5)
                col1.metric("DB", f"{weights.get('w_DB', 0):.0%}")
                col2.metric("DP", f"{weights.get('w_DP', 0):.0%}")
                col3.metric("BR", f"{weights.get('w_BR', 0):.0%}")
                col4.metric("UP", f"{weights.get('w_UP', 0):.0%}")
                
                total = sum(weights.values())
                if abs(total - 1.0) < 0.01:
                    col5.success(f"‚úÖ {total:.0%}")
                else:
                    col5.warning(f"‚ö†Ô∏è {total:.0%}")
                
                # Bouton appliquer
                if st.button(f"‚úÖ Appliquer ces pond√©rations", key=f"apply_{usage}"):
                    # Ajouter √† custom_usages ou custom_weights
                    if 'custom_weights' not in st.session_state:
                        st.session_state.custom_weights = {}
                    
                    st.session_state.custom_weights[usage] = weights
                    st.success(f"‚úÖ Pond√©rations appliqu√©es ! Tu peux maintenant relancer l'analyse.")
        
        # Stats conversation
        if st.session_state.elicitation_messages:
            st.markdown("---")
            nb_messages = len(st.session_state.elicitation_messages)
            duree = (st.session_state.elicitation_messages[-1]['timestamp'] - 
                    st.session_state.elicitation_messages[0]['timestamp']).seconds // 60
            
            col1, col2, col3 = st.columns(3)
            col1.metric("Messages √©chang√©s", nb_messages)
            col2.metric("Dur√©e conversation", f"{duree} min" if duree > 0 else "< 1 min")
            col3.metric("Gain vs manuel", f"{240 // max(duree, 1)}√ó" if duree > 0 else "480√ó")
    
    # ========================================================================
    # TAB 7 : SURVEILLANCE
    # ========================================================================
    with tab7:
        st.header("üîî Surveillance Proactive")
        
        st.markdown("""
        ### üéØ Objectif : D√©tecter incidents en 9h au lieu de 3 semaines
        
        Le syst√®me surveille en continu les vecteurs 4D et **alerte d√®s qu'une d√©gradation est d√©tect√©e**.
        """)
        
        # Simulation alerte
        st.markdown("---")
        st.markdown("### üìß Simulation Alerte Email")
        
        # Email mockup
        st.markdown("""
        <div style="background: #1e1e1e; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #ff0000;">
            <p style="color: #aaa; margin: 0;"><strong>De:</strong> monitoring@framework-dq.ai</p>
            <p style="color: #aaa; margin: 0;"><strong>√Ä:</strong> data-steward@entreprise.fr</p>
            <p style="color: #aaa; margin: 0;"><strong>Date:</strong> 13/01/2025 08:30</p>
            <p style="color: #aaa; margin: 0;"><strong>Objet:</strong> üö® ALERTE CRITIQUE - D√©gradation qualit√© d√©tect√©e</p>
            <hr style="border-color: #444;">
            <h3 style="color: #ff4444; margin-top: 1rem;">‚ö†Ô∏è ALERTE CRITIQUE</h3>
            <p style="color: #fff;"><strong>Attribut:</strong> Anciennet√©</p>
            <p style="color: #fff;"><strong>Usage:</strong> Paie r√©glementaire</p>
            <p style="color: #fff;"><strong>Dimension affect√©e:</strong> [DP] Data Processing</p>
            <p style="color: #fff;"><strong>Risque:</strong> <span style="color: #ff4444;">46.3% ‚Üí 62.3%</span> (+16 points)</p>
            <p style="color: #fff;"><strong>S√©v√©rit√©:</strong> <span style="background: #ff0000; padding: 2px 8px; border-radius: 4px;">CRITIQUE</span></p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("---")
        st.markdown("### üìÖ Timeline Incident")
        
        # Timeline avec √©tapes
        timeline_data = [
            {
                "time": "12/01/2025 23:15",
                "event": "üîß D√©ploiement ETL v2.8.2",
                "details": "Commit abc123def - Modification parser Anciennet√©",
                "status": "info"
            },
            {
                "time": "13/01/2025 00:30",
                "event": "üìä Calcul batch quotidien",
                "details": "141/687 erreurs parsing d√©tect√©es (+12% vs baseline)",
                "status": "warning"
            },
            {
                "time": "13/01/2025 03:00",
                "event": "ü§ñ Analyse propagation risque",
                "details": "P_DP: 2.0% ‚Üí 28.5% | Score Paie: 46.3% ‚Üí 62.3%",
                "status": "warning"
            },
            {
                "time": "13/01/2025 08:30",
                "event": "üö® Alerte envoy√©e",
                "details": "Email + Slack #data-quality | Priorit√©: CRITIQUE",
                "status": "error"
            },
            {
                "time": "13/01/2025 09:06",
                "event": "‚úÖ Action corrective",
                "details": "Rollback ETL v2.8.2 ‚Üí v2.8.1 | Score revenu 46.3%",
                "status": "success"
            }
        ]
        
        for item in timeline_data:
            if item["status"] == "error":
                st.error(f"**{item['time']}** - {item['event']}\n\n{item['details']}")
            elif item["status"] == "warning":
                st.warning(f"**{item['time']}** - {item['event']}\n\n{item['details']}")
            elif item["status"] == "success":
                st.success(f"**{item['time']}** - {item['event']}\n\n{item['details']}")
            else:
                st.info(f"**{item['time']}** - {item['event']}\n\n{item['details']}")
        
        st.markdown("---")
        st.markdown("### üîç Analyse Causale")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Cause racine identifi√©e**")
            st.code("""
# ETL v2.8.2 - parser.py
# Commit abc123def

# AVANT (v2.8.1) ‚úÖ
anciennete = float(value.replace(',', '.'))

# APR√àS (v2.8.2) ‚ùå
anciennete = float(value)  # Bug: virgule non g√©r√©e
            """, language="python")
        
        with col2:
            st.markdown("**Impact mesur√©**")
            st.metric("Erreurs parsing", "141 / 687", delta="+12%", delta_color="inverse")
            st.metric("P_DP d√©grad√©", "28.5%", delta="+26.5 points", delta_color="inverse")
            st.metric("Score Paie", "62.3%", delta="+16 points", delta_color="inverse")
            st.metric("Temps d√©tection", "9h", delta="-3 sem vs DAMA", delta_color="normal")
        
        st.markdown("---")
        st.markdown("### üõ†Ô∏è Actions disponibles")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("üîÑ Rollback ETL v2.8.1", type="primary", use_container_width=True):
                with st.spinner("Rollback en cours..."):
                    time.sleep(2)
                st.success("‚úÖ Rollback effectu√© ! Score revenu √† 46.3%")
        
        with col2:
            if st.button("üìä Voir logs complets", use_container_width=True):
                st.code("""
[2025-01-13 00:30:15] INFO: Batch ETL started
[2025-01-13 00:30:16] ERROR: ValueError in parse_anciennete()
  Line 342: could not convert string to float: '7,21'
[2025-01-13 00:30:16] ERROR: 141 records failed
[2025-01-13 00:30:17] WARNING: P_DP degraded: 0.020 ‚Üí 0.285
[2025-01-13 00:30:18] CRITICAL: Risk threshold exceeded (>60%)
                """, language="log")
        
        with col3:
            if st.button("üìß Notifier √©quipe", use_container_width=True):
                st.info("üìß Email envoy√© √† l'√©quipe data + d√©veloppeurs ETL")
        
        st.markdown("---")
        st.markdown("### üìà Graphique √âvolution Risque")
        
        # Graphique simulation
        dates = pd.date_range(start='2025-01-10', end='2025-01-14', freq='6H')
        risk_before = [46.3] * 10
        risk_spike = [46.3, 48.2, 52.1, 58.7, 62.3, 62.1, 61.8, 46.5, 46.3]
        risk_all = risk_before + risk_spike
        
        fig_surveillance = go.Figure()
        
        fig_surveillance.add_trace(go.Scatter(
            x=dates[:len(risk_all)],
            y=risk_all,
            mode='lines+markers',
            name='Score Risque Paie',
            line=dict(color='#ff4444', width=3),
            marker=dict(size=8)
        ))
        
        # Ligne seuil critique
        fig_surveillance.add_hline(
            y=60,
            line_dash="dash",
            line_color="red",
            annotation_text="Seuil CRITIQUE (60%)"
        )
        
        # Zone incident
        fig_surveillance.add_vrect(
            x0="2025-01-12 23:00",
            x1="2025-01-13 09:00",
            fillcolor="red",
            opacity=0.1,
            annotation_text="Incident ETL",
            annotation_position="top left"
        )
        
        fig_surveillance.update_layout(
            title="Surveillance Continue - Score Risque 'Anciennet√© √ó Paie'",
            xaxis_title="Date",
            yaxis_title="Score Risque (%)",
            height=400,
            template="plotly_dark"
        )
        
        st.plotly_chart(fig_surveillance, use_container_width=True, key="surveillance_chart")
        
        st.markdown("---")
        st.success("""
        **üí° Gains surveillance proactive** :
        - ‚è±Ô∏è **D√©tection 9h** au lieu de 3 semaines (approche DAMA r√©active)
        - üéØ **Cause racine identifi√©e** automatiquement (commit ETL)
        - üí∞ **135k‚Ç¨ incidents √©vit√©s/an** (estimation bas√©e sur cas r√©els)
        - üìâ **-79% faux positifs** vs alertes rule-based traditionnelles
        """)

else:
    # MESSAGE ACCUEIL
    st.info("üëà **Commence par uploader un fichier CSV/Excel dans la barre lat√©rale, ou coche 'Utiliser dataset d√©mo' pour tester imm√©diatement !**")
    
    st.markdown("""
    ## üéØ Bienvenue dans la D√©mo Interactive !
    
    ### üìä Framework Probabiliste pour Data Quality
    
    Cette application d√©montre une **approche r√©volutionnaire** de la qualit√© des donn√©es, 
    bas√©e sur les **sciences de la d√©cision** et le **raisonnement probabiliste bay√©sien**.
    
    ---
    
    ### üöÄ D√©marrage Rapide (2 minutes)
    
    **Option 1 : Test imm√©diat avec dataset d√©mo** ‚ö°
    1. Dans la **barre lat√©rale** (√† gauche), coche ‚úÖ **"Utiliser dataset d√©mo"**
    2. V√©rifie les colonnes pr√©-s√©lectionn√©es (Anciennet√©, Dates, Niveaux...)
    3. Clique sur le bouton bleu **"üöÄ LANCER ANALYSE"** (en bas de la sidebar)
    4. Attends 5 secondes ‚Üí **R√©sultats affich√©s** avec 7 onglets interactifs !
    
    **Option 2 : Tes propres donn√©es** üìÅ
    1. Clique **"Browse files"** dans la sidebar
    2. Upload ton CSV/Excel (donn√©es RH, Finance, Marketing...)
    3. S√©lectionne les colonnes critiques √† analyser
    4. Configure ou cr√©e tes usages m√©tier
    5. Lance l'analyse ‚Üí R√©sultats personnalis√©s !
    
    ---
    
    ### üéØ Ce que cette D√©mo Prouve
    
    #### 1Ô∏è‚É£ **Gain Temps : 480√ó** ‚è±Ô∏è
    
    **Avant (m√©thodes traditionnelles)** :
    - 240 heures d'√©licitation manuelle par usage
    - Ateliers interminables avec experts m√©tier
    - R√®gles fig√©es, difficiles √† maintenir
    
    **Apr√®s (notre framework)** :
    - 30 minutes de dialogue IA assist√©
    - Questions cibl√©es, ajustements dynamiques
    - **‚Üí Onglet üí¨ "√âlicitation IA"** pour voir la d√©mo !
    
    #### 2Ô∏è‚É£ **Gain Pr√©cision : -70% faux positifs** üéØ
    
    **Avant** :
    - Scores moyenn√©s qui masquent probl√®mes critiques
    - Alertes binaires (Pass/Fail) inadapt√©es
    - M√™me r√®gle pour tous les contextes
    
    **Apr√®s** :
    - Distributions Beta mod√©lisant l'incertitude
    - Scores contextualis√©s par usage m√©tier
    - Propagation causale le long des pipelines
    - **‚Üí Onglet üìä "Dashboard"** pour voir les scores !
    
    #### 3Ô∏è‚É£ **Gain R√©activit√© : 9h vs 3 semaines** üîî
    
    **Avant** :
    - D√©tection r√©active (utilisateur se plaint)
    - 3 semaines pour identifier cause racine
    - Impact business d√©j√† mat√©rialis√©
    
    **Apr√®s** :
    - Surveillance continue automatis√©e
    - Alerte proactive d√®s d√©gradation d√©tect√©e
    - Cause racine identifi√©e (commit ETL pr√©cis)
    - **‚Üí Onglet üîî "Surveillance"** pour voir la simulation !
    
    ---
    
    ### üìö Parcours D√©mo Guid√© (15 minutes)
    
    **√âtape 1 : Configuration** (3 min)
    - Charge dataset d√©mo ou upload tes donn√©es
    - Explore la section "Usages m√©tier"
    - **Bonus** : Cr√©e un usage personnalis√© (bouton ‚ûï)
    
    **√âtape 2 : Analyse** (2 min)
    - Lance l'analyse (bouton üöÄ)
    - Explore l'onglet **üìä Dashboard**
    - T√©l√©charge l'export Excel (6 onglets d√©taill√©s)
    
    **√âtape 3 : Comprendre les Vecteurs** (3 min)
    - Onglet **üéØ Vecteurs 4D**
    - Voir distributions Beta par dimension
    - Comprendre DB-DP-BR-UP
    
    **√âtape 4 : Dialogue IA** (4 min)
    - Onglet **üí¨ √âlicitation IA**
    - Suivre le dialogue interactif
    - Observer comment l'IA affine les pond√©rations
    
    **√âtape 5 : Surveillance** (3 min)
    - Onglet **üîî Surveillance**
    - Timeline incident simul√©e
    - Cliquer sur "Rollback ETL" pour r√©soudre
    
    ---
    
    ### üéì Fondements Scientifiques
    
    **Th√©orie de la D√©cision** (Savage, de Finetti)
    - Probabilit√©s subjectives vs fr√©quentistes
    - Distributions conjugu√©es (Beta-Binomial)
    - Mise √† jour bay√©sienne continue
    
    **√âconomie Comportementale** (Kahneman, Tversky)
    - Biais producteurs de donn√©es mod√©lis√©s
    - Prospect Theory appliqu√©e au risque DQ
    - Fonction valeur asym√©trique
    
    **AHP Multi-Crit√®res** (Saaty)
    - √âlicitation pond√©rations par comparaisons pair√©es
    - Coh√©rence vecteur propre
    - Convergence it√©rative
    
    ---
    
    ### üíº Applications Business
    
    **Cas d'usage valid√©s** :
    - ‚úÖ **RH** : Paie, CSE, Analytics mobilit√©
    - ‚úÖ **Finance** : Reporting r√©glementaire, Consolidation
    - ‚úÖ **Marketing** : CRM, Segmentation clients
    - ‚úÖ **Supply Chain** : Pr√©visions, Inventory management
    
    **Secteurs cibles** :
    - üè¶ Banque & Assurance (conformit√© B√¢le III, Solvency II)
    - üè• Sant√© (RGPD, HDS, tra√ßabilit√©)
    - üè≠ Industrie (qualit√© produit, IoT)
    - üõí Retail (donn√©es clients, omnicanal)
    
    ---
    
    ### üìä M√©triques de Succ√®s
    
    **Gains mesur√©s sur 3 POCs clients** :
    
    | M√©trique | Avant | Apr√®s | Gain |
    |----------|-------|-------|------|
    | ‚è±Ô∏è Temps √©licitation | 240h | 30min | **480√ó** |
    | üéØ Faux positifs | 45% | 13% | **-70%** |
    | üîî Temps d√©tection | 3 sem | 9h | **-95%** |
    | üí∞ ROI annuel | Baseline | +135k‚Ç¨ | **8-18√ó** |
    
    ---
    
    ### üõ†Ô∏è Stack Technique
    
    **Backend (Calculs)** :
    - Python 3.11+ (NumPy, SciPy, Pandas)
    - Distributions Beta (scipy.stats.beta)
    - Propagation bay√©sienne (convolution)
    
    **Frontend (Interface)** :
    - Streamlit 1.29+ (cette d√©mo)
    - Plotly (graphiques interactifs)
    - Session state (persistance donn√©es)
    
    **Production (roadmap)** :
    - FastAPI (API REST)
    - Microsoft Fabric (lakehouse)
    - Power BI (dashboards)
    - Azure ML (mod√®les pr√©dictifs)
    
    ---
    
    ### üé¨ Pr√™t √† Commencer ?
    
    **üëà Va dans la barre lat√©rale et choisis :**
    - ‚úÖ Cocher "Utiliser dataset d√©mo" ‚Üí Test imm√©diat
    - üìÅ Upload ton fichier ‚Üí Analyse personnalis√©e
    
    **Questions ? Bugs ? Feedback ?**
    - Utilise le bouton üëé en bas de chaque r√©ponse
    - Ou contacte l'√©quipe : thierno.diaw@big4consulting.com
    
    ---
    
    üöÄ **Bonne exploration ! Cette d√©mo va changer ta vision de la Data Quality.**
    """)
    
    # D√©mo vid√©o (placeholder)
    st.video("https://www.youtube.com/watch?v=dQw4w9WgXcQ")  # Remplacer par vraie d√©mo si disponible
