"""
Framework Probabiliste DQ - Application Streamlit
Interface web compl√®te pour analyse qualit√© donn√©es

Lancement : streamlit run app.py
"""

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from typing import Dict, List
import sys
import os
from datetime import datetime, timedelta
import time
import json

# Import CSS minimaliste Apple
from streamlit_minimal_css import apply_minimal_css

# Import Anthropic pour LLM
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
import anthropic
import json

# Ajouter path backend
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

# Import moteur
from backend.engine import (
    analyze_dataset,
    compute_all_beta_vectors,
    elicit_weights_auto,
    compute_risk_scores,
    get_top_priorities,
    simulate_lineage,
    compare_dama_vs_probabiliste
)

# Import syst√®me d√©tection anomalies
from streamlit_anomaly_detection import render_anomaly_detection_tab


# ============================================================================
# CONFIGURATION PAGE
# ============================================================================

st.set_page_config(
    page_title="Framework Probabiliste DQ",
    page_icon="üéØ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Appliquer CSS minimaliste
apply_minimal_css()



# ============================================================================
# SESSION STATE (pour conserver donn√©es entre interactions)
# ============================================================================

if 'df' not in st.session_state:
    st.session_state.df = None
if 'results' not in st.session_state:
    st.session_state.results = None
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False


# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================

def get_risk_color(score: float) -> str:
    """Retourne couleur selon score risque"""
    if score >= 0.40:
        return "#ff0000"  # Rouge
    elif score >= 0.25:
        return "#ff9900"  # Orange
    elif score >= 0.15:
        return "#ffcc00"  # Jaune
    elif score >= 0.10:
        return "#90ee90"  # Vert clair
    else:
        return "#00ff00"  # Vert


def call_claude_api(messages: List[Dict], system_prompt: str = None) -> str:
    """
    Appelle Claude API pour dialogue √©licitation
    
    Args:
        messages: Historique conversation [{"role": "user/assistant", "content": "..."}]
        system_prompt: Prompt syst√®me optionnel
    
    Returns:
        R√©ponse de Claude
    """
    try:
        # V√©rifier si cl√© API configur√©e
        api_key = st.session_state.get('claude_api_key', None)
        
        if not api_key:
            return """‚ö†Ô∏è **Cl√© API Claude non configur√©e**
            
Pour activer le dialogue IA r√©el, configure ta cl√© API dans la sidebar :
1. Va sur https://console.anthropic.com/
2. Cr√©e une cl√© API
3. Colle-la dans le champ "Cl√© API Claude" (sidebar)

**En attendant, voici une r√©ponse simul√©e** :
‚úÖ Not√© ! J'augmente **w_DB √† 40%** pour la Paie.

Les donn√©es structurelles (VARCHAR au lieu DECIMAL) sont effectivement critiques pour la conformit√© r√©glementaire de la paie.

Quelle est la **deuxi√®me dimension la plus importante** pour ce cas d'usage ?"""
        
        # Import dynamique (seulement si cl√© pr√©sente)
        try:
            import anthropic
        except ImportError:
            return """‚ö†Ô∏è **Module Anthropic non install√©**
            
Pour installer : `pip install anthropic`

R√©ponse simul√©e en attendant..."""
        
        # Appeler API Claude
        client = anthropic.Anthropic(api_key=api_key)
        
        # Prompt syst√®me par d√©faut
        if not system_prompt:
            system_prompt = """Tu es un expert en Data Quality et en sciences de la d√©cision. 
Tu aides √† √©liciter les pond√©rations AHP pour un framework probabiliste de qualit√© des donn√©es.

Tu poses des questions progressives pour comprendre l'importance relative des 4 dimensions :
- [DB] Database : Contraintes structurelles (types, nullable, cl√©s)
- [DP] Data Processing : Erreurs transformations ETL
- [BR] Business Rules : Violations r√®gles m√©tier
- [UP] Usage-fit : Inad√©quation contextuelle

Ton style :
- Questions courtes et cibl√©es
- Exemples concrets
- Validation des choix utilisateur
- Propositions de valeurs Beta si pertinent

Contexte actuel : √âlicitation pour usage "Paie r√©glementaire" (conformit√© l√©gale stricte)."""
        
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1000,
            system=system_prompt,
            messages=messages
        )
        
        # Extraire texte r√©ponse
        return response.content[0].text
        
    except Exception as e:
        return f"""‚ö†Ô∏è **Erreur API Claude** : {str(e)}

V√©rifie que :
- Ta cl√© API est valide
- Tu as des cr√©dits disponibles
- Ta connexion internet fonctionne

R√©ponse simul√©e en attendant..."""


def get_risk_color(score: float) -> str:
    """Retourne couleur selon score risque"""
    if score >= 0.40:
        return "#ff0000"  # Rouge
    elif score >= 0.25:
        return "#ff9900"  # Orange
    elif score >= 0.15:
        return "#ffcc00"  # Jaune
    elif score >= 0.10:
        return "#90ee90"  # Vert clair
    else:
        return "#00ff00"  # Vert


def create_vector_chart(vector_4d: Dict) -> go.Figure:
    """Cr√©e graphique barres vecteur 4D"""
    dimensions = ['DB', 'DP', 'BR', 'UP']
    values = [
        vector_4d.get('P_DB', 0) * 100,
        vector_4d.get('P_DP', 0) * 100,
        vector_4d.get('P_BR', 0) * 100,
        vector_4d.get('P_UP', 0) * 100
    ]
    
    colors = [get_risk_color(v/100) for v in values]
    
    fig = go.Figure(data=[
        go.Bar(
            x=dimensions,
            y=values,
            marker=dict(color=colors),
            text=[f"{v:.1f}%" for v in values],
            textposition='outside'
        )
    ])
    
    fig.update_layout(
        title="Vecteur Qualit√© 4D",
        xaxis_title="Dimensions",
        yaxis_title="Taux erreur (%)",
        height=400,
        template="plotly_dark"
    )
    
    return fig


def export_to_excel(results: Dict, output_path: str = "resultats_framework_dq.xlsx"):
    """
    Exporte r√©sultats complets vers Excel multi-onglets
    """
    from datetime import datetime
    import openpyxl
    from openpyxl.styles import Font, PatternFill, Alignment
    
    # Ajouter timestamp au nom fichier
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f"resultats_framework_dq_{timestamp}.xlsx"
    
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        
        # ====================================================================
        # ONGLET 1 : VECTEURS 4D
        # ====================================================================
        vecteurs_data = []
        for attr, vector in results['vecteurs_4d'].items():
            vecteurs_data.append({
                'Attribut': attr,
                'P_DB': vector.get('P_DB', 0),
                'alpha_DB': vector.get('alpha_DB', 0),
                'beta_DB': vector.get('beta_DB', 0),
                'std_DB': vector.get('std_DB', 0),
                'ci_lower_DB': vector.get('ci_lower_DB', 0),
                'ci_upper_DB': vector.get('ci_upper_DB', 0),
                'P_DP': vector.get('P_DP', 0),
                'alpha_DP': vector.get('alpha_DP', 0),
                'beta_DP': vector.get('beta_DP', 0),
                'std_DP': vector.get('std_DP', 0),
                'ci_lower_DP': vector.get('ci_lower_DP', 0),
                'ci_upper_DP': vector.get('ci_upper_DP', 0),
                'P_BR': vector.get('P_BR', 0),
                'alpha_BR': vector.get('alpha_BR', 0),
                'beta_BR': vector.get('beta_BR', 0),
                'std_BR': vector.get('std_BR', 0),
                'ci_lower_BR': vector.get('ci_lower_BR', 0),
                'ci_upper_BR': vector.get('ci_upper_BR', 0),
                'P_UP': vector.get('P_UP', 0),
                'alpha_UP': vector.get('alpha_UP', 0),
                'beta_UP': vector.get('beta_UP', 0),
                'std_UP': vector.get('std_UP', 0),
                'ci_lower_UP': vector.get('ci_lower_UP', 0),
                'ci_upper_UP': vector.get('ci_upper_UP', 0)
            })
        
        df_vecteurs = pd.DataFrame(vecteurs_data)
        df_vecteurs.to_excel(writer, sheet_name='Vecteurs_4D', index=False)
        
        # ====================================================================
        # ONGLET 2 : SCORES RISQUE
        # ====================================================================
        scores_data = []
        for key, score in results['scores'].items():
            parts = key.rsplit('_', 1)
            attribut = parts[0] if len(parts) > 1 else key
            usage = parts[1] if len(parts) > 1 else "Unknown"
            
            scores_data.append({
                'Attribut': attribut,
                'Usage': usage,
                'Score_Risque': score,
                'Score_Pourcent': f"{score:.1%}",
                'Criticit√©': 'CRITIQUE' if score > 0.4 else '√âLEV√â' if score > 0.25 else 'MOYEN' if score > 0.15 else 'ACCEPTABLE'
            })
        
        df_scores = pd.DataFrame(scores_data)
        df_scores_pivot = df_scores.pivot(index='Attribut', columns='Usage', values='Score_Risque')
        df_scores_pivot.to_excel(writer, sheet_name='Scores_Risque')
        
        # ====================================================================
        # ONGLET 3 : POND√âRATIONS USAGES
        # ====================================================================
        weights_data = []
        for usage_name, weights in results['weights'].items():
            weights_data.append({
                'Usage': usage_name,
                'w_DB': weights.get('w_DB', 0),
                'w_DP': weights.get('w_DP', 0),
                'w_BR': weights.get('w_BR', 0),
                'w_UP': weights.get('w_UP', 0),
                'Rationale': weights.get('rationale', '')
            })
        
        df_weights = pd.DataFrame(weights_data)
        df_weights.to_excel(writer, sheet_name='Pond√©rations_AHP', index=False)
        
        # ====================================================================
        # ONGLET 4 : PRIORIT√âS ACTIONS
        # ====================================================================
        priorities_data = []
        for i, priority in enumerate(results['top_priorities'], 1):
            priorities_data.append({
                'Rang': i,
                'Attribut': priority['attribut'],
                'Usage': priority['usage'],
                'Score_Risque': priority['score'],
                'Score_Pourcent': f"{priority['score']:.1%}",
                'S√©v√©rit√©': priority['severite'],
                'Enregistrements_Affect√©s': priority['records_affected'],
                'Impact_Mensuel_‚Ç¨': priority['impact_mensuel'],
                'Action_Principale': priority['actions'][0] if priority.get('actions') else ''
            })
        
        df_priorities = pd.DataFrame(priorities_data)
        df_priorities.to_excel(writer, sheet_name='Priorit√©s_Actions', index=False)
        
        # ====================================================================
        # ONGLET 5 : LINEAGE (si disponible)
        # ====================================================================
        if results.get('lineage'):
            lineage = results['lineage']
            lineage_data = []
            
            for step in lineage['history']:
                lineage_data.append({
                    '√âtape': step['etape'],
                    'P_DB': step['P_DB'],
                    'P_DP': step['P_DP'],
                    'P_BR': step['P_BR'],
                    'P_UP': step['P_UP']
                })
            
            df_lineage = pd.DataFrame(lineage_data)
            df_lineage.to_excel(writer, sheet_name='Lineage_Propagation', index=False)
            
            # M√©triques lineage
            lineage_summary = pd.DataFrame([{
                'Risque_Source': lineage['risk_source'],
                'Risque_Final': lineage['risk_final'],
                'Delta_Absolu': lineage['delta']['delta_absolute'],
                'Delta_Relatif': lineage['delta']['delta_relative'],
                'Interpr√©tation': lineage['delta']['interpretation']
            }])
            lineage_summary.to_excel(writer, sheet_name='Lineage_Summary', index=False)
        
        # ====================================================================
        # ONGLET 6 : COMPARAISON DAMA
        # ====================================================================
        dama_data = []
        for attr, dama_score in results['comparaison']['dama_scores'].items():
            dama_data.append({
                'Attribut': attr,
                'Score_DAMA_Global': dama_score['score_global'],
                'DAMA_Completeness': dama_score.get('completeness', 0),
                'DAMA_Consistency': dama_score.get('consistency', 0),
                'DAMA_Accuracy': dama_score.get('accuracy', 0),
                'DAMA_Validity': dama_score.get('validity', 0)
            })
        
        df_dama = pd.DataFrame(dama_data)
        df_dama.to_excel(writer, sheet_name='Comparaison_DAMA', index=False)
        
        # Probl√®mes masqu√©s
        if results['comparaison'].get('problemes_masques'):
            problemes_data = []
            for pb in results['comparaison']['problemes_masques']:
                problemes_data.append({
                    'Attribut': pb['attribut'],
                    'Type_Probl√®me': pb['type'],
                    'Explication': pb['explication']
                })
            
            df_problemes = pd.DataFrame(problemes_data)
            df_problemes.to_excel(writer, sheet_name='Probl√®mes_Masqu√©s_DAMA', index=False)
    
    return output_path


def export_to_csv(results: Dict, output_dir: str = "."):
    """
    Exporte r√©sultats en plusieurs fichiers CSV
    """
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    files_created = []
    
    # Vecteurs 4D
    vecteurs_data = []
    for attr, vector in results['vecteurs_4d'].items():
        vecteurs_data.append({
            'Attribut': attr,
            **{k: v for k, v in vector.items()}
        })
    
    df_vecteurs = pd.DataFrame(vecteurs_data)
    file_vecteurs = f"{output_dir}/vecteurs_4d_{timestamp}.csv"
    df_vecteurs.to_csv(file_vecteurs, index=False)
    files_created.append(file_vecteurs)
    
    # Scores
    scores_data = []
    for key, score in results['scores'].items():
        parts = key.rsplit('_', 1)
        scores_data.append({
            'Attribut': parts[0] if len(parts) > 1 else key,
            'Usage': parts[1] if len(parts) > 1 else "Unknown",
            'Score_Risque': score
        })
    
    df_scores = pd.DataFrame(scores_data)
    file_scores = f"{output_dir}/scores_risque_{timestamp}.csv"
    df_scores.to_csv(file_scores, index=False)
    files_created.append(file_scores)
    
    # Priorit√©s
    df_priorities = pd.DataFrame(results['top_priorities'])
    file_priorities = f"{output_dir}/priorites_{timestamp}.csv"
    df_priorities.to_csv(file_priorities, index=False)
    files_created.append(file_priorities)
    
    return files_created


def generate_ai_comment(context: str, data: Dict, api_key: str) -> str:
    """
    G√©n√®re un commentaire IA contextualis√© avec Claude
    
    Args:
        context: Type de section (heatmap, vector, priority, lineage, dama)
        data: Donn√©es √† commenter
        api_key: Cl√© API Anthropic
    
    Returns:
        Commentaire IA en texte
    """
    try:
        client = anthropic.Anthropic(api_key=api_key)
        
        # Construire prompt selon contexte
        prompts = {
            "heatmap": f"""Tu es un expert Data Quality. Analyse cette matrice de scores risque et fournis un commentaire de 3-4 phrases maximum.

Donn√©es: {json.dumps(data, indent=2)}

Concentre-toi sur:
1. Les patterns principaux (attributs/usages les plus risqu√©s)
2. Les diff√©rences de scores selon contexte (m√™me attribut, usages diff√©rents)
3. Une recommandation prioritaire

Ton style: Direct, p√©dagogique, actionnable. Utilise "Je remarque que...", "Cela signifie...", "Je recommande...".""",

            "vector": f"""Tu es un expert Data Quality. Commente ce vecteur 4D Beta et explique sa signification en 3-4 phrases maximum.

Attribut: {data.get('attribut', 'Unknown')}
Vecteurs:
- P_DB = {data.get('P_DB', 0):.1%} (Database)
- P_DP = {data.get('P_DP', 0):.1%} (Data Processing)  
- P_BR = {data.get('P_BR', 0):.1%} (Business Rules)
- P_UP = {data.get('P_UP', 0):.1%} (Usage-fit)

Explique:
1. Quelle dimension domine et pourquoi
2. Ce que r√©v√®le la distribution Beta sous-jacente (confiance)
3. Implication op√©rationnelle concr√®te

Style: Technique mais accessible, p√©dagogique.""",

            "priority": f"""Tu es un expert Data Quality. Commente cette priorit√© d'action en 3-4 phrases maximum.

Priorit√©: {data.get('attribut', 'Unknown')} √ó {data.get('usage', 'Unknown')}
Score risque: {data.get('score', 0):.1%}
S√©v√©rit√©: {data.get('severite', 'Unknown')}
Impact: {data.get('records_affected', 0)} enregistrements, {data.get('impact_mensuel', 0):,}‚Ç¨/mois

Fournis:
1. Pourquoi ce combo attribut√óusage est critique
2. Recommandation d'action imm√©diate concr√®te
3. Estimation ROI si correction

Style: Consultant senior, actionnable, chiffr√©.""",

            "lineage": f"""Tu es un expert Data Quality. Commente cette propagation de risque en 3-4 phrases maximum.

Propagation:
- Risque initial: {data.get('risk_source', 0):.1%}
- Risque final: {data.get('risk_final', 0):.1%}
- Delta: {data.get('delta_absolute', 0):.1%} points
- √âtapes pipeline: {len(data.get('history', []))}

Explique:
1. Comment le risque s'est aggrav√© dans le pipeline
2. Quelle √©tape a le plus d√©grad√© (si visible)
3. Gain d√©tection proactive (9h vs 3 semaines)

Style: Analyse forensique, factuelle, impactante.""",

            "dama": f"""Tu es un expert Data Quality. Compare les approches DAMA vs Probabiliste en 3-4 phrases maximum.

Comparaison:
- Score DAMA moyen: {data.get('dama_avg', 0):.1%}
- Probl√®mes masqu√©s: {data.get('masked_count', 0)}
- Gains m√©thodologiques: {len(data.get('gains', []))}

Synth√©tise:
1. Pourquoi DAMA masque certains probl√®mes critiques
2. L'avantage principal du framework probabiliste
3. Impact business concret (-70% faux positifs, ROI 8-18√ó)

Style: Comparatif mais objectif, factuel, business-oriented."""
        }
        
        prompt = prompts.get(context, prompts["heatmap"])
        
        # Appel API
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=500,
            system="Tu es un expert Data Quality qui commente des analyses avec un style direct, p√©dagogique et actionnable. Toujours 3-4 phrases maximum, sans jargon inutile.",
            messages=[{"role": "user", "content": prompt}]
        )
        
        return response.content[0].text
        
    except Exception as e:
        return f"‚ö†Ô∏è Erreur g√©n√©ration commentaire : {str(e)}"


def create_heatmap(scores: Dict) -> go.Figure:
    """Cr√©e heatmap scores [Attribut √ó Usage]"""
    # Parser scores en matrice
    attributs = set()
    usages = set()
    
    for key in scores.keys():
        parts = key.rsplit('_', 1)
        if len(parts) == 2:
            attributs.add(parts[0])
            usages.add(parts[1])
    
    attributs = sorted(list(attributs))
    usages = sorted(list(usages))
    
    # Construire matrice
    matrix = []
    for attr in attributs:
        row = []
        for usage in usages:
            key = f"{attr}_{usage}"
            row.append(scores.get(key, 0) * 100)
        matrix.append(row)
    
    fig = go.Figure(data=go.Heatmap(
        z=matrix,
        x=usages,
        y=attributs,
        colorscale='RdYlGn_r',
        text=[[f"{v:.1f}%" for v in row] for row in matrix],
        texttemplate='%{text}',
        textfont={"size": 12},
        colorbar=dict(title="Risque (%)")
    ))
    
    fig.update_layout(
        title="Matrice Scores Risque [Attribut √ó Usage]",
        height=400,
        template="plotly_dark"
    )
    
    return fig


# ============================================================================
# HEADER
# ============================================================================


# ============================================================================
# TOGGLE THEME
# ============================================================================

col_theme, col_main = st.columns([1, 11])
with col_theme:
    theme_icon = "üåô" if st.session_state.theme == 'light' else "‚òÄÔ∏è"
    if st.button(theme_icon, key="theme_toggle", help="Changer th√®me"):
        st.session_state.theme = 'light' if st.session_state.theme == 'dark' else 'dark'
        st.rerun()

st.title("üìä Framework Probabiliste DQ")

# Descriptif projet
with st.expander("üìã √Ä propos", expanded=False):
    st.markdown("""
    Framework probabiliste de Data Quality bas√© sur des distributions Beta.

    **Dimensions** : DB (Database), DP (Data Processing), BR (Business Rules), UP (Usage-fit)

    **Contact** : Thierno DIAW
    """)

st.markdown("---")


# ============================================================================
# SIDEBAR - CONFIGURATION
# ============================================================================

with st.sidebar:
    st.header("‚öôÔ∏è Configuration")
    
    # Configuration API Claude
    st.subheader("üîë API Claude (Anthropic)")
    
    # Tenter de charger cl√© API depuis Streamlit secrets (Cloud) ou variable env
    api_key_default = ""
    try:
        # Streamlit Cloud secrets
        api_key_default = st.secrets.get("ANTHROPIC_API_KEY", "")
    except:
        # Variable environnement locale
        api_key_default = os.getenv("ANTHROPIC_API_KEY", "")
    
    api_key_input = st.text_input(
        "Cl√© API Claude",
        type="password",
        value=api_key_default,
        help="N√©cessaire pour le dialogue IA dans l'onglet √âlicitation. Obtenir une cl√© : https://console.anthropic.com/",
        placeholder="sk-ant-api03-..."
    )
    
    if api_key_input:
        st.session_state.anthropic_api_key = api_key_input
        st.success("‚úÖ Cl√© API configur√©e")
    else:
        if 'anthropic_api_key' not in st.session_state:
            st.warning("‚ö†Ô∏è Onglet √âlicitation IA n√©cessite une cl√© API")
            with st.expander("üí° Comment obtenir une cl√© ?"):
                st.markdown("Obtenir une cl√© API sur https://console.anthropic.com/")
    
    st.markdown("---")
    
    # Upload fichier
    st.subheader("1Ô∏è‚É£ Upload Ton Dataset")
    uploaded_file = st.file_uploader(
        "üìÅ Choisis ton fichier CSV ou Excel",
        type=['csv', 'xlsx', 'xls'],
        help="Ton dataset √† analyser (tous secteurs : RH, Finance, Marketing, Supply Chain, etc.)"
    )
    
    # Si fichier upload√©
    if uploaded_file:
        # Charger donn√©es
        if uploaded_file.name.endswith('.csv'):
            st.session_state.df = pd.read_csv(uploaded_file)
        else:
            st.session_state.df = pd.read_excel(uploaded_file)
        st.success(f"‚úÖ {uploaded_file.name} charg√© ({len(st.session_state.df)} lignes)")
        
        # Afficher preview
        with st.expander("üëÅÔ∏è Preview donn√©es"):
            st.dataframe(st.session_state.df.head(10))
        
        # S√©lection colonnes
        st.subheader("2Ô∏è‚É£ Colonnes √† analyser")
        all_columns = st.session_state.df.columns.tolist()
        
        # Suggestions automatiques
        suggested = [col for col in all_columns if any(
            kw in col.lower() for kw in ['anciennete', 'date', 'level', 'salaire', 'prime', 'matricule']
        )]
        
        selected_columns = st.multiselect(
            "S√©lectionner colonnes",
            options=all_columns,
            default=suggested if suggested else all_columns[:3],
            help="Colonnes critiques pour analyse qualit√©"
        )
        
        # Configuration usages
        st.subheader("3Ô∏è‚É£ Usages m√©tier")
        
        # Usages presets
        preset_usage_types = {
            "Paie r√©glementaire": "paie_reglementaire",
            "Reporting Social (CSE)": "reporting_social",
            "Dashboard Op√©rationnel": "dashboard_operationnel",
            "Audit Conformit√©": "audit_conformite",
            "Analytics D√©cisionnel": "analytics_decisional"
        }
        
        # Stocker usages personnalis√©s dans session state
        if 'custom_usages' not in st.session_state:
            st.session_state.custom_usages = {}
        
        # Combiner presets + custom
        all_usage_types = {**preset_usage_types, **st.session_state.custom_usages}
        
        # Bouton ajouter usage personnalis√©
        col_add1, col_add2 = st.columns([3, 1])
        with col_add1:
            st.markdown("**Usages disponibles**")
        with col_add2:
            add_usage = st.button("‚ûï Ajouter", use_container_width=True, help="Cr√©er un usage m√©tier personnalis√©")
        
        # Formulaire ajout usage
        if add_usage or 'show_add_form' in st.session_state and st.session_state.show_add_form:
            st.session_state.show_add_form = True
            
            with st.expander("‚ûï Nouvel usage personnalis√©", expanded=True):
                with st.form("add_custom_usage_form"):
                    st.markdown("### Cr√©er un usage m√©tier")
                    
                    usage_name = st.text_input(
                        "Nom de l'usage",
                        placeholder="Ex: Reporting URSSAF, Pilotage RH, Audit CNIL...",
                        help="Nom descriptif de votre usage m√©tier"
                    )
                    
                    st.markdown("#### Pond√©rations AHP")
                    st.caption("D√©finir l'importance de chaque dimension (total = 100%)")
                    
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        w_db_new = st.slider("DB", 0, 100, 25, 5, key="new_db", help="Database: Contraintes structurelles")
                    with col2:
                        w_dp_new = st.slider("DP", 0, 100, 25, 5, key="new_dp", help="Data Processing: Transformations ETL")
                    with col3:
                        w_br_new = st.slider("BR", 0, 100, 25, 5, key="new_br", help="Business Rules: R√®gles m√©tier")
                    with col4:
                        w_up_new = st.slider("UP", 0, 100, 25, 5, key="new_up", help="Usage-fit: Ad√©quation contextuelle")
                    
                    total_new = w_db_new + w_dp_new + w_br_new + w_up_new
                    
                    if total_new == 100:
                        st.success(f"‚úÖ Somme = {total_new}%")
                    else:
                        st.error(f"‚ö†Ô∏è Somme = {total_new}% (doit √™tre 100%)")
                    
                    rationale = st.text_area(
                        "Justification (optionnel)",
                        placeholder="Ex: Priorit√© conformit√© l√©gale, peu de flexibilit√© contextuelle...",
                        help="Expliquer pourquoi ces pond√©rations"
                    )
                    
                    col_btn1, col_btn2 = st.columns(2)
                    
                    with col_btn1:
                        submitted = st.form_submit_button("‚úÖ Cr√©er usage", type="primary", use_container_width=True)
                    
                    with col_btn2:
                        cancelled = st.form_submit_button("‚ùå Annuler", use_container_width=True)
                    
                    if submitted:
                        if not usage_name:
                            st.error("‚ö†Ô∏è Le nom de l'usage est obligatoire")
                        elif usage_name in all_usage_types:
                            st.error(f"‚ö†Ô∏è Un usage '{usage_name}' existe d√©j√†")
                        elif total_new != 100:
                            st.error("‚ö†Ô∏è La somme des pond√©rations doit √™tre 100%")
                        else:
                            # Ajouter usage personnalis√©
                            st.session_state.custom_usages[usage_name] = {
                                'type': 'custom',
                                'w_DB': w_db_new / 100,
                                'w_DP': w_dp_new / 100,
                                'w_BR': w_br_new / 100,
                                'w_UP': w_up_new / 100,
                                'rationale': rationale if rationale else f"Usage personnalis√© : {usage_name}"
                            }
                            st.session_state.show_add_form = False
                            st.success(f"‚úÖ Usage '{usage_name}' cr√©√© avec succ√®s !")
                            st.rerun()
                    
                    if cancelled:
                        st.session_state.show_add_form = False
                        st.rerun()
        
        # S√©lection usages (presets + custom)
        selected_usages = st.multiselect(
            "S√©lectionner usages",
            options=list(all_usage_types.keys()),
            default=[k for k in list(all_usage_types.keys())[:2] if k in all_usage_types],
            help="Contextes m√©tier pour scoring risque"
        )
        
        # Afficher usages personnalis√©s avec option suppression
        if st.session_state.custom_usages:
            with st.expander("üóëÔ∏è G√©rer usages personnalis√©s", expanded=False):
                for custom_name, custom_data in list(st.session_state.custom_usages.items()):
                    col1, col2 = st.columns([4, 1])
                    with col1:
                        st.write(f"**{custom_name}**")
                        st.caption(f"DB={custom_data['w_DB']:.0%}, DP={custom_data['w_DP']:.0%}, BR={custom_data['w_BR']:.0%}, UP={custom_data['w_UP']:.0%}")
                    with col2:
                        if st.button("üóëÔ∏è", key=f"delete_{custom_name}", help=f"Supprimer {custom_name}"):
                            del st.session_state.custom_usages[custom_name]
                            # Retirer de s√©lection si pr√©sent
                            if custom_name in selected_usages:
                                selected_usages.remove(custom_name)
                            st.rerun()
                    st.markdown("---")
        
        # Mode √©dition pond√©rations
        if selected_usages:
            from backend.engine.ahp_elicitor import AHPElicitor
            elicitor = AHPElicitor()
            
            # Stocker pond√©rations personnalis√©es dans session state
            if 'custom_weights' not in st.session_state:
                st.session_state.custom_weights = {}
            
            st.markdown("---")
            st.markdown("### ‚öôÔ∏è Configuration pond√©rations")
            
            mode = st.radio(
                "Mode configuration",
                ["üìä Utiliser valeurs par d√©faut", "‚úèÔ∏è Ajuster finement les pond√©rations"],
                help="Par d√©faut = valeurs expertes, Ajuster = modifier manuellement"
            )
            
            if mode == "‚úèÔ∏è Ajuster finement les pond√©rations":
                st.info("üí° Ajuste les pond√©rations avec les sliders. La somme doit faire 100%.")
                
                for usage_name in selected_usages:
                    # R√©cup√©rer valeurs par d√©faut
                    if usage_name not in st.session_state.custom_weights:
                        # Si preset, charger preset
                        if usage_name in preset_usage_types:
                            usage_type = preset_usage_types[usage_name]
                            preset = elicitor.get_weights_preset(usage_type)
                            st.session_state.custom_weights[usage_name] = {
                                'w_DB': preset['w_DB'],
                                'w_DP': preset['w_DP'],
                                'w_BR': preset['w_BR'],
                                'w_UP': preset['w_UP']
                            }
                        # Si custom, utiliser valeurs custom
                        elif usage_name in st.session_state.custom_usages:
                            custom_data = st.session_state.custom_usages[usage_name]
                            st.session_state.custom_weights[usage_name] = {
                                'w_DB': custom_data['w_DB'],
                                'w_DP': custom_data['w_DP'],
                                'w_BR': custom_data['w_BR'],
                                'w_UP': custom_data['w_UP']
                            }
                    
                    with st.expander(f"‚öôÔ∏è {usage_name}", expanded=True):
                        # Indicateur si custom
                        if usage_name in st.session_state.custom_usages:
                            st.badge("Custom", icon="‚ú®")
                        
                        st.markdown(f"**{usage_name}**")
                        
                        col1, col2, col3, col4 = st.columns(4)
                        
                        with col1:
                            w_db = st.slider(
                                "DB",
                                min_value=0,
                                max_value=100,
                                value=int(st.session_state.custom_weights[usage_name]['w_DB'] * 100),
                                step=5,
                                key=f"slider_db_{usage_name}",
                                help="Database: Contraintes structurelles"
                            )
                        
                        with col2:
                            w_dp = st.slider(
                                "DP",
                                min_value=0,
                                max_value=100,
                                value=int(st.session_state.custom_weights[usage_name]['w_DP'] * 100),
                                step=5,
                                key=f"slider_dp_{usage_name}",
                                help="Data Processing: Transformations ETL"
                            )
                        
                        with col3:
                            w_br = st.slider(
                                "BR",
                                min_value=0,
                                max_value=100,
                                value=int(st.session_state.custom_weights[usage_name]['w_BR'] * 100),
                                step=5,
                                key=f"slider_br_{usage_name}",
                                help="Business Rules: R√®gles m√©tier"
                            )
                        
                        with col4:
                            w_up = st.slider(
                                "UP",
                                min_value=0,
                                max_value=100,
                                value=int(st.session_state.custom_weights[usage_name]['w_UP'] * 100),
                                step=5,
                                key=f"slider_up_{usage_name}",
                                help="Usage-fit: Ad√©quation contextuelle"
                            )
                        
                        # Calculer somme
                        total = w_db + w_dp + w_br + w_up
                        
                        # Afficher somme avec couleur
                        if total == 100:
                            st.success(f"‚úÖ Somme = {total}% (OK)")
                        else:
                            st.error(f"‚ö†Ô∏è Somme = {total}% (doit √™tre 100%)")
                        
                        # Normaliser automatiquement si demand√©
                        if total != 100:
                            if st.button(f"üîÑ Normaliser automatiquement", key=f"normalize_{usage_name}"):
                                # Normaliser √† 100%
                                if total > 0:
                                    st.session_state.custom_weights[usage_name] = {
                                        'w_DB': round(w_db / total, 2),
                                        'w_DP': round(w_dp / total, 2),
                                        'w_BR': round(w_br / total, 2),
                                        'w_UP': round(w_up / total, 2)
                                    }
                                    st.rerun()
                        else:
                            # Sauvegarder valeurs
                            st.session_state.custom_weights[usage_name] = {
                                'w_DB': w_db / 100,
                                'w_DP': w_dp / 100,
                                'w_BR': w_br / 100,
                                'w_UP': w_up / 100
                            }
                        
                        # Bouton reset
                        if st.button(f"üîÑ R√©initialiser", key=f"reset_{usage_name}"):
                            if usage_name in preset_usage_types:
                                usage_type = preset_usage_types[usage_name]
                                preset = elicitor.get_weights_preset(usage_type)
                                st.session_state.custom_weights[usage_name] = {
                                    'w_DB': preset['w_DB'],
                                    'w_DP': preset['w_DP'],
                                    'w_BR': preset['w_BR'],
                                    'w_UP': preset['w_UP']
                                }
                            elif usage_name in st.session_state.custom_usages:
                                custom_data = st.session_state.custom_usages[usage_name]
                                st.session_state.custom_weights[usage_name] = {
                                    'w_DB': custom_data['w_DB'],
                                    'w_DP': custom_data['w_DP'],
                                    'w_BR': custom_data['w_BR'],
                                    'w_UP': custom_data['w_UP']
                                }
                            st.rerun()
            
            else:
                # Mode par d√©faut : afficher juste les valeurs
                with st.expander("üìä Voir pond√©rations (valeurs par d√©faut)", expanded=False):
                    for usage_name in selected_usages:
                        # Charger valeurs par d√©faut
                        if usage_name in preset_usage_types:
                            usage_type = preset_usage_types[usage_name]
                            weights = elicitor.get_weights_preset(usage_type)
                        elif usage_name in st.session_state.custom_usages:
                            weights = st.session_state.custom_usages[usage_name]
                        else:
                            continue
                        
                        st.markdown(f"**{usage_name}**")
                        if usage_name in st.session_state.custom_usages:
                            st.caption("‚ú® Usage personnalis√©")
                        
                        col1, col2, col3, col4 = st.columns(4)
                        col1.metric("DB", f"{weights['w_DB']:.0%}")
                        col2.metric("DP", f"{weights['w_DP']:.0%}")
                        col3.metric("BR", f"{weights['w_BR']:.0%}")
                        col4.metric("UP", f"{weights['w_UP']:.0%}")
                        st.caption(weights.get('rationale', ''))
                        st.markdown("---")
        
        # Bouton ANALYSER
        st.markdown("---")
        if st.button("üöÄ LANCER ANALYSE", type="primary", use_container_width=True):
            if not selected_columns:
                st.error("‚ö†Ô∏è S√©lectionne au moins une colonne !")
            elif not selected_usages:
                st.error("‚ö†Ô∏è S√©lectionne au moins un usage !")
            else:
                # LANCER ANALYSE
                with st.spinner("‚è≥ Analyse en cours..."):
                    try:
                        # Pr√©parer config usages
                        usages = []
                        for name in selected_usages:
                            # Utiliser valeurs custom si c'est un usage custom
                            if name in st.session_state.custom_usages:
                                usages.append({
                                    "nom": name,
                                    "type": "custom",
                                    "criticite": "MEDIUM"
                                })
                            else:
                                usages.append({
                                    "nom": name.split('(')[0].strip(),
                                    "type": all_usage_types[name],
                                    "criticite": "HIGH" if "Paie" in name else "MEDIUM"
                                })
                        
                        # 1. Stats exploratoires
                        stats = analyze_dataset(st.session_state.df, selected_columns)
                        
                        # 2. Vecteurs 4D
                        vecteurs_4d = compute_all_beta_vectors(st.session_state.df, selected_columns, stats)
                        
                        # 3. Pond√©rations
                        # Si mode personnalis√© ET poids valides, utiliser custom, sinon valeurs par d√©faut
                        if mode == "‚úèÔ∏è Ajuster finement les pond√©rations" and st.session_state.custom_weights:
                            # V√©rifier que tous les usages ont des poids valides
                            weights = {}
                            all_valid = True
                            
                            for usage_name in selected_usages:
                                if usage_name in st.session_state.custom_weights:
                                    custom = st.session_state.custom_weights[usage_name]
                                    total = sum(custom.values())
                                    
                                    if abs(total - 1.0) < 0.01:  # Tol√©rance 1%
                                        # Utiliser nom nettoy√©
                                        clean_name = usage_name.split('(')[0].strip() if '(' in usage_name else usage_name
                                        weights[clean_name] = {
                                            'w_DB': custom['w_DB'],
                                            'w_DP': custom['w_DP'],
                                            'w_BR': custom['w_BR'],
                                            'w_UP': custom['w_UP'],
                                            'rationale': 'Pond√©rations ajust√©es manuellement'
                                        }
                                    else:
                                        all_valid = False
                                        st.error(f"‚ö†Ô∏è {usage_name}: somme = {total:.0%} (doit √™tre 100%)")
                            
                            if not all_valid:
                                st.stop()
                        else:
                            # Utiliser valeurs par d√©faut
                            weights = {}
                            for usage in usages:
                                usage_name = usage['nom']
                                
                                # Si usage custom, utiliser ses valeurs
                                if usage_name in st.session_state.custom_usages:
                                    custom_data = st.session_state.custom_usages[usage_name]
                                    weights[usage_name] = {
                                        'w_DB': custom_data['w_DB'],
                                        'w_DP': custom_data['w_DP'],
                                        'w_BR': custom_data['w_BR'],
                                        'w_UP': custom_data['w_UP'],
                                        'rationale': custom_data.get('rationale', 'Usage personnalis√©')
                                    }
                                # Sinon utiliser preset
                                else:
                                    preset_weights = elicit_weights_auto([usage], vecteurs_4d)
                                    weights.update(preset_weights)
                        
                        # 4. Scores risque
                        scores = compute_risk_scores(vecteurs_4d, weights, usages)
                        
                        # 5. Top priorit√©s
                        top_priorities = get_top_priorities(scores, top_n=5)
                        
                        # 6. Lineage (premier attribut/usage)
                        lineage = None
                        if len(selected_columns) > 0 and len(usages) > 0:
                            first_attr = selected_columns[0]
                            first_usage_name = usages[0]['nom']
                            if first_attr in vecteurs_4d and first_usage_name in weights:
                                lineage = simulate_lineage(vecteurs_4d[first_attr], weights[first_usage_name])
                        
                        # 7. Comparaison DAMA
                        comparaison = compare_dama_vs_probabiliste(
                            st.session_state.df,
                            selected_columns,
                            scores,
                            vecteurs_4d
                        )
                        
                        # Stocker r√©sultats
                        st.session_state.results = {
                            'stats': stats,
                            'vecteurs_4d': vecteurs_4d,
                            'weights': weights,
                            'scores': scores,
                            'top_priorities': top_priorities,
                            'lineage': lineage,
                            'comparaison': comparaison
                        }
                        
                        st.session_state.analysis_done = True
                        st.success("‚úÖ Analyse termin√©e !")
                        st.balloons()
                        
                    except Exception as e:
                        st.error(f"‚ùå Erreur analyse : {str(e)}")
                        st.exception(e)


# ============================================================================
# MAIN - AFFICHAGE R√âSULTATS
# ============================================================================

if st.session_state.analysis_done and st.session_state.results:
    results = st.session_state.results
    
    # ONGLETS - SCAN EN PREMIER !
    tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8, tab9 = st.tabs([
        "üîç Scan Anomalies",  # ‚Üê NOUVEAU 1ER !
        "üìä Dashboard",
        "üéØ Vecteurs 4D",
        "‚ö†Ô∏è Priorit√©s",
        "üîÑ Lineage",
        "üìà Comparaison DAMA",
        "üí¨ √âlicitation IA",
        "üîî Surveillance",
        "üìã Restitution Adaptative"
    ])
    
    # ========================================================================
    # TAB 1 : SCAN ANOMALIES (NOUVEAU PREMIER ONGLET)
    # ========================================================================
    with tab1:
        render_anomaly_detection_tab()
    
    # ========================================================================
    # TAB 2 : DASHBOARD (ancien tab1)
    # ========================================================================
    with tab3:
        st.header("üìä Dashboard Qualit√©")
        
        # Boutons Export en haut
        col_export1, col_export2, col_export3 = st.columns([2, 2, 4])
        
        with col_export1:
            if st.button("üì• Export Excel", type="primary", use_container_width=True):
                try:
                    output_file = export_to_excel(results)
                    
                    # Lire fichier pour download
                    with open(output_file, 'rb') as f:
                        st.download_button(
                            label="üíæ T√©l√©charger Excel",
                            data=f,
                            file_name=output_file,
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True
                        )
                    st.success(f"‚úÖ Excel cr√©√© : {output_file}")
                except Exception as e:
                    st.error(f"‚ùå Erreur export Excel : {str(e)}")
        
        with col_export2:
            if st.button("üì• Export CSV", use_container_width=True):
                try:
                    files = export_to_csv(results)
                    st.success(f"‚úÖ {len(files)} fichiers CSV cr√©√©s")
                    for file in files:
                        st.text(f"‚Ä¢ {file}")
                except Exception as e:
                    st.error(f"‚ùå Erreur export CSV : {str(e)}")
        
        st.markdown("---")
        
        # M√©triques cl√©s
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "Attributs analys√©s",
                len(results['vecteurs_4d']),
                help="Nombre colonnes analys√©es"
            )
        
        with col2:
            st.metric(
                "Usages m√©tier",
                len(results['weights']),
                help="Contextes m√©tier configur√©s"
            )
        
        with col3:
            top_score = max(results['scores'].values()) if results['scores'] else 0
            st.metric(
                "Risque max",
                f"{top_score:.1%}",
                delta=f"{'CRITIQUE' if top_score > 0.4 else 'OK'}",
                delta_color="inverse"
            )
        
        with col4:
            critical_count = len([s for s in results['scores'].values() if s > 0.4])
            st.metric(
                "Alertes critiques",
                critical_count,
                help="Scores > 40%"
            )
        
        st.markdown("---")
        
        # Heatmap scores
        st.subheader("üî• Matrice Scores Risque")
        fig_heatmap = create_heatmap(results['scores'])
        st.plotly_chart(fig_heatmap, use_container_width=True, key="heatmap_dashboard")
        
        # Bouton commentaire IA
        if st.button("üí¨ Expliquer avec Claude", key="explain_heatmap", help="G√©n√©rer un commentaire IA sur cette matrice"):
            if 'anthropic_api_key' in st.session_state and st.session_state.anthropic_api_key:
                with st.spinner("Claude analyse la matrice..."):
                    # Pr√©parer donn√©es pour IA
                    top_scores = sorted(results['scores'].items(), key=lambda x: x[1], reverse=True)[:5]
                    data_for_ai = {
                        "total_combinations": len(results['scores']),
                        "top_5_risks": [{"combo": k, "score": f"{v:.1%}"} for k, v in top_scores],
                        "critical_count": len([s for s in results['scores'].values() if s > 0.4]),
                        "attributs_analyzed": len(results['vecteurs_4d']),
                        "usages_configured": len(results['weights'])
                    }
                    
                    comment = generate_ai_comment("heatmap", data_for_ai, st.session_state.anthropic_api_key)
                    
                    st.info(f"üí¨ **Commentaire Claude** :\n\n{comment}")
            else:
                st.warning("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar pour utiliser les commentaires IA")
    
    # ========================================================================
    # TAB 2 : VECTEURS 4D
    # ========================================================================
    with tab3:
        st.header("üéØ Vecteurs Qualit√© 4D")
        
        for attr, vector in results['vecteurs_4d'].items():
            st.subheader(f"üìå {attr}")
            
            # Graphique
            fig = create_vector_chart(vector)
            st.plotly_chart(fig, use_container_width=True, key=f"vector_chart_{attr}")
            
            # Bouton commentaire IA
            if st.button("üí¨ Expliquer avec Claude", key=f"explain_vector_{attr}", help="Analyser ce vecteur 4D"):
                if 'anthropic_api_key' in st.session_state and st.session_state.anthropic_api_key:
                    with st.spinner("Claude analyse le vecteur..."):
                        data_for_ai = {
                            "attribut": attr,
                            "P_DB": vector.get('P_DB', 0),
                            "P_DP": vector.get('P_DP', 0),
                            "P_BR": vector.get('P_BR', 0),
                            "P_UP": vector.get('P_UP', 0),
                            "alpha_DB": vector.get('alpha_DB', 0),
                            "beta_DB": vector.get('beta_DB', 0),
                            "confidence": "HIGH" if vector.get('beta_DB', 0) > 50 else "MEDIUM"
                        }
                        
                        comment = generate_ai_comment("vector", data_for_ai, st.session_state.anthropic_api_key)
                        
                        st.info(f"üí¨ **Commentaire Claude** :\n\n{comment}")
                else:
                    st.warning("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar")
            
            # D√©tails Beta
            with st.expander("üî¨ Param√®tres Beta d√©taill√©s"):
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.markdown("**[DB] Database**")
                    st.write(f"Beta({vector['alpha_DB']:.1f}, {vector['beta_DB']:.1f})")
                    st.write(f"P = {vector['P_DB']:.3f}")
                    st.write(f"œÉ = {vector.get('std_DB', 0):.3f}")
                
                with col2:
                    st.markdown("**[DP] Data Processing**")
                    st.write(f"Beta({vector['alpha_DP']:.1f}, {vector['beta_DP']:.1f})")
                    st.write(f"P = {vector['P_DP']:.3f}")
                    st.write(f"œÉ = {vector.get('std_DP', 0):.3f}")
                
                with col3:
                    st.markdown("**[BR] Business Rules**")
                    st.write(f"Beta({vector['alpha_BR']:.1f}, {vector['beta_BR']:.1f})")
                    st.write(f"P = {vector['P_BR']:.3f}")
                    st.write(f"œÉ = {vector.get('std_BR', 0):.3f}")
                
                with col4:
                    st.markdown("**[UP] Usage-fit**")
                    st.write(f"Beta({vector['alpha_UP']:.1f}, {vector['beta_UP']:.1f})")
                    st.write(f"P = {vector['P_UP']:.3f}")
                    st.write(f"œÉ = {vector.get('std_UP', 0):.3f}")
            
            st.markdown("---")
    
    # ========================================================================
    # TAB 3 : PRIORIT√âS
    # ========================================================================
    with tab4:
        st.header("‚ö†Ô∏è Top Priorit√©s Actions")
        
        for i, priority in enumerate(results['top_priorities'][:3], 1):  # Top 3 seulement avec bouton IA
            # Couleur box selon s√©v√©rit√©
            if priority['severite'] == 'CRITIQUE':
                box_class = "danger-box"
                emoji = "üö®"
            elif priority['severite'] == '√âLEV√â':
                box_class = "warning-box"
                emoji = "‚ö†Ô∏è"
            else:
                box_class = "success-box"
                emoji = "‚úÖ"
            
            st.markdown(f"""
            <div class="{box_class}">
                <h3>{emoji} #{i} - {priority['attribut']} √ó {priority['usage']}</h3>
                <p><strong>Score risque :</strong> {priority['score']:.1%} ({priority['severite']})</p>
                <p><strong>Impact :</strong> {priority['records_affected']} enregistrements, {priority['impact_mensuel']:,}‚Ç¨/mois</p>
            </div>
            """, unsafe_allow_html=True)
            
            # Actions recommand√©es
            with st.expander("üìã Actions recommand√©es"):
                for action in priority.get('actions', []):
                    st.write(f"‚Ä¢ {action}")
            
            # Bouton commentaire IA
            if st.button("üí¨ Analyser avec Claude", key=f"explain_priority_{i}", help="Recommandation IA pour cette priorit√©"):
                if 'anthropic_api_key' in st.session_state and st.session_state.anthropic_api_key:
                    with st.spinner("Claude analyse la priorit√©..."):
                        comment = generate_ai_comment("priority", priority, st.session_state.anthropic_api_key)
                        st.info(f"üí¨ **Recommandation Claude** :\n\n{comment}")
                else:
                    st.warning("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar")
            
            st.markdown("<br>", unsafe_allow_html=True)
        
        # Afficher les priorit√©s restantes sans bouton IA
        if len(results['top_priorities']) > 3:
            st.markdown("---")
            st.subheader("Autres priorit√©s")
            
            for i, priority in enumerate(results['top_priorities'][3:], 4):
                # Couleur box selon s√©v√©rit√© (version simplifi√©e sans IA)
                if priority['severite'] == 'CRITIQUE':
                    box_class = "danger-box"
                    emoji = "üö®"
                elif priority['severite'] == '√âLEV√â':
                    box_class = "warning-box"
                    emoji = "‚ö†Ô∏è"
                else:
                    box_class = "success-box"
                    emoji = "‚úÖ"
                
                st.markdown(f"""
                <div class="{box_class}">
                    <h4>{emoji} #{i} - {priority['attribut']} √ó {priority['usage']}</h4>
                    <p><strong>Score :</strong> {priority['score']:.1%} ({priority['severite']})</p>
                </div>
                """, unsafe_allow_html=True)
                st.markdown("<br>", unsafe_allow_html=True)
    
    # ========================================================================
    # TAB 4 : LINEAGE AVEC SC√âNARIOS R√âALISTES
    # ========================================================================
    with tab5:
        st.header("üîÑ Propagation Risque Lineage")
        
        st.markdown("""
        ### üéØ Objectif : Simuler la d√©gradation qualit√© le long des pipelines
        
        S√©lectionne un **sc√©nario r√©aliste** pour voir comment le risque se propage et s'aggrave √† travers les transformations.
        """)
        
        # S√©lection sc√©nario
        st.markdown("### üìã Choisis un sc√©nario")
        
        scenario = st.radio(
            "Incidents bas√©s sur cas r√©els fr√©quents en entreprise",
            [
                "‚úÖ Baseline (pas d'incident - qualit√© stable)",
                "üîß Incident ETL Parser (bug d√©ploiement - tr√®s fr√©quent)",
                "üìä Enrichissement M√©tier D√©faillant (r√©f√©rentiel obsol√®te)",
                "üìà Reporting D√©faillant (donn√©es incompl√®tes - deadline approche)",
                "‚ö†Ô∏è D√©gradation Cumulative (micro-erreurs qui s'accumulent)",
                "‚úèÔ∏è Personnalis√© (cr√©er ton propre sc√©nario avec Claude)"
            ],
            help="Sc√©narios benchmark√©s sur incidents r√©els industriels + option personnalis√©e"
        )
        
        # D√©finir donn√©es selon sc√©nario
        if "Baseline" in scenario:
            # Sc√©nario stable (actuel)
            scenario_data = {
                "nom": "Baseline - Qualit√© Stable",
                "emoji": "‚úÖ",
                "contexte": """**Situation normale** : Aucun incident d√©tect√©.
                
Les pipelines ETL fonctionnent correctement, les transformations pr√©servent la qualit√©.
C'est la situation de r√©f√©rence (baseline) pour comparer avec les incidents.""",
                "risk_source": 0.332,
                "risk_final": 0.337,
                "delta": 0.005,
                "timeline": [
                    {"etape": "Source SIRH", "P_DB": 0.367, "P_DP": 1.000, "P_BR": 0.050, "P_UP": 0.100},
                    {"etape": "ETL Extraction", "P_DB": 0.367, "P_DP": 1.000, "P_BR": 0.050, "P_UP": 0.100},
                    {"etape": "Enrichissement", "P_DB": 0.367, "P_DP": 1.000, "P_BR": 0.069, "P_UP": 0.100},
                    {"etape": "Agr√©gation Paie", "P_DB": 0.367, "P_DP": 1.000, "P_BR": 0.069, "P_UP": 0.100},
                    {"etape": "Calcul Final", "P_DB": 0.367, "P_DP": 1.000, "P_BR": 0.069, "P_UP": 0.100}
                ],
                "interpretation": "‚û°Ô∏è STABLE - Qualit√© pr√©serv√©e tout au long du pipeline",
                "cause_racine": "Aucun incident",
                "impact_records": 0,
                "impact_financier": 0,
                "temps_detection": "N/A",
                "actions": []
            }
        
        elif "Incident ETL" in scenario:
            # Sc√©nario ETL cass√© (li√© √† tab Surveillance)
            scenario_data = {
                "nom": "Incident ETL Parser",
                "emoji": "üîß",
                "contexte": """**D√©ploiement ETL v2.8.2** le 12/01/2025 √† 23:15 (commit abc123def).

**Bug introduit** : Parser Anciennet√© modifi√©, virgules d√©cimales non g√©r√©es.
```python
# AVANT (v2.8.1) ‚úÖ
anciennete = float(value.replace(',', '.'))

# APR√àS (v2.8.2) ‚ùå  
anciennete = float(value)  # Crash sur "7,21"
```

**R√©sultat** : 141/687 enregistrements cass√©s lors de l'extraction ETL.""",
                "risk_source": 0.463,
                "risk_final": 0.623,
                "delta": 0.160,
                "timeline": [
                    {"etape": "Source SIRH", "P_DB": 0.990, "P_DP": 0.020, "P_BR": 0.200, "P_UP": 0.100},
                    {"etape": "ETL Extraction", "P_DB": 0.990, "P_DP": 0.285, "P_BR": 0.200, "P_UP": 0.100},
                    {"etape": "Enrichissement", "P_DB": 0.990, "P_DP": 0.285, "P_BR": 0.220, "P_UP": 0.100},
                    {"etape": "Agr√©gation Paie", "P_DB": 0.990, "P_DP": 0.310, "P_BR": 0.220, "P_UP": 0.100},
                    {"etape": "Calcul Final", "P_DB": 0.990, "P_DP": 0.320, "P_BR": 0.220, "P_UP": 0.100}
                ],
                "interpretation": "üö® CRITIQUE - D√©gradation DP massive (+26.5 points) d√®s extraction ETL",
                "cause_racine": "Commit abc123def - Parser Anciennet√© cass√© (virgule non g√©r√©e)",
                "impact_records": 141,
                "impact_financier": 45080,
                "temps_detection": "9 heures (vs 3 semaines avec DAMA)",
                "actions": [
                    "üîÑ Rollback imm√©diat vers ETL v2.8.1",
                    "üîß Fix parser : value.replace(',', '.') avant float()",
                    "üìä Batch correction des 141 enregistrements",
                    "üîî Activer monitoring continu DP sur Anciennet√©",
                    "üß™ Ajouter test unitaire parser avec virgules"
                ]
            }
        
        elif "Enrichissement" in scenario:
            # Sc√©nario r√©f√©rentiel obsol√®te
            scenario_data = {
                "nom": "Enrichissement M√©tier D√©faillant",
                "emoji": "üìä",
                "contexte": """**Fusion d√©partements RH** : Nord + Sud ‚Üí Grand Nord (01/12/2024).

**Probl√®me** : Table r√©f√©rentiel `ref_departements` non mise √† jour.

**Impact progressif** :
- D√©cembre : 15 employ√©s mal rattach√©s (2%)
- Janvier : 45 employ√©s mal rattach√©s (7%)
- F√©vrier : 89 employ√©s mal rattach√©s (13%)

**D√©couverte** : Audit interne CSE d√©tecte incoh√©rences reporting social.""",
                "risk_source": 0.332,
                "risk_final": 0.485,
                "delta": 0.153,
                "timeline": [
                    {"etape": "Source SIRH", "P_DB": 0.367, "P_DP": 0.020, "P_BR": 0.050, "P_UP": 0.100},
                    {"etape": "ETL Extraction", "P_DB": 0.367, "P_DP": 0.020, "P_BR": 0.050, "P_UP": 0.100},
                    {"etape": "Enrichissement", "P_DB": 0.367, "P_DP": 0.020, "P_BR": 0.280, "P_UP": 0.100},
                    {"etape": "Agr√©gation Paie", "P_DB": 0.367, "P_DP": 0.025, "P_BR": 0.280, "P_UP": 0.100},
                    {"etape": "Calcul Final", "P_DB": 0.367, "P_DP": 0.030, "P_BR": 0.280, "P_UP": 0.100}
                ],
                "interpretation": "‚ö†Ô∏è √âLEV√â - D√©gradation BR progressive (+23 points) lors enrichissement m√©tier",
                "cause_racine": "Table ref_departements obsol√®te (fusion Nord+Sud non int√©gr√©e)",
                "impact_records": 89,
                "impact_financier": 12400,
                "temps_detection": "2 jours avec monitoring BR (vs 3 semaines manuellement)",
                "actions": [
                    "üìã Mise √† jour table ref_departements (fusion Nord+Sud)",
                    "üîÑ Recalcul batch des rattachements depuis 01/12",
                    "üìä Correction des 89 employ√©s mal rattach√©s",
                    "üîî Alerte automatique sur changements r√©f√©rentiels",
                    "üìÖ Workflow validation changements orga (DRH‚ÜíIT)"
                ]
            }
        
        elif "Reporting" in scenario:
            # NOUVEAU : Sc√©nario reporting d√©faillant
            scenario_data = {
                "nom": "Reporting CSE D√©faillant",
                "emoji": "üìà",
                "contexte": """**Rapport mensuel CSE** (Comit√© Social √âconomique) - Deadline : 05/02/2025.

**D√©couverte J-2** (03/02) lors revue DRH : 252/687 dates promotions manquantes (37%).

**Cause racine** : Workflow validation promotions non respect√©.
- Managers ne saisissent pas les promotions dans SIRH
- Processus manuel, pas d'alerte automatique
- D√©gradation progressive sur 6 mois (jamais d√©tect√©e)

**Impact** : Reporting incomplet, cr√©dibilit√© DRH aupr√®s CSE compromise.""",
                "risk_source": 0.364,
                "risk_final": 0.520,
                "delta": 0.156,
                "timeline": [
                    {"etape": "Source SIRH", "P_DB": 0.364, "P_DP": 0.143, "P_BR": 0.252, "P_UP": 0.250},
                    {"etape": "ETL Extraction", "P_DB": 0.364, "P_DP": 0.150, "P_BR": 0.252, "P_UP": 0.250},
                    {"etape": "Enrichissement", "P_DB": 0.364, "P_DP": 0.160, "P_BR": 0.280, "P_UP": 0.250},
                    {"etape": "Agr√©gation CSE", "P_DB": 0.364, "P_DP": 0.170, "P_BR": 0.300, "P_UP": 0.380},
                    {"etape": "Export Reporting", "P_DB": 0.364, "P_DP": 0.180, "P_BR": 0.310, "P_UP": 0.420}
                ],
                "interpretation": "‚ö†Ô∏è √âLEV√â - D√©gradation multi-dimensions (UP +17 pts, BR +6 pts) pour reporting CSE",
                "cause_racine": "Workflow promotions non respect√© - 252 dates manquantes (37%)",
                "impact_records": 252,
                "impact_financier": 7500,
                "temps_detection": "J-2 avant deadline (d√©tection manuelle tardive)",
                "actions": [
                    "üö® URGENT : Relance managers pour saisie promotions (252 cas)",
                    "üìß Email automatique rappel mensuel + escalade N+1",
                    "üîî Dashboard temps r√©el compl√©tude donn√©es promotions",
                    "üìä Alerte seuil critique (<90%) J-15 avant deadline",
                    "üîÑ Processus backup : extraction fichiers Direction (plan B)",
                    "üí∞ Co√ªt opportunit√© : cr√©dibilit√© DRH + risque contentieux CSE"
                ]
            }
        
        elif "Personnalis√©" in scenario:
            # SC√âNARIO PERSONNALISABLE
            st.markdown("### ‚úèÔ∏è Cr√©er ton Sc√©nario Personnalis√©")
            
            # Choix mode
            mode_custom = st.radio(
                "Comment veux-tu cr√©er ton sc√©nario ?",
                [
                    "ü§ñ Mode IA : D√©cris l'incident, Claude g√©n√®re les chiffres",
                    "üìä Mode Manuel : Entre les chiffres toi-m√™me"
                ],
                help="Mode IA = plus rapide, Mode Manuel = contr√¥le total"
            )
            
            if "Mode IA" in mode_custom:
                # MODE IA : Description textuelle ‚Üí Claude g√©n√®re les chiffres
                st.markdown("#### ü§ñ D√©cris ton incident √† Claude")
                
                with st.form("custom_scenario_ia_form"):
                    nom_incident = st.text_input(
                        "Nom de l'incident",
                        placeholder="Ex: Migration base de donn√©es rat√©e",
                        help="Titre court pour identifier l'incident"
                    )
                    
                    description_incident = st.text_area(
                        "D√©cris l'incident en d√©tail",
                        placeholder="""Ex: Notre migration de SQL Server vers PostgreSQL a caus√© des pertes de donn√©es.
Les colonnes avec NULL ont √©t√© mal converties, cr√©ant des erreurs dans les calculs.
Les transformations ETL ont √©chou√© sur 30% des records.
Le reporting mensuel √©tait incomplet pendant 2 semaines.""",
                        height=150,
                        help="Plus tu donnes de d√©tails, mieux Claude pourra g√©n√©rer les chiffres r√©alistes"
                    )
                    
                    pipeline_etapes = st.text_input(
                        "√âtapes du pipeline (s√©par√©es par virgule)",
                        value="Source DB, Migration, ETL, Transformation, Reporting",
                        help="Ex: Source, Extraction, Enrichissement, Agr√©gation, Export"
                    )
                    
                    impact_estime = st.selectbox(
                        "Impact estim√©",
                        ["Faible (5-15%)", "Moyen (15-30%)", "√âlev√© (30-50%)", "Critique (>50%)"],
                        help="Gravit√© globale de l'incident"
                    )
                    
                    submitted_ia = st.form_submit_button("üöÄ G√©n√©rer avec Claude", type="primary")
                
                if submitted_ia:
                    if not nom_incident or not description_incident:
                        st.error("‚ö†Ô∏è Le nom et la description sont obligatoires")
                    elif 'anthropic_api_key' not in st.session_state or not st.session_state.anthropic_api_key:
                        st.error("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar")
                    else:
                        with st.spinner("ü§ñ Claude g√©n√®re ton sc√©nario personnalis√©..."):
                            try:
                                client = anthropic.Anthropic(api_key=st.session_state.anthropic_api_key)
                                
                                # Prompt pour g√©n√©ration sc√©nario
                                prompt_generation = f"""Tu es un expert Data Quality. G√©n√®re un sc√©nario de lineage r√©aliste bas√© sur cette description d'incident.

**Incident** : {nom_incident}

**Description** : {description_incident}

**√âtapes pipeline** : {pipeline_etapes}

**Impact estim√©** : {impact_estime}

**G√©n√®re un JSON avec cette structure exacte** :

```json
{{
  "nom": "Nom incident",
  "contexte": "Description d√©taill√©e 3-4 phrases",
  "risk_source": 0.XX (probabilit√© 0-1),
  "risk_final": 0.XX (probabilit√© 0-1),
  "delta": 0.XX (diff√©rence),
  "timeline": [
    {{"etape": "Nom √©tape 1", "P_DB": 0.XX, "P_DP": 0.XX, "P_BR": 0.XX, "P_UP": 0.XX}},
    {{"etape": "Nom √©tape 2", "P_DB": 0.XX, "P_DP": 0.XX, "P_BR": 0.XX, "P_UP": 0.XX}},
    ...
  ],
  "interpretation": "Texte explication",
  "cause_racine": "Cause technique pr√©cise",
  "impact_records": nombre (entier),
  "impact_financier": montant euros/mois (entier),
  "temps_detection": "Dur√©e",
  "actions": ["Action 1", "Action 2", ...]
}}
```

**R√àGLES CRITIQUES** :
1. Les probabilit√©s P_DB, P_DP, P_BR, P_UP sont entre 0.0 et 1.0
2. Chaque √©tape doit montrer une √©volution r√©aliste
3. Les dimensions affect√©es d√©pendent du type d'incident :
   - Probl√®me structurel ‚Üí P_DB √©lev√©
   - Erreur transformation ‚Üí P_DP √©lev√©
   - R√®gle m√©tier viol√©e ‚Üí P_BR √©lev√©
   - Inad√©quation usage ‚Üí P_UP √©lev√©
4. Le risque doit progresser ou rester stable (jamais diminuer sans correction)
5. Impact financier r√©aliste : 5k‚Ç¨-150k‚Ç¨/mois selon gravit√©
6. Temps d√©tection : comparer avec approche DAMA (heures vs semaines)

**R√©ponds UNIQUEMENT avec le JSON, rien d'autre.**"""
                                
                                response = client.messages.create(
                                    model="claude-sonnet-4-20250514",
                                    max_tokens=2000,
                                    system="Tu es un expert Data Quality qui g√©n√®re des sc√©narios de lineage r√©alistes au format JSON. Tu r√©ponds UNIQUEMENT avec le JSON valide, sans markdown ni texte additionnel.",
                                    messages=[{"role": "user", "content": prompt_generation}]
                                )
                                
                                # Extraire JSON
                                response_text = response.content[0].text
                                
                                # Nettoyer markdown si pr√©sent
                                if "```json" in response_text:
                                    json_start = response_text.find("```json") + 7
                                    json_end = response_text.find("```", json_start)
                                    response_text = response_text[json_start:json_end].strip()
                                elif "```" in response_text:
                                    json_start = response_text.find("```") + 3
                                    json_end = response_text.find("```", json_start)
                                    response_text = response_text[json_start:json_end].strip()
                                
                                # Parser JSON
                                scenario_data = json.loads(response_text)
                                scenario_data['emoji'] = "‚úèÔ∏è"
                                
                                # Stocker dans session state
                                st.session_state.custom_scenario_data = scenario_data
                                
                                st.success("‚úÖ Sc√©nario g√©n√©r√© avec succ√®s par Claude !")
                                st.balloons()
                                
                            except json.JSONDecodeError as e:
                                st.error(f"‚ùå Erreur parsing JSON : {str(e)}")
                                st.code(response_text)
                            except Exception as e:
                                st.error(f"‚ùå Erreur g√©n√©ration : {str(e)}")
                
                # Afficher sc√©nario g√©n√©r√© si disponible
                if 'custom_scenario_data' in st.session_state:
                    scenario_data = st.session_state.custom_scenario_data
                    st.success("‚úÖ Sc√©nario personnalis√© g√©n√©r√© par Claude")
                    
                    # Afficher r√©sum√© du sc√©nario
                    with st.expander("üìã R√©sum√© du sc√©nario g√©n√©r√©", expanded=True):
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("Risque initial", f"{scenario_data.get('risk_source', 0)*100:.1f}%")
                            st.metric("Impact records", f"{scenario_data.get('impact_records', 0):,}")
                        with col2:
                            st.metric("Risque final", f"{scenario_data.get('risk_final', 0)*100:.1f}%", 
                                     delta=f"+{scenario_data.get('delta', 0)*100:.1f}%")
                            st.metric("Impact financier", f"{scenario_data.get('impact_financier', 0):,}‚Ç¨/mois")
                        
                        st.markdown("**üìä Probabilit√©s par √©tape du pipeline :**")
                        
                        # Tableau des probabilit√©s (sans heatmap)
                        timeline_df = pd.DataFrame(scenario_data.get('timeline', []))
                        if not timeline_df.empty:
                            # Formatter en pourcentages
                            display_df = timeline_df.copy()
                            for col in ['P_DB', 'P_DP', 'P_BR', 'P_UP']:
                                if col in display_df.columns:
                                    display_df[col] = display_df[col].apply(lambda x: f"{x*100:.1f}%")
                            
                            st.dataframe(display_df, use_container_width=True)
                        
                        st.markdown(f"**üîç Cause racine :** {scenario_data.get('cause_racine', 'N/A')}")
                        st.markdown(f"**‚è±Ô∏è Temps d√©tection :** {scenario_data.get('temps_detection', 'N/A')}")
                        
                        if scenario_data.get('actions'):
                            st.markdown("**‚úÖ Actions recommand√©es :**")
                            for action in scenario_data['actions']:
                                st.markdown(f"- {action}")
                    
                else:
                    st.info("üí° Remplis le formulaire ci-dessus et clique 'G√©n√©rer' pour cr√©er ton sc√©nario")
                    st.stop()
            
            else:
                # MODE MANUEL : Saisie des chiffres
                st.markdown("#### üìä Entre les chiffres manuellement")
                
                with st.form("custom_scenario_manual_form"):
                    nom_incident_manual = st.text_input(
                        "Nom de l'incident",
                        value="Mon Incident Personnalis√©"
                    )
                    
                    contexte_manual = st.text_area(
                        "Contexte / Description",
                        value="Description de l'incident...",
                        height=100
                    )
                    
                    st.markdown("**Nombre d'√©tapes du pipeline**")
                    nb_etapes = st.slider("Nombre d'√©tapes", 3, 8, 5)
                    
                    timeline_manual = []
                    
                    for i in range(nb_etapes):
                        st.markdown(f"##### üîπ √âtape {i+1}")
                        
                        col_nom, col_db, col_dp, col_br, col_up = st.columns([2,1,1,1,1])
                        
                        with col_nom:
                            nom_etape = st.text_input(
                                f"Nom √©tape {i+1}",
                                value=f"√âtape {i+1}",
                                key=f"nom_etape_{i}"
                            )
                        
                        with col_db:
                            p_db = st.number_input(
                                "DB (%)",
                                0, 100,
                                value=min(10 + i*5, 90),
                                key=f"p_db_{i}"
                            )
                        
                        with col_dp:
                            p_dp = st.number_input(
                                "DP (%)",
                                0, 100,
                                value=min(5 + i*8, 85),
                                key=f"p_dp_{i}"
                            )
                        
                        with col_br:
                            p_br = st.number_input(
                                "BR (%)",
                                0, 100,
                                value=min(8 + i*3, 75),
                                key=f"p_br_{i}"
                            )
                        
                        with col_up:
                            p_up = st.number_input(
                                "UP (%)",
                                0, 100,
                                value=min(12 + i*2, 70),
                                key=f"p_up_{i}"
                            )
                        
                        timeline_manual.append({
                            "etape": nom_etape,
                            "P_DB": p_db / 100,
                            "P_DP": p_dp / 100,
                            "P_BR": p_br / 100,
                            "P_UP": p_up / 100
                        })
                    
                    st.markdown("**Impact business**")
                    col_rec, col_fin = st.columns(2)
                    
                    with col_rec:
                        impact_records_manual = st.number_input(
                            "Records affect√©s",
                            0, 10000,
                            value=100
                        )
                    
                    with col_fin:
                        impact_financier_manual = st.number_input(
                            "Impact ‚Ç¨/mois",
                            0, 500000,
                            value=15000,
                            step=1000
                        )
                    
                    submitted_manual = st.form_submit_button("‚úÖ Cr√©er Sc√©nario", type="primary")
                
                if submitted_manual:
                    # Calculer risques
                    risk_source = (timeline_manual[0]['P_DB'] + timeline_manual[0]['P_DP'] + 
                                 timeline_manual[0]['P_BR'] + timeline_manual[0]['P_UP']) / 4
                    risk_final = (timeline_manual[-1]['P_DB'] + timeline_manual[-1]['P_DP'] + 
                                timeline_manual[-1]['P_BR'] + timeline_manual[-1]['P_UP']) / 4
                    
                    scenario_data = {
                        "nom": nom_incident_manual,
                        "emoji": "‚úèÔ∏è",
                        "contexte": contexte_manual,
                        "risk_source": risk_source,
                        "risk_final": risk_final,
                        "delta": risk_final - risk_source,
                        "timeline": timeline_manual,
                        "interpretation": f"{'üö® CRITIQUE' if risk_final > 0.5 else '‚ö†Ô∏è √âLEV√â' if risk_final > 0.3 else '‚û°Ô∏è MOYEN'} - Sc√©nario personnalis√©",
                        "cause_racine": "Incident personnalis√© (d√©tails dans contexte)",
                        "impact_records": impact_records_manual,
                        "impact_financier": impact_financier_manual,
                        "temps_detection": "√Ä d√©finir selon monitoring",
                        "actions": [
                            "Action 1 : √Ä d√©finir selon contexte",
                            "Action 2 : Impl√©menter monitoring sp√©cifique",
                            "Action 3 : Corriger cause racine identifi√©e"
                        ]
                    }
                    
                    st.session_state.custom_scenario_data = scenario_data
                    st.success("‚úÖ Sc√©nario manuel cr√©√© !")
                    st.balloons()
                
                # Afficher sc√©nario manuel si disponible
                if 'custom_scenario_data' in st.session_state:
                    scenario_data = st.session_state.custom_scenario_data
                    st.success("‚úÖ Utilisation du sc√©nario manuel")
                else:
                    st.info("üí° Remplis le formulaire ci-dessus et clique 'Cr√©er' pour d√©finir ton sc√©nario")
                    st.stop()
        
        else:  # D√©gradation cumulative
            scenario_data = {
                "nom": "D√©gradation Cumulative Multi-√âtapes",
                "emoji": "‚ö†Ô∏è",
                "contexte": """**Absence monitoring interm√©diaire** : Chaque √©tape d√©grade l√©g√®rement la qualit√©.

**Probl√®me** : Aucune micro-d√©gradation n'est critique individuellement, mais l'accumulation devient critique.

**√âtapes** :
1. Extraction : +3% erreurs parsing (tol√©r√©e)
2. Enrichissement : +5% incoh√©rences r√©f√©rentiels (tol√©r√©e)
3. Transformation : +4% erreurs calculs (tol√©r√©e)
4. Agr√©gation : +6% doublons (tol√©r√©e)
5. **Cumul : +18% = CRITIQUE** ‚ùå

**D√©couverte** : R√©clamations utilisateurs apr√®s 1 semaine production.""",
                "risk_source": 0.332,
                "risk_final": 0.518,
                "delta": 0.186,
                "timeline": [
                    {"etape": "Source SIRH", "P_DB": 0.367, "P_DP": 0.020, "P_BR": 0.050, "P_UP": 0.100},
                    {"etape": "ETL Extraction", "P_DB": 0.367, "P_DP": 0.050, "P_BR": 0.050, "P_UP": 0.100},
                    {"etape": "Enrichissement", "P_DB": 0.367, "P_DP": 0.085, "P_BR": 0.100, "P_UP": 0.100},
                    {"etape": "Transformation", "P_DB": 0.367, "P_DP": 0.125, "P_BR": 0.140, "P_UP": 0.120},
                    {"etape": "Agr√©gation", "P_DB": 0.367, "P_DP": 0.180, "P_BR": 0.180, "P_UP": 0.150},
                    {"etape": "Calcul Final", "P_DB": 0.367, "P_DP": 0.245, "P_BR": 0.230, "P_UP": 0.180}
                ],
                "interpretation": "‚ö†Ô∏è √âLEV√â - Accumulation progressive (+18.6 pts) pass√©e inaper√ßue sans monitoring",
                "cause_racine": "Absence monitoring interm√©diaire granulaire (only input/output)",
                "impact_records": 127,
                "impact_financier": 18200,
                "temps_detection": "1 semaine (r√©clamations users vs d√©tection proactive impossible)",
                "actions": [
                    "üîî Impl√©menter monitoring INTER-√âTAPES (pas juste I/O)",
                    "üìä Seuils tol√©rance par √©tape (ex: +2% max)",
                    "üö® Alerte cumulative si somme d√©gradations >10%",
                    "üìà Dashboard lineage temps r√©el (visualiser accumulation)",
                    "üß™ Tests automatis√©s apr√®s chaque transformation"
                ]
            }
        
        # Afficher contexte sc√©nario
        st.markdown("---")
        st.markdown(f"### {scenario_data['emoji']} {scenario_data['nom']}")
        
        with st.expander("üìã Contexte d√©taill√©", expanded=True):
            st.markdown(scenario_data['contexte'])
        
        # M√©triques impact
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "Risque source",
                f"{scenario_data['risk_source']:.1%}"
            )
        
        with col2:
            delta_val = scenario_data['delta']
            st.metric(
                "Risque final",
                f"{scenario_data['risk_final']:.1%}",
                delta=f"+{delta_val:.1%}",
                delta_color="inverse"
            )
        
        with col3:
            st.metric(
                "Enregistrements affect√©s",
                f"{scenario_data['impact_records']}"
            )
        
        with col4:
            if scenario_data['impact_financier'] > 0:
                st.metric(
                    "Impact financier/mois",
                    f"{scenario_data['impact_financier']:,}‚Ç¨"
                )
            else:
                st.metric("Impact financier", "0‚Ç¨")
        
        st.markdown("---")
        
        # Timeline propagation
        st.subheader("üìÖ Timeline Propagation D√©taill√©e")
        
        for idx, step in enumerate(scenario_data['timeline']):
            etape = step['etape']
            
            # Afficher nom √©tape
            st.markdown(f"### üîπ √âtape {idx+1} : {etape}")
            
            # M√©triques 4D
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("DB", f"{step['P_DB']:.1%}", 
                       delta=f"{(step['P_DB'] - scenario_data['timeline'][max(0,idx-1)]['P_DB']):.1%}" if idx > 0 else None,
                       delta_color="inverse")
            col2.metric("DP", f"{step['P_DP']:.1%}",
                       delta=f"{(step['P_DP'] - scenario_data['timeline'][max(0,idx-1)]['P_DP']):.1%}" if idx > 0 else None,
                       delta_color="inverse")
            col3.metric("BR", f"{step['P_BR']:.1%}",
                       delta=f"{(step['P_BR'] - scenario_data['timeline'][max(0,idx-1)]['P_BR']):.1%}" if idx > 0 else None,
                       delta_color="inverse")
            col4.metric("UP", f"{step['P_UP']:.1%}",
                       delta=f"{(step['P_UP'] - scenario_data['timeline'][max(0,idx-1)]['P_UP']):.1%}" if idx > 0 else None,
                       delta_color="inverse")
            
            # Bouton explication par √©tape
            if st.button(f"üí¨ Expliquer cette √©tape", key=f"explain_lineage_step_{idx}", help=f"Que se passe-t-il dans '{etape}' ?"):
                if 'anthropic_api_key' in st.session_state and st.session_state.anthropic_api_key:
                    with st.spinner(f"Claude analyse l'√©tape '{etape}'..."):
                        # Pr√©parer contexte pour IA
                        step_context = f"""Tu es un expert Data Quality. Explique ce qui se passe dans cette √©tape de lineage en 3-4 phrases maximum.

**Sc√©nario global** : {scenario_data['nom']}
**√âtape actuelle** : {etape} (√©tape {idx+1}/{len(scenario_data['timeline'])})

**Probabilit√©s actuelles** :
- P_DB = {step['P_DB']:.1%} (Database)
- P_DP = {step['P_DP']:.1%} (Data Processing)
- P_BR = {step['P_BR']:.1%} (Business Rules)
- P_UP = {step['P_UP']:.1%} (Usage-fit)

**√âvolution depuis √©tape pr√©c√©dente** :
{f"- Delta DB : {(step['P_DB'] - scenario_data['timeline'][idx-1]['P_DB'])*100:+.1f} points" if idx > 0 else "- √âtape source (baseline)"}
{f"- Delta DP : {(step['P_DP'] - scenario_data['timeline'][idx-1]['P_DP'])*100:+.1f} points" if idx > 0 else ""}
{f"- Delta BR : {(step['P_BR'] - scenario_data['timeline'][idx-1]['P_BR'])*100:+.1f} points" if idx > 0 else ""}
{f"- Delta UP : {(step['P_UP'] - scenario_data['timeline'][idx-1]['P_UP'])*100:+.1f} points" if idx > 0 else ""}

**Contexte incident** : {scenario_data['contexte'][:200]}...

Explique :
1. Ce que fait cette √©tape techniquement (extraction, transformation, agr√©gation, etc.)
2. Pourquoi telle(s) dimension(s) sont affect√©es √† cette √©tape
3. L'impact op√©rationnel concret de cette d√©gradation

Style : Technique mais accessible, p√©dagogique, avec exemples concrets."""
                        
                        try:
                            client = anthropic.Anthropic(api_key=st.session_state.anthropic_api_key)
                            
                            response = client.messages.create(
                                model="claude-sonnet-4-20250514",
                                max_tokens=500,
                                system="Tu es un expert Data Quality qui explique les transformations de donn√©es avec un style direct, p√©dagogique et technique. Toujours 3-4 phrases maximum.",
                                messages=[{"role": "user", "content": step_context}]
                            )
                            
                            comment = response.content[0].text
                            st.info(f"üí¨ **Explication Claude - {etape}** :\n\n{comment}")
                            
                        except Exception as e:
                            st.error(f"‚ö†Ô∏è Erreur g√©n√©ration commentaire : {str(e)}")
                else:
                    st.warning("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar")
            
            st.markdown("---")
        
        # Interpr√©tation
        st.info(f"**Interpr√©tation :** {scenario_data['interpretation']}")
        
        # Cause racine + Impact
        st.markdown("### üîç Analyse Incident")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**üéØ Cause Racine**")
            st.write(scenario_data['cause_racine'])
            
            if scenario_data['temps_detection'] != "N/A":
                st.success(f"‚è±Ô∏è **Temps d√©tection** : {scenario_data['temps_detection']}")
        
        with col2:
            st.markdown("**üìä Impact Mesur√©**")
            st.write(f"‚Ä¢ Records affect√©s : **{scenario_data['impact_records']}** / 687")
            if scenario_data['impact_financier'] > 0:
                st.write(f"‚Ä¢ Impact financier : **{scenario_data['impact_financier']:,}‚Ç¨/mois**")
            st.write(f"‚Ä¢ Delta risque : **+{scenario_data['delta']:.1%}** points")
        
        # Actions recommand√©es
        if scenario_data['actions']:
            st.markdown("### üõ†Ô∏è Actions Recommand√©es")
            
            for action in scenario_data['actions']:
                st.write(f"‚Ä¢ {action}")
        
        # Bouton commentaire IA
        st.markdown("---")
        if st.button("üí¨ Analyser avec Claude", key="explain_lineage", help="Analyse d√©taill√©e de la propagation"):
            if 'anthropic_api_key' in st.session_state and st.session_state.anthropic_api_key:
                with st.spinner("Claude analyse la propagation..."):
                    data_for_ai = {
                        "scenario": scenario_data['nom'],
                        "risk_source": scenario_data['risk_source'],
                        "risk_final": scenario_data['risk_final'],
                        "delta_absolute": scenario_data['delta'],
                        "cause_racine": scenario_data['cause_racine'],
                        "impact_records": scenario_data['impact_records'],
                        "impact_financier": scenario_data['impact_financier'],
                        "temps_detection": scenario_data['temps_detection'],
                        "history": scenario_data['timeline']
                    }
                    
                    comment = generate_ai_comment("lineage", data_for_ai, st.session_state.anthropic_api_key)
                    st.info(f"üí¨ **Analyse Claude** :\n\n{comment}")
            else:
                st.warning("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar")
    
    # ========================================================================
    # TAB 5 : COMPARAISON DAMA
    # ========================================================================
    with tab6:
        st.header("üìà Comparaison DAMA vs Probabiliste")
        
        # SECTION EXPLICATIVE EN HAUT
        with st.expander("üìö D√©finition des Indicateurs DAMA (6 dimensions)", expanded=False):
            st.markdown("""
            Le **DAMA-DMBOK** (Data Management Body of Knowledge) d√©finit **6 dimensions classiques** de qualit√© :
            
            | Dimension | D√©finition | Comment calcul√© dans cette d√©mo |
            |-----------|------------|--------------------------------|
            | **Compl√©tude** (Completeness) | % de donn√©es pr√©sentes vs attendues | `1 - (nb_null / nb_total)` |
            | **Exactitude** (Accuracy) | % de donn√©es correctes vs valeurs r√©elles | D√©tection erreurs typage + formats |
            | **Coh√©rence** (Consistency) | % respect r√®gles m√©tier et contraintes | V√©rif r√®gles business d√©finies |
            | **Fra√Æcheur** (Timeliness) | % donn√©es √† jour selon fra√Æcheur attendue | Analyse champs date/timestamp |
            | **Validit√©** (Validity) | % donn√©es conformes formats/domaines | Validation domaines valeurs |
            | **Unicit√©** (Uniqueness) | % absence doublons | D√©tection doublons exacts |
            
            **üìä Score Global DAMA** = Moyenne simple des 6 dimensions (poids √©gaux)
            
            ‚ö†Ô∏è **Limites approche DAMA** :
            - Toutes dimensions comptent pareil (pas de pond√©ration usage)
            - Seuils binaires (OK/KO) au lieu de probabilit√©s continues
            - Ne capture pas l'incertitude ni la confiance dans les mesures
            - Pas de contexte d'usage m√©tier int√©gr√©
            """)
        
        with st.expander("üéØ Approche Probabiliste (Framework 4D)", expanded=False):
            st.markdown("""
            Notre framework utilise **4 dimensions avec pond√©rations contextuelles** :
            
            | Dimension | Formule Beta | Pond√©ration usage |
            |-----------|--------------|-------------------|
            | **P_DB** (Structure) | Beta(Œ±, Œ≤) depuis nb_null, errors | w_DB selon usage |
            | **P_DP** (Traitements) | Beta depuis erreurs transformation | w_DP selon usage |
            | **P_BR** (R√®gles M√©tier) | Beta depuis violations business | w_BR selon usage |
            | **P_UP** (Utilisabilit√©) | Beta depuis inad√©quation usage | w_UP selon usage |
            
            **üéØ Score Risque Contextualis√©** = Œ£ (P_dim √ó w_dim)
            
            ‚úÖ **Avantages** :
            - Probabilit√©s continues (0-100%) avec incertitude captur√©e
            - Pond√©rations adapt√©es √† l'usage m√©tier sp√©cifique
            - Distributions Beta permettent agr√©gation math√©matique rigoureuse
            - D√©tection proactive vs r√©active
            """)
        
        st.markdown("---")
        
        comparaison = results['comparaison']
        
        # Scores DAMA
        st.subheader("üìä Scores DAMA Traditionnels")
        st.caption("üí° Survolez les m√©triques pour voir comment chaque score est calcul√©")
        
        for attr, dama_score in comparaison['dama_scores'].items():
            col1, col2 = st.columns([1, 3])
            
            with col1:
                # Calculer formule affich√©e
                dims = ['completeness', 'consistency', 'accuracy', 'timeliness', 'validity', 'uniqueness']
                # G√©rer valeurs None (dimensions incalculables)
                dim_values = []
                dim_labels = []
                for d in dims:
                    val = dama_score.get(d)
                    if val is not None:
                        dim_values.append(val)
                        dim_labels.append(f"{val:.1%}")
                    else:
                        dim_values.append(None)
                        dim_labels.append("N/A")
                
                # Compter dimensions calculables
                nb_calculables = dama_score.get('dimensions_calculables', len([v for v in dim_values if v is not None]))
                nb_total = dama_score.get('dimensions_total', 6)
                
                # Formule avec N/A
                formule_parts = []
                for v, label in zip(dim_values, dim_labels):
                    formule_parts.append(label if v is not None else "N/A")
                formule = " + ".join(formule_parts)
                
                score_global = dama_score['score_global']
                
                st.metric(
                    attr,
                    f"{score_global:.1%}",
                    help=f"""**Calcul** : ({formule}) / {nb_calculables} = {score_global:.1%}

Dimensions DAMA : {nb_calculables}/{nb_total} calculables

‚úÖ Completeness, Uniqueness
‚ùå Consistency, Accuracy, Timeliness, Validity (m√©tadonn√©es manquantes)
"""
                )
            
            with col2:
                # TOUJOURS afficher les 6 dimensions DAMA (m√™me si non calculables)
                vals = []
                colors = []
                texts = []
                hovertexts = []
                patterns = []  # Pour barres hachur√©es
                
                for d in dims:
                    val = dama_score.get(d)
                    if val is not None:
                        # Dimension calcul√©e ‚úÖ
                        vals.append(val * 100)
                        colors.append('rgba(100, 200, 255, 0.8)')  # Bleu
                        texts.append(f"{val*100:.1f}%")
                        hovertexts.append(f"<b>{d}</b><br>‚úÖ Calcul√©<br>Score: {val*100:.1f}%")
                        patterns.append("")
                    else:
                        # Dimension non calculable ‚ùå
                        vals.append(15)  # Petite barre pour voir la croix
                        colors.append('rgba(200, 50, 50, 0.3)')  # Rouge transparent
                        texts.append("‚ùå")
                        hovertexts.append(f"<b>{d}</b><br>‚ùå Non calculable<br>M√©tadonn√©es manquantes")
                        patterns.append("/")  # Hachures
                
                # Graphique avec les 6 barres
                fig = go.Figure(data=[
                    go.Bar(
                        x=dims, 
                        y=vals, 
                        marker=dict(
                            color=colors,
                            line=dict(color='white', width=1)
                        ),
                        text=texts,
                        textposition='outside',
                        textfont=dict(size=14),
                        hovertext=hovertexts,
                        hoverinfo='text'
                    )
                ])
                
                # Ajouter des croix rouges pour les N/A
                annotations = []
                for i, (val, d) in enumerate(zip(vals, dims)):
                    if dama_score.get(d) is None:
                        annotations.append(
                            dict(
                                x=i,
                                y=val + 5,
                                text="<b>‚ùå</b>",
                                showarrow=False,
                                font=dict(size=20, color='red')
                            )
                        )
                
                fig.update_layout(
                    height=220, 
                    template="plotly_dark", 
                    showlegend=False,
                    yaxis=dict(
                        title="Score (%)",
                        range=[0, 110]
                    ),
                    xaxis_title="Dimensions DAMA (6 dimensions ISO 8000)",
                    annotations=annotations
                )
                st.plotly_chart(fig, use_container_width=True, key=f"dama_chart_{attr}")
                
                # L√©gende explicative CLAIRE
                col_legend1, col_legend2 = st.columns(2)
                with col_legend1:
                    st.caption("üîµ **Calcul√©** : Formule appliqu√©e sur donn√©es")
                with col_legend2:
                    st.caption("‚ùå **Non calculable** : M√©tadonn√©es manquantes")
                
                # Afficher formule d√©taill√©e
                with st.expander(f"üîç D√©tail calcul {attr}", expanded=False):
                    st.markdown("**Formule score global :**")
                    st.code(f"""
Score DAMA = Moyenne des dimensions CALCULABLES uniquement

Compl√©tude     : {"‚úÖ " + f"{dama_score.get('completeness', 0)*100:.1f}%" if dama_score.get('completeness') is not None else "‚ùå N/A"}
Coh√©rence      : {"‚úÖ " + f"{dama_score.get('consistency', 0)*100:.1f}%" if dama_score.get('consistency') is not None else "‚ùå N/A (r√®gles manquantes)"}
Exactitude     : {"‚úÖ " + f"{dama_score.get('accuracy', 0)*100:.1f}%" if dama_score.get('accuracy') is not None else "‚ùå N/A (ground truth manquant)"}
Fra√Æcheur      : {"‚úÖ " + f"{dama_score.get('timeliness', 0)*100:.1f}%" if dama_score.get('timeliness') is not None else "‚ùå N/A (r√®gle fra√Æcheur manquante)"}
Validit√©       : {"‚úÖ " + f"{dama_score.get('validity', 0)*100:.1f}%" if dama_score.get('validity') is not None else "‚ùå N/A (domaine manquant)"}
Unicit√©        : {"‚úÖ " + f"{dama_score.get('uniqueness', 0)*100:.1f}%" if dama_score.get('uniqueness') is not None else "‚ùå N/A"}

= ({formule}) / {nb_calculables} = {score_global:.1%}
                    """, language="text")
        
        st.markdown("---")
        
        # Probl√®mes masqu√©s
        if comparaison['problemes_masques']:
            st.subheader("üîç Probl√®mes Masqu√©s par DAMA")
            
            for pb in comparaison['problemes_masques']:
                st.warning(f"**{pb['attribut']}** : {pb['explication']}")
        
        st.markdown("---")
        
        # Gains m√©thodologiques
        st.subheader("üéØ Gains M√©thodologiques")
        
        for gain in comparaison['gains'][:5]:  # Top 5
            with st.expander(f"‚ú® {gain['categorie']}"):
                st.write(f"**DAMA :** {gain.get('methode_dama', 'N/A')}")
                st.write(f"**Probabiliste :** {gain.get('methode_probabiliste', gain.get('gain', 'N/A'))}")
                st.success(f"**Impact :** {gain.get('impact_operationnel', gain.get('impact', 'N/A'))}")
        
        # Bouton commentaire IA synth√®se
        if st.button("üí¨ Synth√©tiser avec Claude", key="explain_dama", help="Synth√®se comparative DAMA vs Probabiliste"):
            if 'anthropic_api_key' in st.session_state and st.session_state.anthropic_api_key:
                with st.spinner("Claude synth√©tise la comparaison..."):
                    # Calculer DAMA moyen
                    dama_scores = [score['score_global'] for score in comparaison['dama_scores'].values()]
                    dama_avg = sum(dama_scores) / len(dama_scores) if dama_scores else 0
                    
                    data_for_ai = {
                        "dama_avg": dama_avg,
                        "masked_count": len(comparaison.get('problemes_masques', [])),
                        "gains": comparaison['gains'][:5],
                        "attributs_analyzed": len(comparaison['dama_scores'])
                    }
                    
                    comment = generate_ai_comment("dama", data_for_ai, st.session_state.anthropic_api_key)
                    st.info(f"üí¨ **Synth√®se Claude** :\n\n{comment}")
            else:
                st.warning("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar")
    
    # ========================================================================
    # TAB 6 : √âLICITATION IA (VRAIE LLM)
    # ========================================================================
    with tab7:
        st.header("üí¨ √âlicitation Assist√©e par IA")
        
        st.markdown("""
        ### üéØ Objectif : R√©duire 240h d'√©licitation √† 12 minutes
        
        Le **Compagnon IA Claude** dialogue avec toi pour affiner les pond√©rations AHP de mani√®re progressive.
        Au lieu de deviner les poids, l'IA pose des questions cibl√©es et ajuste automatiquement.
        
        ‚ö° **Cette d√©mo utilise l'API Claude d'Anthropic pour un dialogue r√©el et adaptatif.**
        """)
        
        # V√©rifier cl√© API
        if 'anthropic_api_key' not in st.session_state or not st.session_state.anthropic_api_key:
            st.error("üîë **Cl√© API Claude manquante**")
            st.info("Configure ta cl√© API Claude dans la sidebar pour activer cette fonctionnalit√©.")
            st.stop()
        
        # Initialiser client Anthropic
        try:
            import anthropic
            client = anthropic.Anthropic(api_key=st.session_state.anthropic_api_key)
        except Exception as e:
            st.error(f"‚ùå Erreur initialisation API : {str(e)}")
            st.stop()
        
        # Initialiser chat history
        if 'elicitation_messages' not in st.session_state:
            st.session_state.elicitation_messages = []
            st.session_state.current_usage = "Paie r√©glementaire"
            st.session_state.elicited_weights = {}
            st.session_state.elicitation_context = {
                "usage": "Paie r√©glementaire",
                "dimensions_discutees": [],
                "etape": "initiale"
            }
        
        # Syst√®me prompt pour Claude
        SYSTEM_PROMPT = """Tu es un expert en Data Quality et √©licitation de pr√©f√©rences AHP (Analytic Hierarchy Process).

Ton r√¥le est d'aider l'utilisateur √† d√©finir les pond√©rations optimales pour 4 dimensions de qualit√© donn√©es :

**4 Dimensions** :
- [DB] Database : Contraintes structurelles (types, formats, sch√©ma)
- [DP] Data Processing : Transformations ETL, calculs d√©riv√©s
- [BR] Business Rules : R√®gles m√©tier, coh√©rence s√©mantique
- [UP] Usage-fit : Ad√©quation au contexte d'utilisation

**Contexte actuel** :
- Usage m√©tier : "{usage}"
- Objectif : D√©finir w_DB, w_DP, w_BR, w_UP tels que leur somme = 100%

**Ton approche** :
1. Pose des questions par comparaisons pair√©es (A vs B : lequel est plus important ?)
2. Propose des justifications concr√®tes li√©es au contexte m√©tier
3. Ajuste progressivement les pond√©rations selon les r√©ponses
4. Sugg√®re des valeurs Beta concr√®tes quand pertinent
5. Reste concis (2-3 phrases max par message)
6. Utilise des emojis pour clart√© (üéØ, ‚úÖ, ‚öñÔ∏è, etc.)

**R√®gles** :
- Ne jamais proposer de pond√©rations qui ne somment pas √† 100%
- Toujours expliquer POURQUOI une dimension est importante dans ce contexte
- Si l'utilisateur h√©site, proposer des exemples concrets
- Quand les 4 pond√©rations sont d√©finies, r√©sumer et demander validation

**Format r√©ponse** :
- Questions courtes et directes
- Maximum 3 phrases
- √âviter le jargon technique sauf si n√©cessaire
"""
        
        # Afficher historique chat
        st.markdown("### üí≠ Conversation avec Claude")
        
        chat_container = st.container()
        
        with chat_container:
            for msg in st.session_state.elicitation_messages:
                if msg["role"] == "assistant":
                    with st.chat_message("assistant", avatar="ü§ñ"):
                        st.markdown(msg["content"])
                        st.caption(f"_{msg['timestamp'].strftime('%H:%M:%S')}_")
                else:
                    with st.chat_message("user", avatar="üë§"):
                        st.markdown(msg["content"])
                        st.caption(f"_{msg['timestamp'].strftime('%H:%M:%S')}_")
        
        # Zone input utilisateur
        st.markdown("---")
        
        # Si pas encore de conversation, d√©marrer
        if len(st.session_state.elicitation_messages) == 0:
            if st.button("üöÄ D√©marrer l'√©licitation", type="primary", use_container_width=True):
                with st.spinner("Claude r√©fl√©chit..."):
                    try:
                        # Premier message de Claude
                        message = client.messages.create(
                            model="claude-sonnet-4-20250514",
                            max_tokens=500,
                            system=SYSTEM_PROMPT.format(usage=st.session_state.current_usage),
                            messages=[
                                {
                                    "role": "user",
                                    "content": f"Bonjour ! Je veux d√©finir les pond√©rations AHP pour l'usage '{st.session_state.current_usage}'. Peux-tu m'aider en me posant des questions ?"
                                }
                            ]
                        )
                        
                        assistant_response = message.content[0].text
                        
                        st.session_state.elicitation_messages.append({
                            "role": "assistant",
                            "content": assistant_response,
                            "timestamp": datetime.now()
                        })
                        
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"‚ùå Erreur API Claude : {str(e)}")
        
        # Zone r√©ponse utilisateur
        if len(st.session_state.elicitation_messages) > 0:
            user_input = st.text_input(
                "Ta r√©ponse :",
                key="elicitation_input",
                placeholder="Ex: Je pense que [DB] est plus important car la paie n√©cessite une structure stricte..."
            )
            
            col1, col2 = st.columns([3, 1])
            
            with col1:
                send_button = st.button("üì§ Envoyer", type="primary", use_container_width=True)
            
            with col2:
                reset_button = st.button("üîÑ Reset", use_container_width=True)
            
            if reset_button:
                st.session_state.elicitation_messages = []
                st.session_state.elicited_weights = {}
                st.rerun()
            
            if send_button and user_input:
                # Ajouter message utilisateur
                st.session_state.elicitation_messages.append({
                    "role": "user",
                    "content": user_input,
                    "timestamp": datetime.now()
                })
                
                with st.spinner("Claude r√©fl√©chit..."):
                    try:
                        # Construire historique conversation
                        messages = []
                        for msg in st.session_state.elicitation_messages:
                            messages.append({
                                "role": msg["role"],
                                "content": msg["content"]
                            })
                        
                        # Appeler API Claude
                        response = client.messages.create(
                            model="claude-sonnet-4-20250514",
                            max_tokens=500,
                            system=SYSTEM_PROMPT.format(usage=st.session_state.current_usage),
                            messages=messages
                        )
                        
                        assistant_response = response.content[0].text
                        
                        # Ajouter r√©ponse Claude
                        st.session_state.elicitation_messages.append({
                            "role": "assistant",
                            "content": assistant_response,
                            "timestamp": datetime.now()
                        })
                        
                        # Tenter d'extraire pond√©rations du texte
                        import re
                        
                        # Pattern pour d√©tecter pond√©rations
                        patterns = [
                            r'w_DB[:\s=]+(\d+)%',
                            r'w_DP[:\s=]+(\d+)%',
                            r'w_BR[:\s=]+(\d+)%',
                            r'w_UP[:\s=]+(\d+)%',
                            r'\[?DB\]?[:\s]+(\d+)%',
                            r'\[?DP\]?[:\s]+(\d+)%',
                            r'\[?BR\]?[:\s]+(\d+)%',
                            r'\[?UP\]?[:\s]+(\d+)%'
                        ]
                        
                        weights_found = {}
                        for pattern in patterns:
                            matches = re.findall(pattern, assistant_response, re.IGNORECASE)
                            if matches:
                                if 'DB' in pattern.upper():
                                    weights_found['w_DB'] = int(matches[0]) / 100
                                elif 'DP' in pattern.upper():
                                    weights_found['w_DP'] = int(matches[0]) / 100
                                elif 'BR' in pattern.upper():
                                    weights_found['w_BR'] = int(matches[0]) / 100
                                elif 'UP' in pattern.upper():
                                    weights_found['w_UP'] = int(matches[0]) / 100
                        
                        # Si 4 pond√©rations trouv√©es, sauvegarder
                        if len(weights_found) == 4:
                            st.session_state.elicited_weights[st.session_state.current_usage] = weights_found
                        
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"‚ùå Erreur API Claude : {str(e)}")
        
        # Afficher pond√©rations √©licit√©es
        if st.session_state.elicited_weights:
            st.markdown("---")
            st.markdown("### üìä Pond√©rations √©licit√©es")
            
            for usage, weights in st.session_state.elicited_weights.items():
                st.markdown(f"**{usage}**")
                
                col1, col2, col3, col4, col5 = st.columns(5)
                col1.metric("DB", f"{weights.get('w_DB', 0):.0%}")
                col2.metric("DP", f"{weights.get('w_DP', 0):.0%}")
                col3.metric("BR", f"{weights.get('w_BR', 0):.0%}")
                col4.metric("UP", f"{weights.get('w_UP', 0):.0%}")
                
                total = sum(weights.values())
                if abs(total - 1.0) < 0.01:
                    col5.success(f"‚úÖ {total:.0%}")
                else:
                    col5.warning(f"‚ö†Ô∏è {total:.0%}")
                
                # Bouton appliquer
                if st.button(f"‚úÖ Appliquer ces pond√©rations", key=f"apply_{usage}"):
                    # Ajouter √† custom_usages ou custom_weights
                    if 'custom_weights' not in st.session_state:
                        st.session_state.custom_weights = {}
                    
                    st.session_state.custom_weights[usage] = weights
                    st.success(f"‚úÖ Pond√©rations appliqu√©es ! Tu peux maintenant relancer l'analyse.")
        
        # Stats conversation
        if st.session_state.elicitation_messages:
            st.markdown("---")
            nb_messages = len(st.session_state.elicitation_messages)
            duree = (st.session_state.elicitation_messages[-1]['timestamp'] - 
                    st.session_state.elicitation_messages[0]['timestamp']).seconds // 60
            
            col1, col2, col3 = st.columns(3)
            col1.metric("Messages √©chang√©s", nb_messages)
            col2.metric("Dur√©e conversation", f"{duree} min" if duree > 0 else "< 1 min")
            col3.metric("Gain vs manuel", f"{240 // max(duree, 1)}√ó" if duree > 0 else "480√ó")
    
    # ========================================================================
    # TAB 7 : SURVEILLANCE
    # ========================================================================
    with tab8:
        st.header("üîî Surveillance Proactive")
        
        st.markdown("""
        ### üéØ Objectif : D√©tecter incidents en 9h au lieu de 3 semaines
        
        Le syst√®me surveille en continu les vecteurs 4D et **alerte d√®s qu'une d√©gradation est d√©tect√©e**.
        """)
        
        # Simulation alerte
        st.markdown("---")
        st.markdown("### üìß Simulation Alerte Email")
        
        # Email mockup - version √©pur√©e
        st.markdown("""
        <div style="background: #ffffff; padding: 1.5rem; border-radius: 8px; border: 2px solid #E57373; border-left: 4px solid #ef4444;">
            <p style="color: #000000; margin: 0; font-size: 0.9rem;"><strong>De:</strong> monitoring@framework-dq.ai</p>
            <p style="color: #000000; margin: 0; font-size: 0.9rem;"><strong>√Ä:</strong> data-steward@entreprise.fr</p>
            <p style="color: #000000; margin: 0; font-size: 0.9rem;"><strong>Date:</strong> 13/01/2025 08:30</p>
            <p style="color: #000000; margin: 0; font-size: 0.9rem;"><strong>Objet:</strong> üö® ALERTE CRITIQUE - D√©gradation qualit√© d√©tect√©e</p>
            <hr style="border-color: #d2d2d7; margin: 1rem 0;">
            <h3 style="color: #ef4444; margin-top: 0.5rem; font-size: 1.3rem;">‚ö†Ô∏è ALERTE CRITIQUE</h3>
            <p style="color: #000000; margin: 0.3rem 0;"><strong>Attribut:</strong> Anciennet√©</p>
            <p style="color: #000000; margin: 0.3rem 0;"><strong>Usage:</strong> Paie r√©glementaire</p>
            <p style="color: #000000; margin: 0.3rem 0;"><strong>Dimension affect√©e:</strong> [DP] Data Processing</p>
            <p style="color: #000000; margin: 0.3rem 0;"><strong>Risque:</strong> <span style="color: #ef4444; font-weight: 600;">46.3% ‚Üí 62.3%</span> (+16 points)</p>
            <p style="color: #000000; margin: 0.3rem 0;"><strong>S√©v√©rit√©:</strong> <span style="background: #FFEBEE; color: #ef4444; padding: 4px 12px; border-radius: 4px; border: 1px solid #E57373; font-weight: 600;">CRITIQUE</span></p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("---")
        st.markdown("### üìÖ Timeline Incident")
        
        # Timeline avec √©tapes
        timeline_data = [
            {
                "time": "12/01/2025 23:15",
                "event": "üîß D√©ploiement ETL v2.8.2",
                "details": "Commit abc123def - Modification parser Anciennet√©",
                "status": "info"
            },
            {
                "time": "13/01/2025 00:30",
                "event": "üìä Calcul batch quotidien",
                "details": "141/687 erreurs parsing d√©tect√©es (+12% vs baseline)",
                "status": "warning"
            },
            {
                "time": "13/01/2025 03:00",
                "event": "ü§ñ Analyse propagation risque",
                "details": "P_DP: 2.0% ‚Üí 28.5% | Score Paie: 46.3% ‚Üí 62.3%",
                "status": "warning"
            },
            {
                "time": "13/01/2025 08:30",
                "event": "üö® Alerte envoy√©e",
                "details": "Email + Slack #data-quality | Priorit√©: CRITIQUE",
                "status": "error"
            },
            {
                "time": "13/01/2025 09:06",
                "event": "‚úÖ Action corrective",
                "details": "Rollback ETL v2.8.2 ‚Üí v2.8.1 | Score revenu 46.3%",
                "status": "success"
            }
        ]
        
        for item in timeline_data:
            if item["status"] == "error":
                st.error(f"**{item['time']}** - {item['event']}\n\n{item['details']}")
            elif item["status"] == "warning":
                st.warning(f"**{item['time']}** - {item['event']}\n\n{item['details']}")
            elif item["status"] == "success":
                st.success(f"**{item['time']}** - {item['event']}\n\n{item['details']}")
            else:
                st.info(f"**{item['time']}** - {item['event']}\n\n{item['details']}")
        
        st.markdown("---")
        st.markdown("### üîç Analyse Causale")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Cause racine identifi√©e**")
            st.code("""
# ETL v2.8.2 - parser.py
# Commit abc123def

# AVANT (v2.8.1) ‚úÖ
anciennete = float(value.replace(',', '.'))

# APR√àS (v2.8.2) ‚ùå
anciennete = float(value)  # Bug: virgule non g√©r√©e
            """, language="python")
        
        with col2:
            st.markdown("**Impact mesur√©**")
            st.metric("Erreurs parsing", "141 / 687", delta="+12%", delta_color="inverse")
            st.metric("P_DP d√©grad√©", "28.5%", delta="+26.5 points", delta_color="inverse")
            st.metric("Score Paie", "62.3%", delta="+16 points", delta_color="inverse")
            st.metric("Temps d√©tection", "9h", delta="-3 sem vs DAMA", delta_color="normal")
        
        st.markdown("---")
        st.markdown("### üõ†Ô∏è Actions disponibles")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("üîÑ Rollback ETL v2.8.1", type="primary", use_container_width=True):
                with st.spinner("Rollback en cours..."):
                    time.sleep(2)
                st.success("‚úÖ Rollback effectu√© ! Score revenu √† 46.3%")
        
        with col2:
            if st.button("üìä Voir logs complets", use_container_width=True):
                st.code("""
[2025-01-13 00:30:15] INFO: Batch ETL started
[2025-01-13 00:30:16] ERROR: ValueError in parse_anciennete()
  Line 342: could not convert string to float: '7,21'
[2025-01-13 00:30:16] ERROR: 141 records failed
[2025-01-13 00:30:17] WARNING: P_DP degraded: 0.020 ‚Üí 0.285
[2025-01-13 00:30:18] CRITICAL: Risk threshold exceeded (>60%)
                """, language="log")
        
        with col3:
            if st.button("üìß Notifier √©quipe", use_container_width=True):
                st.info("üìß Email envoy√© √† l'√©quipe data + d√©veloppeurs ETL")
        
        st.markdown("---")
        st.markdown("### üìà Graphique √âvolution Risque")
        
        # Graphique simulation
        dates = pd.date_range(start='2025-01-10', end='2025-01-14', freq='6H')
        risk_before = [46.3] * 10
        risk_spike = [46.3, 48.2, 52.1, 58.7, 62.3, 62.1, 61.8, 46.5, 46.3]
        risk_all = risk_before + risk_spike
        
        fig_surveillance = go.Figure()
        
        fig_surveillance.add_trace(go.Scatter(
            x=dates[:len(risk_all)],
            y=risk_all,
            mode='lines+markers',
            name='Score Risque Paie',
            line=dict(color='#ff4444', width=3),
            marker=dict(size=8)
        ))
        
        # Ligne seuil critique
        fig_surveillance.add_hline(
            y=60,
            line_dash="dash",
            line_color="red",
            annotation_text="Seuil CRITIQUE (60%)"
        )
        
        # Zone incident
        fig_surveillance.add_vrect(
            x0="2025-01-12 23:00",
            x1="2025-01-13 09:00",
            fillcolor="red",
            opacity=0.1,
            annotation_text="Incident ETL",
            annotation_position="top left"
        )
        
        fig_surveillance.update_layout(
            title="Surveillance Continue - Score Risque 'Anciennet√© √ó Paie'",
            xaxis_title="Date",
            yaxis_title="Score Risque (%)",
            height=400,
            template="plotly_dark"
        )
        
        st.plotly_chart(fig_surveillance, use_container_width=True, key="surveillance_chart")
        
        st.markdown("---")
        st.success("D√©tection 9h vs 3 semaines | -79% faux positifs")
    
    # ========================================================================
    # TAB 8 : RESTITUTION ADAPTATIVE
    # ========================================================================
    with tab9:
        st.header("üìã Restitution Adaptative")
        
        st.markdown("""
        ### üéØ Rapport personnalis√© selon TON profil
        
        Que tu sois **CFO**, **Data Engineer**, **DRH**, **Auditeur** ou autre, 
        **Claude g√©n√®re un rapport adapt√©** √† tes besoins sp√©cifiques.
        """)
        
        # Initialiser √©tat session
        if 'restitution_phase' not in st.session_state:
            st.session_state.restitution_phase = "discovery"
        if 'restitution_history' not in st.session_state:
            st.session_state.restitution_history = []
        if 'profil_utilisateur' not in st.session_state:
            st.session_state.profil_utilisateur = None
        if 'rapport_genere' not in st.session_state:
            st.session_state.rapport_genere = None
        
        # R√âCUP√âRER VRAIES DONN√âES DEPUIS L'ANALYSE
        if 'results' not in st.session_state or st.session_state.results is None:
            st.warning("‚ö†Ô∏è Aucune analyse disponible. Lance d'abord une analyse dans l'onglet Dashboard.")
            st.stop()
        
        results = st.session_state.results
        
        # Utiliser les top_priorities qui contient d√©j√† tout calcul√©
        if not results.get('top_priorities') or len(results['top_priorities']) == 0:
            st.warning("‚ö†Ô∏è Aucune priorit√© d√©tect√©e dans l'analyse.")
            st.stop()
        
        # Prendre la priorit√© #1 (plus haut risque)
        top_priority = results['top_priorities'][0]
        
        attribut_critique = top_priority['attribut']
        usage_critique = top_priority['usage']
        risque_pourcent = top_priority['score'] * 100
        impact_records = top_priority['records_affected']
        impact_financier_mois = top_priority['impact_mensuel']
        
        # R√©cup√©rer vecteurs 4D de cet attribut
        vecteurs_bruts = results['vecteurs_4d'].get(attribut_critique, {})
        
        # S√âCURIT√â : Filtrer UNIQUEMENT les 4 dimensions et convertir en float
        vecteurs_critique = {}
        for dim in ['P_DB', 'P_DP', 'P_BR', 'P_UP']:
            val = vecteurs_bruts.get(dim, 0.0)
            try:
                vecteurs_critique[dim] = float(val)
            except (ValueError, TypeError):
                # Si conversion √©choue (ex: 'HIGH'), mettre 0.0
                vecteurs_critique[dim] = 0.0
        
        # Identifier dimension la plus critique (celle avec le plus haut %)
        if vecteurs_critique and any(v > 0 for v in vecteurs_critique.values()):
            dim_max = max(vecteurs_critique.keys(), key=lambda k: vecteurs_critique[k])
        else:
            dim_max = 'P_DB'  # D√©faut si toutes valeurs nulles
        
        # R√©cup√©rer total records
        total_records = len(st.session_state.df) if 'df' in st.session_state else 0
        
        # R√©cup√©rer les pond√©rations pour cet usage
        weights_usage = results['weights'].get(usage_critique, {
            'w_DB': 0.25,
            'w_DP': 0.25,
            'w_BR': 0.25,
            'w_UP': 0.25
        })
        
        # Construire donn√©es incident R√âELLES
        incident_data = {
            "attribut_critique": attribut_critique,
            "usage_critique": usage_critique,
            "risque_pourcent": round(risque_pourcent, 1),
            "severite": top_priority['severite'],
            "impact_records": impact_records,
            "total_records": total_records,
            "impact_financier_mois": impact_financier_mois,
            "vecteurs": vecteurs_critique,
            "dimension_critique": dim_max,
            "pond√©rations": weights_usage,
            "actions_recommandees": top_priority.get('actions', [])
        }
        
        # ========================================================================
        # PHASE 1 : D√âCOUVERTE (Dialogue intelligent)
        # ========================================================================
        if st.session_state.restitution_phase == "discovery":
            
            st.markdown("---")
            st.markdown("### üí¨ Dialogue avec Claude")
            
            # Message initial si historique vide
            if len(st.session_state.restitution_history) == 0:
                initial_msg = f"""**Attribut** : {incident_data['attribut_critique']}
**Risque** : {incident_data['risque_pourcent']}% - {incident_data['severite']}
**Records** : {incident_data['impact_records']}/{incident_data['total_records']}

Ton r√¥le et besoins ?"""
                
                st.session_state.restitution_history.append({
                    "role": "assistant",
                    "content": initial_msg
                })
            
            # Afficher historique dialogue
            for msg in st.session_state.restitution_history:
                if msg["role"] == "assistant":
                    st.info(f"**ü§ñ Claude** : {msg['content']}")
                else:
                    st.success(f"**üë§ Toi** : {msg['content']}")
            
            # Input utilisateur
            user_input = st.text_area(
                "Ta r√©ponse :",
                placeholder="Ex: Je suis CFO, je veux savoir l'impact P&L et si je dois valider le budget de correction...",
                height=100,
                key="restitution_input"
            )
            
            col1, col2 = st.columns([1, 4])
            with col1:
                if st.button("‚û°Ô∏è Envoyer", type="primary", use_container_width=True):
                    if user_input and user_input.strip():
                        # Ajouter √† historique
                        st.session_state.restitution_history.append({
                            "role": "user",
                            "content": user_input
                        })
                        
                        # Appeler Claude pour analyser besoin
                        if 'anthropic_api_key' in st.session_state and st.session_state.anthropic_api_key:
                            with st.spinner("ü§ñ Claude analyse ton besoin..."):
                                try:
                                    client = anthropic.Anthropic(api_key=st.session_state.anthropic_api_key)
                                    
                                    # Prompt analyse besoin
                                    prompt_analyse = f"""Analyse l'√©change avec un utilisateur qui veut un rapport Data Quality personnalis√©.

**HISTORIQUE DIALOGUE** :
{json.dumps(st.session_state.restitution_history, indent=2, ensure_ascii=False)}

**INCIDENT √Ä REPORTER** :
{json.dumps(incident_data, indent=2)}

**D√âTECTE ET RETOURNE JSON** :

{{
  "role": "R√¥le pr√©cis (CFO, Data Engineer, DRH, Auditeur, Manager...)",
  "confiance": 0.0-1.0,
  "besoins_specifiques": [
    "Liste des √©l√©ments sp√©cifiques que l'utilisateur veut voir",
    "Ex: Impact P&L mensuel, Root cause technique, Communication CSE..."
  ],
  "format": "email|rapport|dashboard|presentation|autre",
  "ton": "executif|technique|pedagogique|formel|autre",
  "contraintes": "Deadline, longueur, autres contraintes mentionn√©es",
  "question_manquante": "Question √† poser si confiance < 0.8 (ou null si confiance OK)"
}}

**R√àGLES** :
- Si confiance >= 0.8 : question_manquante = null
- Si confiance < 0.8 : poser UNE question pr√©cise pour clarifier
- Besoins sp√©cifiques = liste concr√®te d'√©l√©ments √† inclure dans le rapport
- Format = type de document attendu
- Ton = style r√©dactionnel adapt√©

R√©ponds UNIQUEMENT avec le JSON, rien d'autre."""

                                    response = client.messages.create(
                                        model="claude-sonnet-4-20250514",
                                        max_tokens=1000,
                                        messages=[{"role": "user", "content": prompt_analyse}]
                                    )
                                    
                                    # Parser JSON
                                    json_text = response.content[0].text.strip()
                                    if "```json" in json_text:
                                        json_text = json_text.split("```json")[1].split("```")[0].strip()
                                    elif "```" in json_text:
                                        json_text = json_text.split("```")[1].split("```")[0].strip()
                                    
                                    profil_detecte = json.loads(json_text)
                                    
                                    # V√©rifier confiance
                                    if profil_detecte.get('confiance', 0) >= 0.8:
                                        # Besoin clarifi√© ‚Üí Passer √† g√©n√©ration
                                        st.session_state.profil_utilisateur = profil_detecte
                                        st.session_state.restitution_phase = "confirmation"
                                        st.rerun()
                                    else:
                                        # Poser question de suivi
                                        question_suivi = profil_detecte.get('question_manquante', 
                                            "Peux-tu pr√©ciser un peu plus ce qui t'int√©resse ?")
                                        st.session_state.restitution_history.append({
                                            "role": "assistant",
                                            "content": question_suivi
                                        })
                                        st.rerun()
                                
                                except Exception as e:
                                    st.error(f"‚ö†Ô∏è Erreur analyse : {str(e)}")
                        else:
                            st.error("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar pour utiliser cette fonctionnalit√©")
                    else:
                        st.warning("‚ö†Ô∏è √âcris quelque chose d'abord !")
            
            with col2:
                if st.button("üîÑ Recommencer", use_container_width=True):
                    st.session_state.restitution_phase = "discovery"
                    st.session_state.restitution_history = []
                    st.session_state.profil_utilisateur = None
                    st.session_state.rapport_genere = None
                    st.rerun()
        
        # ========================================================================
        # PHASE 2 : CONFIRMATION
        # ========================================================================
        elif st.session_state.restitution_phase == "confirmation":
            
            profil = st.session_state.profil_utilisateur
            
            st.markdown("---")
            st.success(f"‚úÖ **Profil d√©tect√©** : {profil.get('role', 'Non d√©fini')}")
            st.info(f"üéØ **Confiance** : {int(profil.get('confiance', 0) * 100)}%")
            
            st.markdown("### üìã Contenu du rapport personnalis√©")
            
            col1, col2 = st.columns([1, 1])
            
            with col1:
                st.markdown("**üìä Base commune (tous profils)** :")
                st.markdown(f"""
                - **Incident** : {incident_data['attribut_critique']}
                - **Risque** : {incident_data['risque_pourcent']}% üö® CRITIQUE
                - **Records affect√©s** : {incident_data['impact_records']}
                - **Impact** : {incident_data['impact_financier_mois']:,}‚Ç¨/mois
                """)
            
            with col2:
                st.markdown(f"**‚ú® Sp√©cifique {profil.get('role')}** :")
                for item in profil.get('besoins_specifiques', []):
                    st.markdown(f"- {item}")
            
            st.markdown("---")
            
            # Afficher format et ton
            col_f, col_t = st.columns(2)
            with col_f:
                st.metric("Format", profil.get('format', 'Rapport').capitalize())
            with col_t:
                st.metric("Ton", profil.get('ton', 'Professionnel').capitalize())
            
            if profil.get('contraintes'):
                st.warning(f"‚ö†Ô∏è **Contraintes** : {profil['contraintes']}")
            
            st.markdown("---")
            
            # Boutons action
            col1, col2 = st.columns([2, 1])
            
            with col1:
                if st.button("üöÄ G√©n√©rer Rapport Personnalis√©", type="primary", use_container_width=True):
                    if 'anthropic_api_key' in st.session_state and st.session_state.anthropic_api_key:
                        with st.spinner("ü§ñ Claude g√©n√®re ton rapport sur-mesure... (10-20 sec)"):
                            try:
                                client = anthropic.Anthropic(api_key=st.session_state.anthropic_api_key)
                                
                                # Prompt g√©n√©ration rapport avec structure graphique
                                prompt_rapport = f"""Tu es un expert Data Quality qui g√©n√®re un rapport personnalis√© **en fran√ßais simple et accessible**.

**PROFIL UTILISATEUR** :
- R√¥le : {profil['role']}
- Besoins sp√©cifiques : {', '.join(profil['besoins_specifiques'])}
- Format : {profil['format']}
- Ton : {profil['ton']}
- Contraintes : {profil.get('contraintes', 'Aucune')}

**DONN√âES R√âELLES √Ä UTILISER** :
{json.dumps(incident_data, indent=2, ensure_ascii=False)}

**R√àGLES CRITIQUES** :
‚ö†Ô∏è FRAN√áAIS SIMPLE (niveau lyc√©en)
‚ö†Ô∏è Explications AVANT chiffres
‚ö†Ô∏è VRAIES donn√©es uniquement (pas de simulation)
‚ö†Ô∏è PAS de timeline dans le rapport
‚ö†Ô∏è Identifier la cause racine depuis les donn√©es (pas d'invention)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
STRUCTURE OBLIGATOIRE DU RAPPORT
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# RAPPORT QUALIT√â DONN√âES
Incident : {incident_data['attribut_critique']} - {incident_data['risque_pourcent']}%
Date : [Date du jour]
Destinataire : {profil['role']}

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  PARTIE 1 : SYNTH√àSE EX√âCUTIVE                        ‚ïë
‚ïë  Pour d√©cideurs press√©s - Lecture : 2 minutes         ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

## üö® SITUATION EN UN COUP D'≈íIL

[GRAPHIQUE 1 : Jauge de risque - ASCII art]
G√©n√®re une jauge visuelle ASCII avec {incident_data['risque_pourcent']}%

Exemple :
```
NIVEAU DE RISQUE
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 62.3%

üü¢‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄüü°‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄüî¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ö´
OK     Surveill√©  Critique  Bloquant
                    ‚Üë
                  VOUS √äTES ICI
```

## üí° L'ESSENTIEL EN 3 POINTS

1Ô∏è‚É£ **QU'EST-CE QUI S'EST PASS√â ?**
[Explique l'incident en 2-3 phrases simples avec les vraies donn√©es]
- Bug identifi√©
- {incident_data['impact_records']} enregistrements affect√©s

2Ô∏è‚É£ **QUEL IMPACT ?**
[Explique l'impact avec les vraies donn√©es]
- {incident_data['vecteurs']['P_DP']*100:.0f}% des transformations d√©faillantes
- {incident_data['impact_financier_mois']:,}‚Ç¨/mois si non corrig√©
- Process bloqu√©s

3Ô∏è‚É£ **QUE FAIRE MAINTENANT ?**
[Action imm√©diate claire]
- Action : [D√©cris l'action prioritaire]
- Dur√©e : [Temps estim√©]
- Gain : √âvite pertes financi√®res

## üìä QUALIT√â PAR DIMENSION

[GRAPHIQUE 2 : Radar 4D - ASCII art]
G√©n√®re un radar avec les vraies valeurs :
- DB : {incident_data['vecteurs']['P_DB']*100:.1f}%
- DP : {incident_data['vecteurs']['P_DP']*100:.1f}%
- BR : {incident_data['vecteurs']['P_BR']*100:.1f}%
- UP : {incident_data['vecteurs']['P_UP']*100:.1f}%

Exemple format :
```
        Traitements
           XX% üî¥
             ‚ñ≤
             ‚îÇ
Structure ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ R√®gles
  XX%        ‚îÇ     XX%
             ‚îÇ
             ‚ñº
        Utilisabilit√©
           XX%
```

[GRAPHIQUE 3 : Barres horizontales]
G√©n√®re barres pour chaque dimension avec % de qualit√© (100 - P_dimension)

Exemple :
```
Structure (Base de donn√©es)
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë XX.X% ‚úÖ BON NIVEAU
[Explication 1 phrase]

Traitements (Transformations)  
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë XX.X% üö® ACTION URGENTE
[Explication 1 phrase]

R√®gles M√©tier (Coh√©rence)
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë XX.X% ‚úÖ EXCELLENT
[Explication 1 phrase]

Utilisabilit√© (Exploitabilit√©)
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë XX.X% ‚úÖ BON NIVEAU
[Explication 1 phrase]
```

üéØ DIMENSION CRITIQUE : [Identifier dimension avec plus haut risque]

## üí∞ IMPACT FINANCIER

[GRAPHIQUE 4 : Comparaison co√ªts]
G√©n√®re comparaison visuelle ASCII :

```
Si on ne fait rien :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  XX,XXX ‚Ç¨ / mois     ‚îÇ ‚Üê Pertes continues
‚îÇ  XXX,XXX ‚Ç¨ / an      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Solution propos√©e :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ X,XXX ‚Ç¨  ‚îÇ ‚Üê Co√ªt unique
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üí° ROI : √âconomie de XXX√ó le co√ªt
```

## ‚úÖ PLAN D'ACTION (3 PRIORIT√âS)

G√©n√®re tableau structur√© :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Priorit√©  ‚îÇ Dur√©e    ‚îÇ Impact       ‚îÇ Gain            ‚îÇ Responsable  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ üî• #1     ‚îÇ XX       ‚îÇ [Impact]     ‚îÇ [Gain ‚Ç¨]        ‚îÇ [√âquipe]     ‚îÇ
‚îÇ [Action]  ‚îÇ URGENT   ‚îÇ              ‚îÇ                 ‚îÇ              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚öôÔ∏è #2     ‚îÇ XX       ‚îÇ [Impact]     ‚îÇ [Gain]          ‚îÇ [√âquipe]     ‚îÇ
‚îÇ [Action]  ‚îÇ          ‚îÇ              ‚îÇ                 ‚îÇ              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ üõ°Ô∏è #3     ‚îÇ XX       ‚îÇ [Impact]     ‚îÇ [Gain]          ‚îÇ [√âquipe]     ‚îÇ
‚îÇ [Action]  ‚îÇ          ‚îÇ              ‚îÇ                 ‚îÇ              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Puis d√©taille chaque action :

**üî• ACTION 1 - CRITIQUE IMM√âDIATE**
Ce qu'il faut faire : [Description simple]
Temps n√©cessaire : [Dur√©e]
Impact : [Impact concret]
Gain : [Gain chiffr√© ou qualitatif]
Responsable : [√âquipe]

[Idem pour actions 2 et 3]

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
   FIN SYNTH√àSE EX√âCUTIVE - Lecture : 2 minutes
   üëá Suite = D√©tails techniques (optionnel)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  PARTIE 2 : SOCLE DE BASE D√âTAILL√â                   ‚ïë
‚ïë  (Commun √† tous les profils)                          ‚ïë
‚ïë  Lecture : 5 minutes                                  ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

## üìä SECTION 1 : PROBL√àMES QUALIT√â D√âTECT√âS

Pour chaque dimension, **TEXTE D'ABORD**, chiffres apr√®s.

**üóÑÔ∏è BASE DE DONN√âES (Structure)**

[Explication en fran√ßais simple - 2-3 phrases]
"[D√©cris ce qui se passe avec les vraies donn√©es]"

Chiffres techniques : {incident_data['vecteurs']['P_DB']*100:.1f}% de probl√®mes (marge ¬±3%)

**‚öôÔ∏è TRANSFORMATIONS INFORMATIQUES (Traitements)**

[Si dimension critique, ajouter üö® ALERTE MAJEURE]
[Explication en fran√ßais simple - 2-3 phrases]
"[D√©cris avec les vraies donn√©es]"

Chiffres techniques : {incident_data['vecteurs']['P_DP']*100:.1f}% de probl√®mes (marge ¬±3%)

[Optionnel : Sch√©ma propagation si pertinent]

**üìã R√àGLES M√âTIER (Coh√©rence)**

[Explication en fran√ßais simple]
"[D√©cris avec les vraies donn√©es]"

Chiffres techniques : {incident_data['vecteurs']['P_BR']*100:.1f}% de probl√®mes (marge ¬±3%)

**üéØ AD√âQUATION √Ä L'USAGE (Utilisabilit√©)**

[Explication en fran√ßais simple]
"[D√©cris avec les vraies donn√©es]"

Chiffres techniques : {incident_data['vecteurs']['P_UP']*100:.1f}% de probl√®mes (marge ¬±4%)

---

## ‚öñÔ∏è SECTION 2 : IMPORTANCE DES DIMENSIONS

Usage analys√© : [Nom usage m√©tier]

[GRAPHIQUE 5 : Camembert pond√©rations - ASCII art]
G√©n√®re camembert avec vraies pond√©rations (√† d√©duire du contexte ou demander)

Exemple :
```
       XX%
   Traitements
   ‚ï±        ‚ï≤
  ‚ï±          ‚ï≤
XX%            XX%
Structure   R√®gles
  ‚ï≤          ‚ï±
   ‚ï≤        ‚ï±
     XX%
Utilisabilit√©
```

**Pour cet usage, voici l'importance de chaque dimension :**

‚Ä¢ **STRUCTURE (XX% d'importance)**
  [Explique pourquoi ce % avec usage m√©tier]

‚Ä¢ **TRAITEMENTS (XX% d'importance)**
  [Explique pourquoi - identifier si dimension critique]

‚Ä¢ **R√àGLES M√âTIER (XX% d'importance)**
  [Explique pourquoi]

‚Ä¢ **UTILISABILIT√â (XX% d'importance)**
  [Explique pourquoi]

**Justification :** [2-3 phrases expliquant la logique des pond√©rations]

---

## üéØ SECTION 3 : PLAN D'ACTION D√âTAILL√â

[Reprendre le tableau de la synth√®se, puis d√©tailler]

**üî• ACTION 1 - CRITIQUE IMM√âDIATE**
[D√©tails comme dans synth√®se]

**‚öôÔ∏è ACTION 2 - CORRECTIF TECHNIQUE**
[D√©tails]

**üõ°Ô∏è ACTION 3 - PR√âVENTION LONG TERME**
[D√©tails]

---

## üìà SECTION 4 : QUALIT√â PAR DIMENSION (SANS AGR√âGATION)

‚ö†Ô∏è **ATTENTION** : Chaque dimension a son importance propre
‚ùå **PAS DE MOYENNE GLOBALE** calcul√©e

[G√©n√®re 4 cartes visuelles s√©par√©es]

**üóÑÔ∏è STRUCTURE BASE DE DONN√âES**
```
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë XX.X%  ‚úÖ
```
[Explication accessible 2-3 phrases]
Chiffres : XX.X% de qualit√© (marge ¬±3%)

**‚öôÔ∏è TRAITEMENTS INFORMATIQUES**
```
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë XX.X%  üö®
```
[Si critique : üî¥ ZONE ROUGE : ...]
[Explication + ACTION IMM√âDIATE REQUISE si pertinent]
Chiffres : XX.X% de qualit√© (marge ¬±3%)

**üìã R√àGLES M√âTIER**
```
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë XX.X%  ‚úÖ
```
[Explication accessible]
Chiffres : XX.X% de qualit√© (marge ¬±3%)

**üéØ UTILISABILIT√â**
```
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë XX.X%  ‚úÖ
```
[Explication accessible]
Chiffres : XX.X% de qualit√© (marge ¬±4%)

üéØ **DIMENSION N√âCESSITANT ACTION IMM√âDIATE** : [Identifier dimension critique]

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  PARTIE 3 : √âL√âMENTS SP√âCIFIQUES PROFIL              ‚ïë
‚ïë  "{profil['role']}"                                   ‚ïë
‚ïë  Lecture : 3 minutes                                  ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    ‚Üì SECTION PERSONNALIS√âE POUR TON PROFIL ‚Üì
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

**Besoins sp√©cifiques √† traiter :**
{chr(10).join(['- ' + b for b in profil['besoins_specifiques']])}

**Instructions partie sp√©cifique :**
- Fran√ßais simple et accessible (comme parties 1 et 2)
- Adapter au profil : {profil['role']}
- Utiliser vraies donn√©es incident
- Cr√©er sections pertinentes selon besoins
- Garder ton p√©dagogique

Exemples de sections possibles selon profil :
- Si process/op√©rationnel : Vue domaines, Process d√©faillants, M√©triques suivi
- Si CFO/financier : Analyse ROI, Business case, Recommandation Go/NoGo
- Si technique : Root cause d√©taill√©e, Code d√©faillant, Scripts correction
- Si DRH/communication : Impact collaborateurs, Communication √©quipe, Timeline humaine

[G√©n√®re 3-5 sections pertinentes avec vraies donn√©es]

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

**CONSIGNES FINALES** :
‚úÖ Structure EXACTE ci-dessus
‚úÖ Fran√ßais niveau lyc√©en
‚úÖ Graphiques ASCII art
‚úÖ Explications AVANT chiffres
‚úÖ Vraies donn√©es uniquement
‚úÖ PAS de timeline
‚úÖ PAS de moyenne globale
‚úÖ Ton : {profil['ton']}
‚úÖ Longueur : 1000-1500 mots

**G√âN√àRE LE RAPPORT MAINTENANT** en Markdown."""

                                response = client.messages.create(
                                    model="claude-sonnet-4-20250514",
                                    max_tokens=4000,
                                    messages=[{"role": "user", "content": prompt_rapport}]
                                )
                                
                                rapport = response.content[0].text
                                
                                st.session_state.rapport_genere = rapport
                                st.session_state.restitution_phase = "export"
                                st.rerun()
                            
                            except Exception as e:
                                st.error(f"‚ö†Ô∏è Erreur g√©n√©ration : {str(e)}")
                    else:
                        st.error("‚ö†Ô∏è Configure ta cl√© API Claude dans la sidebar")
            
            with col2:
                if st.button("‚Ü©Ô∏è Modifier profil", use_container_width=True):
                    st.session_state.restitution_phase = "discovery"
                    st.rerun()
        
        # ========================================================================
        # PHASE 3 : EXPORT
        # ========================================================================
        elif st.session_state.restitution_phase == "export":
            
            profil = st.session_state.profil_utilisateur
            rapport = st.session_state.rapport_genere
            
            st.markdown("---")
            st.success(f"‚úÖ **Rapport g√©n√©r√© pour** : {profil.get('role')}")
            
            st.markdown("### üìÑ Ton Rapport Personnalis√©")
            
            # Afficher rapport
            st.markdown(rapport)
            
            st.markdown("---")
            st.markdown("### üì• T√©l√©charger")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                # Export Markdown
                md_bytes = rapport.encode('utf-8')
                st.download_button(
                    "üìù Markdown",
                    md_bytes,
                    f"rapport_{profil.get('role', 'custom').replace(' ', '_')}.md",
                    "text/markdown",
                    use_container_width=True
                )
            
            with col2:
                # Export TXT simple
                txt_bytes = rapport.encode('utf-8')
                st.download_button(
                    "üìÑ Texte",
                    txt_bytes,
                    f"rapport_{profil.get('role', 'custom').replace(' ', '_')}.txt",
                    "text/plain",
                    use_container_width=True
                )
            
            with col3:
                if st.button("üîÑ Nouveau Rapport", use_container_width=True):
                    st.session_state.restitution_phase = "discovery"
                    st.session_state.restitution_history = []
                    st.session_state.profil_utilisateur = None
                    st.session_state.rapport_genere = None
                    st.rerun()
            
            st.markdown("---")
    
    # ========================================================================
    # TAB 9 : D√âTECTION ANOMALIES
    # ========================================================================
else:
    # MESSAGE ACCUEIL
    st.info("üëà **Upload TON dataset CSV/Excel dans la barre lat√©rale pour commencer l'analyse !**")
    
    
    # Supprimer vid√©o placeholder
    # st.video("https://www.youtube.com/watch?v=dQw4w9WgXcQ")
