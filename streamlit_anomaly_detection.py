"""
streamlit_anomaly_detection.py
Interface Streamlit compl√®te pour d√©tection anomalies

Features:
- Scan dataset avec d√©tections r√©elles
- Gestion r√©f√©rentiel (voir, ajouter anomalies)
- Stats apprentissage temps r√©el
- Historique scans
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime

from extended_anomaly_catalog import ExtendedCatalogManager, CoreAnomaly, Dimension, Criticality
from adaptive_scan_engine import AdaptiveScanEngine
from rules_catalog_loader import catalog as _catalog


def render_anomaly_detection_tab():
    """Onglet complet d√©tection anomalies"""
    
    st.header("üîç D√©tection Anomalies Adaptative")
    
    # Initialiser engine en session state
    if 'adaptive_engine' not in st.session_state:
        st.session_state.adaptive_engine = AdaptiveScanEngine()
    
    engine = st.session_state.adaptive_engine
    
    # Tabs internes
    sub_tab1, sub_tab2, sub_tab3, sub_tab4 = st.tabs([
        "üöÄ Scanner Dataset",
        "üìö R√©f√©rentiel",
        "üìà Apprentissage",
        "üìú Historique"
    ])
    
    # ========================================================================
    # TAB 1 : SCANNER DATASET
    # ========================================================================
    
    with sub_tab1:
        st.subheader("Scanner Dataset")
        
        st.info(f"""
        ‚úÖ **15 d√©tecteurs r√©els** op√©rationnels ({len(engine.catalog_manager.catalog)} catalogu√©s)
        üß† **Apprentissage adaptatif** : Le moteur s'am√©liore √† chaque scan
        ‚ö° **3 budgets** : QUICK (top 5) | STANDARD (top 10) | DEEP (tous)
        """)
        
        # V√©rifier si dataset d√©j√† charg√© dans session_state
        if 'df' in st.session_state and st.session_state.df is not None:
            df = st.session_state.df
            
            st.success(f"‚úÖ Dataset charg√© : {len(df):,} lignes √ó {len(df.columns)} colonnes")
            
            # Aper√ßu donn√©es
            with st.expander("üëÅÔ∏è Aper√ßu donn√©es"):
                st.dataframe(df.head(10), use_container_width=True)
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Lignes", f"{len(df):,}")
                with col2:
                    st.metric("Colonnes", len(df.columns))
                with col3:
                    st.metric("Taille m√©moire", f"{df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
            
            # Configuration scan
            st.markdown("---")
            st.subheader("‚öôÔ∏è Configuration Scan")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                budget = st.selectbox(
                    "Budget",
                    options=["QUICK", "STANDARD", "DEEP"],
                    index=1,
                    help="QUICK=Top 5 | STANDARD=Top 10 | DEEP=Tous"
                )
            
            with col2:
                learn = st.checkbox(
                    "Activer apprentissage",
                    value=True,
                    help="Mettre √† jour stats fr√©quence apr√®s scan"
                )
            
            with col3:
                # Utiliser nom de session_state ou d√©faut
                default_name = st.session_state.get('dataset_name', 'dataset')
                dataset_name = st.text_input(
                    "Nom dataset",
                    value=default_name,
                    help="Nom pour identifier ce scan"
                )
            
            # Lancer scan
            if st.button("üöÄ Lancer Scan", type="primary", use_container_width=True):
                try:
                    with st.spinner("üîç Scan en cours..."):
                        report = engine.scan_dataset(
                            df,
                            dataset_name,
                            budget=budget,
                            learn=learn,
                            verbose=False
                        )
                    
                    # R√©sultats
                    st.success(f"‚úÖ Scan termin√© en {report.total_execution_time_s:.2f}s")
                    
                    # M√©triques principales
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric(
                            "Anomalies scann√©es",
                            report.anomalies_scanned,
                            help="Nombre d'anomalies v√©rifi√©es"
                        )
                    
                    with col2:
                        st.metric(
                            "D√©tections",
                            report.anomalies_detected,
                            delta=f"{report.anomalies_detected/report.anomalies_scanned:.0%}" if report.anomalies_scanned > 0 else "0%",
                            delta_color="inverse"
                        )
                    
                    with col3:
                        st.metric(
                            "Lignes affect√©es",
                            sum(r.affected_rows for r in report.results if r.detected),
                            help="Total lignes avec anomalies"
                        )
                    
                    with col4:
                        st.metric(
                            "Temps moyen/anomalie",
                            f"{report.total_execution_time_s*1000/report.anomalies_scanned:.1f}ms" if report.anomalies_scanned > 0 else "0ms"
                        )
                    
                    # Graphique r√©partition
                    st.markdown("---")
                    st.subheader("üìä R√©partition D√©tections")
                    
                    detected_by_dim = report._get_detected_by_dim()
                    
                    if detected_by_dim:
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            fig = px.pie(
                                values=list(detected_by_dim.values()),
                                names=list(detected_by_dim.keys()),
                                title="D√©tections par Dimension"
                            )
                            st.plotly_chart(fig, use_container_width=True)
                        
                        with col2:
                            # Timeline ex√©cution
                            timeline_data = []
                            for r in report.results:
                                timeline_data.append({
                                    'Anomalie': r.anomaly_id,
                                    'Temps (ms)': r.execution_time_ms,
                                    'D√©tect√©': '‚úÖ Oui' if r.detected else '‚ö™ Non'
                                })
                            
                            timeline_df = pd.DataFrame(timeline_data)
                            fig = px.bar(
                                timeline_df,
                                x='Anomalie',
                                y='Temps (ms)',
                                color='D√©tect√©',
                                title="Temps d'ex√©cution par anomalie"
                            )
                            st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info("‚ÑπÔ∏è Aucune anomalie d√©tect√©e")
                    
                    # Tableau d√©taill√© anomalies
                    st.markdown("---")
                    st.subheader("üìã D√©tails Anomalies")
                    
                    tab_all, tab_detected, tab_clean = st.tabs([
                        f"Toutes ({len(report.results)})",
                        f"D√©tect√©es ({report.anomalies_detected})",
                        f"Clean ({len(report.results) - report.anomalies_detected})"
                    ])
                    
                    with tab_all:
                        all_data = []
                        for r in report.results:
                            all_data.append({
                                'ID': r.anomaly_id,
                                'Anomalie': r.anomaly_name,
                                'Statut': '‚úÖ D√©tect√©e' if r.detected else '‚ö™ OK',
                                'Lignes affect√©es': r.affected_rows if r.detected else 0,
                                'Temps (ms)': f"{r.execution_time_ms:.1f}"
                            })
                        st.dataframe(pd.DataFrame(all_data), use_container_width=True, hide_index=True)
                    
                    with tab_detected:
                        detected_data = []
                        for r in report.results:
                            if r.detected:
                                detected_data.append({
                                    'ID': r.anomaly_id,
                                    'Anomalie': r.anomaly_name,
                                    'Lignes affect√©es': r.affected_rows,
                                    '√âchantillon': len(r.sample_data)
                                })
                        
                        if detected_data:
                            st.dataframe(pd.DataFrame(detected_data), use_container_width=True, hide_index=True)
                            
                            # Afficher √©chantillons
                            st.markdown("**üìå √âchantillons lignes affect√©es**")
                            for r in report.results:
                                if r.detected and r.sample_data:
                                    with st.expander(f"{r.anomaly_id}: {r.anomaly_name} ({r.affected_rows} lignes)"):
                                        sample_df = pd.DataFrame(r.sample_data)
                                        st.dataframe(sample_df, use_container_width=True)
                                        
                                        # D√©tails techniques
                                        if r.details:
                                            st.json(r.details)
                        else:
                            st.success("‚úÖ Aucune anomalie d√©tect√©e - Dataset clean !")
                    
                    with tab_clean:
                        clean_data = []
                        for r in report.results:
                            if not r.detected:
                                clean_data.append({
                                    'ID': r.anomaly_id,
                                    'Anomalie': r.anomaly_name,
                                    'Temps (ms)': f"{r.execution_time_ms:.1f}"
                                })
                        st.dataframe(pd.DataFrame(clean_data), use_container_width=True, hide_index=True)
                
                except Exception as e:
                    st.error(f"‚ùå Erreur scan dataset : {e}")
        
        else:
            st.warning("‚ö†Ô∏è Aucun dataset charg√©. Veuillez d'abord charger un dataset dans la barre lat√©rale.")
            st.info("üëà Utilisez la barre lat√©rale pour charger un fichier CSV/Excel")
    
    # ========================================================================
    # TAB 2 : R√âF√âRENTIEL
    # ========================================================================
    
    with sub_tab2:
        st.subheader("üìö R√©f√©rentiel Anomalies")
        
        catalog_manager = engine.catalog_manager
        
        # Stats catalogue
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            st.metric("Total Anomalies", len(catalog_manager.catalog))
        
        with col2:
            db_count = len(catalog_manager.get_by_dimension('DB'))
            st.metric("DB", db_count)
        
        with col3:
            dp_count = len(catalog_manager.get_by_dimension('DP'))
            st.metric("DP", dp_count)
        
        with col4:
            br_count = len(catalog_manager.get_by_dimension('BR'))
            st.metric("BR", br_count)
        
        with col5:
            up_count = len(catalog_manager.get_by_dimension('UP'))
            st.metric("UP", up_count)
        
        # Visualisation catalogue
        st.markdown("---")
        st.markdown("#### üìä Vue Catalogue")
        
        catalog_data = []
        for a in catalog_manager.catalog:
            catalog_data.append({
                'ID': a.id,
                'Nom': a.name,
                'Dimension': a.dimension.value,
                'Criticit√©': a.criticality.name,
                'Woodall': a.woodall_level,
                'Scans': a.scan_count,
                'D√©tections': a.detection_count,
                'Fr√©quence': f"{a.frequency:.1%}" if a.scan_count > 0 else "N/A",
                'Score Priorit√©': f"{a.get_priority_score():.1f}"
            })
        
        catalog_df = pd.DataFrame(catalog_data)
        
        # Filtres
        col1, col2 = st.columns(2)
        
        with col1:
            filter_dim = st.multiselect(
                "Filtrer par dimension",
                options=['DB', 'DP', 'BR', 'UP'],
                default=['DB', 'DP', 'BR', 'UP']
            )
        
        with col2:
            filter_crit = st.multiselect(
                "Filtrer par criticit√©",
                options=['CRITIQUE', '√âLEV√â', 'MOYEN', 'FAIBLE'],
                default=['CRITIQUE', '√âLEV√â', 'MOYEN', 'FAIBLE']
            )
        
        # Appliquer filtres
        filtered_df = catalog_df[
            catalog_df['Dimension'].isin(filter_dim) &
            catalog_df['Criticit√©'].isin(filter_crit)
        ]
        
        st.dataframe(
            filtered_df.sort_values('Score Priorit√©', ascending=False),
            use_container_width=True,
            hide_index=True
        )
        
        # D√©tails anomalie
        st.markdown("---")
        st.markdown("#### üîç D√©tails Anomalie")
        
        selected_id = st.selectbox(
            "S√©lectionner anomalie",
            options=[a.id for a in catalog_manager.catalog],
            format_func=lambda x: f"{x}: {catalog_manager.get_by_id(x).name}"
        )
        
        if selected_id:
            anomaly = catalog_manager.get_by_id(selected_id)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown(f"**Nom** : {anomaly.name}")
                st.markdown(f"**Description** : {anomaly.description}")
                st.markdown(f"**Dimension** : {anomaly.dimension.value}")
                st.markdown(f"**Criticit√©** : {anomaly.criticality.name}")
            
            with col2:
                st.markdown(f"**Woodall** : {anomaly.woodall_level}")
                st.markdown(f"**SQL Template** :")
                st.code(anomaly.sql_template, language='sql')
            
            st.markdown(f"**Exemple** : {anomaly.example}")
            
            # Stats apprentissage
            if anomaly.scan_count > 0:
                st.markdown("**üìà Stats Apprentissage** :")
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("Scans", anomaly.scan_count)
                with col2:
                    st.metric("D√©tections", anomaly.detection_count)
                with col3:
                    st.metric("Fr√©quence", f"{anomaly.frequency:.1%}")
                with col4:
                    st.metric("Score Priorit√©", f"{anomaly.get_priority_score():.1f}")
        
        # Ajouter anomalies par CSV
        st.markdown("---")
        st.markdown("#### ‚ûï Ajouter Anomalies (import CSV)")

        with st.expander("Importer un CSV d'anomalies"):
            st.markdown("""
            **Colonnes obligatoires** : `anomaly_id`, `name`, `description`, `dimension`, `detection`, `criticality`
            **Colonnes optionnelles** : `woodall`, `algorithm`, `business_risk`, `frequency`, `default_rule_type`
            """)

            csv_template = _catalog.generate_csv_template()
            st.download_button(
                label="üìÑ T√©l√©charger le template CSV",
                data=csv_template,
                file_name="anomalies_template.csv",
                mime="text/csv",
                key="scan_csv_template",
            )

            csv_file = st.file_uploader(
                "üìÅ Charger un CSV d'anomalies",
                type=["csv"],
                key="scan_anomaly_csv_upload",
            )

            if csv_file is not None:
                try:
                    import_df = pd.read_csv(csv_file, dtype=str).fillna("")
                    st.markdown(f"**Aper√ßu** ‚Äî {len(import_df)} anomalies trouv√©es :")
                    st.dataframe(import_df, use_container_width=True, hide_index=True)

                    errors = _catalog.validate_import_df(import_df)
                    if errors:
                        for err in errors:
                            st.error(f"‚ùå {err}")
                    else:
                        existing = set(_catalog.anomalies.keys())
                        new_ids = set(import_df["anomaly_id"].str.strip()) - existing
                        update_ids = set(import_df["anomaly_id"].str.strip()) & existing

                        if new_ids:
                            st.info(f"üÜï {len(new_ids)} nouvelles anomalies : {', '.join(sorted(new_ids))}")
                        if update_ids:
                            st.warning(f"‚ôªÔ∏è {len(update_ids)} anomalies d√©j√† existantes : {', '.join(sorted(update_ids))}")

                        col_a, col_b = st.columns(2)
                        with col_a:
                            overwrite = st.checkbox("√âcraser les anomalies existantes", value=False, key="scan_overwrite")
                        with col_b:
                            if st.button("‚úÖ Importer dans le catalogue", type="primary", key="scan_import_btn"):
                                result = _catalog.import_from_dataframe(import_df, overwrite=overwrite)
                                if result["errors"]:
                                    for err in result["errors"]:
                                        st.error(f"‚ùå {err}")
                                else:
                                    msg_parts = []
                                    if result["added"] > 0:
                                        msg_parts.append(f"üÜï {result['added']} ajout√©es")
                                    if result["updated"] > 0:
                                        msg_parts.append(f"‚ôªÔ∏è {result['updated']} mises √† jour")
                                    if result["skipped"] > 0:
                                        msg_parts.append(f"‚è≠Ô∏è {result['skipped']} ignor√©es (d√©j√† existantes)")
                                    st.success(f"‚úÖ Import r√©ussi ‚Äî {' ¬∑ '.join(msg_parts)}")
                                    st.info(f"üìä Le r√©f√©rentiel contient maintenant **{len(_catalog.anomalies)} anomalies**")
                                    # Recharger le catalogue dans le moteur
                                    engine.catalog_manager = ExtendedCatalogManager()
                                    st.rerun()

                except Exception as e:
                    st.error(f"‚ùå Erreur de lecture CSV : {e}")
    
    # ========================================================================
    # TAB 3 : APPRENTISSAGE
    # ========================================================================
    
    with sub_tab3:
        st.subheader("üìà Apprentissage Adaptatif")
        
        st.info("""
        üß† Le moteur **apprend** des scans pass√©s pour **optimiser** les suivants :
        - Fr√©quence : Quelles anomalies sont souvent d√©tect√©es ?
        - Priorisation : Score = Fr√©quence √ó Impact / Complexit√©
        - Adaptation : Budget QUICK cible les top anomalies
        """)
        
        # Stats apprentissage
        stats_df = engine.get_learning_stats()
        
        if not stats_df.empty:
            st.success(f"‚úÖ {len(stats_df)} anomalies avec historique")
            
            # M√©triques globales
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                total_scans = stats_df['Scans'].astype(int).sum()
                st.metric("Total Scans", total_scans)
            
            with col2:
                total_detections = stats_df['D√©tections'].astype(int).sum()
                st.metric("Total D√©tections", total_detections)
            
            with col3:
                # Filtrer les N/A avant conversion
                freq_series = stats_df['Fr√©quence'].str.rstrip('%')
                freq_series = pd.to_numeric(freq_series, errors='coerce')
                avg_freq = freq_series.mean() / 100 if not freq_series.isna().all() else 0
                st.metric("Fr√©quence moyenne", f"{avg_freq:.1%}")
            
            with col4:
                avg_score = stats_df['_score_numeric'].mean()
                st.metric("Score moyen", f"{avg_score:.1f}")
            
            # Tableau stats
            st.markdown("---")
            st.markdown("#### üìä Statistiques par Anomalie")
            
            # Afficher sans la colonne cach√©e
            display_df = stats_df.drop(columns=['_score_numeric'])
            st.dataframe(display_df, use_container_width=True, hide_index=True)
            
            # Graphiques
            st.markdown("---")
            st.markdown("#### üìà Visualisations")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Top 10 fr√©quences
                top_freq = stats_df.nlargest(10, '_score_numeric')
                fig = px.bar(
                    top_freq,
                    x='ID',
                    y='_score_numeric',
                    color='Dimension',
                    title="Top 10 Score Priorit√©",
                    labels={'_score_numeric': 'Score Priorit√©'}
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # R√©partition fr√©quences par dimension
                def safe_freq_mean(x):
                    """Calcule moyenne fr√©quence en g√©rant N/A"""
                    cleaned = x.str.rstrip('%')
                    numeric = pd.to_numeric(cleaned, errors='coerce')
                    return numeric.mean() if not numeric.isna().all() else 0
                
                freq_by_dim = stats_df.groupby('Dimension')['Fr√©quence'].apply(safe_freq_mean).reset_index()
                freq_by_dim.columns = ['Dimension', 'Fr√©quence Moyenne']
                
                fig = px.pie(
                    freq_by_dim,
                    values='Fr√©quence Moyenne',
                    names='Dimension',
                    title="Fr√©quence Moyenne par Dimension"
                )
                st.plotly_chart(fig, use_container_width=True)
            
            # Export stats
            st.markdown("---")
            if st.button("üì• T√©l√©charger Stats CSV"):
                csv = stats_df.to_csv(index=False)
                st.download_button(
                    label="‚¨áÔ∏è stats_apprentissage.csv",
                    data=csv,
                    file_name=f"stats_apprentissage_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                    mime="text/csv"
                )
        
        else:
            st.info("‚ÑπÔ∏è Aucun historique - Lancez un premier scan pour commencer l'apprentissage")
    
    # ========================================================================
    # TAB 4 : HISTORIQUE
    # ========================================================================
    
    with sub_tab4:
        st.subheader("üìú Historique Scans")
        
        history_df = engine.get_scan_history_summary()
        
        if not history_df.empty:
            st.success(f"‚úÖ {len(history_df)} scans dans l'historique")
            
            # Tableau historique
            st.dataframe(history_df, use_container_width=True, hide_index=True)
            
            # √âvolution d√©tections
            st.markdown("---")
            st.markdown("#### üìà √âvolution D√©tections")
            
            fig = go.Figure()
            
            fig.add_trace(go.Scatter(
                x=history_df['Date'],
                y=history_df['D√©tect√©es'].astype(int),
                mode='lines+markers',
                name='D√©tections',
                line=dict(color='red', width=2)
            ))
            
            fig.add_trace(go.Scatter(
                x=history_df['Date'],
                y=history_df['Scann√©es'].astype(int),
                mode='lines+markers',
                name='Scann√©es',
                line=dict(color='blue', width=2, dash='dash')
            ))
            
            fig.update_layout(
                title="√âvolution D√©tections vs Scann√©es",
                xaxis_title="Date",
                yaxis_title="Nombre Anomalies",
                hovermode='x unified'
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        else:
            st.info("‚ÑπÔ∏è Aucun scan dans l'historique")


# ============================================================================
# INT√âGRATION DANS APP.PY
# ============================================================================

"""
Pour int√©grer dans app.py :

1. Ajouter import en haut :
   from streamlit_anomaly_detection import render_anomaly_detection_tab

2. Ajouter onglet dans st.tabs() :
   tab1, tab2, ..., tab_new = st.tabs([
       "üìä Dashboard",
       ...
       "üîç D√©tection Anomalies"
   ])
   
   with tab_new:
       render_anomaly_detection_tab()
"""
